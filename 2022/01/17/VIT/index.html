<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Vison in Transformer</title><meta name="description" content="while(true) me.Learn();"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/tom.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale从2012年AlexNet提出以来，卷积神经网络在计算机视觉领域一直占据统治地位。而本篇论文的研究表明，拥有足够多数据进行预训练的情况下，Transformer网络架构也可以把计算机视觉问题解决的很好。更进一步来说，这篇论文的提出打破了cv和nlp之间的壁垒，在多模态领域也产生了很大影响
Intro
将每一张图片视作由许多16x16的patch组成，每个patch当作nlp领域中的单词，一张图片便可视作一个sequence
CV领域不需要局限于CNNs的结构，纯Transformer在预训练集足够大时在图像分类任务达到了目前CNNs的SOTA，并且需要更少的计.."><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Xavier's blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Xavier's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Vison in Transformer</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><span class="toc-text">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intro"><span class="toc-text">Intro</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Problems-and-Purposes"><span class="toc-text">Problems and Purposes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Method"><span class="toc-text">Method</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Neural%20Network"><i class="tag post-item-tag">Neural Network</i></a><a href="/tags/Computer%20Vision"><i class="tag post-item-tag">Computer Vision</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Vison in Transformer</h1><time class="has-text-grey" datetime="2022-01-17T08:04:28.649Z">2022-01-17</time><article class="mt-2 post-content"><p><img src="/images/VIT/cover.jpg" alt="$cover"></p>
<h1 id="An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><a href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale" class="headerlink" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"></a>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h1><p>从2012年AlexNet提出以来，卷积神经网络在计算机视觉领域一直占据统治地位。而本篇论文的研究表明，拥有足够多数据进行预训练的情况下，Transformer网络架构也可以把计算机视觉问题解决的很好。更进一步来说，这篇论文的提出打破了cv和nlp之间的壁垒，在多模态领域也产生了很大影响</p>
<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a><strong>Intro</strong></h2><ul>
<li>将每一张图片视作由许多16x16的patch组成，每个patch当作nlp领域中的单词，一张图片便可视作一个sequence</li>
<li>CV领域不需要局限于CNNs的结构，纯Transformer在预训练集足够大时在图像分类任务达到了目前CNNs的SOTA，并且需要更少的计算资源</li>
<li>使用Transformer架构，到目前为止还未出现增加数据和模型复杂度导致性能饱和的现象</li>
<li>CNNs具有人为规定的两个先验信息（局部性、平移不变性），而Transformer是缺少这样的信息的，所以在数据集不够的时候时VIT比CNNs的SOTA要差一点（因为VIT需要自己去学习两个先验信息），进一步扩大数据集后，效果达到SOTA</li>
</ul>
<h2 id="Problems-and-Purposes"><a href="#Problems-and-Purposes" class="headerlink" title="Problems and Purposes"></a><strong>Problems and Purposes</strong></h2><ul>
<li>受Transformer在nlp领域可扩展性的成功，本文想尽量少地修改Transformer架构，将这种可扩展性带到CV领域</li>
<li>如何把2d的图片变成1d的序列</li>
<li>输入transformer的长一般为512、1024这个量级，要考虑计算性能，序列过长复杂度无法接受</li>
<li>本文的解决方案：将原图划分为多个16x16地patch，再将patch组成sequence，大大减小了序列长度</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a><strong>Method</strong></h2><p><img src="/images/VIT/cover.jpg" alt="$cover"><br>将原始图片划分出多个patches，patches经过线性投射层展平为一维向量，为每个向量加入位置编码后输入Transformer Encoder。另外引入了一个额外的token（第0位向量），并以该对应的Transformer Encoder输出作为分类的依据</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><em></em><a class="button is-default" href="/2022/01/17/d2l_10/" title="D2L: Convolution and Pooling"><span class="has-text-weight-semibold">Next: D2L: Convolution and Pooling</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="xiaoyu2018/xiaoyu2018.github.io" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/xiaoyu2018"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Xavier 2022</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>