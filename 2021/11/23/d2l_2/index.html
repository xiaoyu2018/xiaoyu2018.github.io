<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>D2L: Linear Regression</title><meta name="description" content="while(true) me.Learn();"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/tom.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="
Linear RegressionBasic Knowledge
线性模型  
基本的线性模型可抽象为如下表示,其可以看作一个单层单神经元的神经网络
线性模型有显示解


优化
梯度下降需要注意的是梯度是t-1时刻得来的，而且梯度是t-1时刻样本点、标签值以及W参数值对应的梯度，因为他们都是损失函数中的因变量
成批计算梯度batch_size不能太大也不能太小。太小：并行计算难以发挥效果；太大：内存消耗增加、易陷入局部最优。batch_size是另一个重要的超参数。



Implementation
从零实现

import random
import torch
from torch.utils.data.dataloader import DataLoader

true_w=torch.tenso.."><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Xavier's blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Xavier's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">D2L: Linear Regression</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Linear-Regression"><span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-Knowledge"><span class="toc-text">Basic Knowledge</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation"><span class="toc-text">Implementation</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">D2L: Linear Regression</h1><time class="has-text-grey" datetime="2021-11-23T07:59:48.186Z">2021-11-23</time><article class="mt-2 post-content"><p><img src="/images/d2l/1/cover.png" alt="$cover"></p>
<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul>
<li>线性模型  <ul>
<li>基本的线性模型可抽象为如下表示,其可以看作一个单层单神经元的神经网络<br><img src="/images/d2l/2/l_model.png"></li>
<li>线性模型有显示解<br><img src="/images/d2l/2/ex_solution.png"></li>
</ul>
</li>
<li>优化<ul>
<li>梯度下降<br>需要注意的是梯度是t-1时刻得来的，而且梯度是t-1时刻样本点、标签值以及W参数值对应的梯度，因为他们都是损失函数中的因变量<br><img src="/images/d2l/2/grad_desc.png"></li>
<li>成批计算梯度<br>batch_size不能太大也不能太小。太小：并行计算难以发挥效果；太大：内存消耗增加、易陷入局部最优。batch_size是另一个重要的超参数。</li>
</ul>
</li>
</ul>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul>
<li>从零实现</li>
</ul>
<pre><code class="python">import random
import torch
from torch.utils.data.dataloader import DataLoader

true_w=torch.tensor([2.2,3])
true_b=torch.tensor([-1.9])
num_examples=2500
batch_size=50
epoch=100
learning_rate=0.01
# 初始化参数
w=torch.normal(0,0.01,true_w.shape,requires_grad=True)
b=torch.zeros(true_b.shape,requires_grad=True)

def SyntheticData(w,b,num_examples):
    X=torch.normal(0,1,size=(num_examples,len(w)))
    # 支持多维度、不同维度的矩阵乘法
    # 直接加b用了广播机制
    y=torch.matmul(X,w) + b
    # 增加噪声，使得不能完全拟合
    y+=torch.normal(0,0.01,y.shape)

    return X,y.view(-1,1)

# 生成小批量
def DataIter(batch_size,features,labels):
    num_examples=len(features)
    
    indices=list(range(num_examples))

    # 打乱顺序
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices=torch.tensor(
            indices[i:min(i+batch_size,num_examples)]
            )
        
        # Tensor可以接受一个列表的下标来取值
        yield features[batch_indices],labels[batch_indices]

# 定义模型
def LinReg(X,w,b):
    
    return torch.matmul(X,w)+b

# 定义损失函数
def SquaredLoss(y_hat,y):
    return (y_hat-y.view(y_hat.shape))**2/2/batch_size

# 优化算法
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param-=lr*param.grad
            param.grad.zero_()
    

features,labels=SyntheticData(true_w,true_b,num_examples)

# 训练
def Train():

    for i in range(epoch):
        e_loss=[]
        for X,y in DataIter(batch_size,features,labels):
            out=LinReg(X,w,b)
            # 将batch_size个损失求和
            l=SquaredLoss(y,out).sum()
            l.backward()
            sgd([w,b],learning_rate,batch_size)

            e_loss.append(l.item())

        print(f"{i+1},loss:{sum(e_loss)/num_examples}")


Train()
# 简单验证
x=torch.tensor([108,2.666])
print(true_w,w)
print(true_b,b)
print(LinReg(x,true_w,true_b).item())
print(LinReg(x,w,b).item())
</code></pre>
<ul>
<li>简洁实现</li>
</ul>
<pre><code class="python">import numpy as np
import torch
from torch.utils import data
from torch import nn
from torch import optim

true_w=torch.tensor([2.2,3])
true_b=torch.tensor([-1.9])
num_examples=5000

def SyntheticData(w,b,num_examples):
    X=torch.normal(0,1,size=(num_examples,len(w)))
    y=torch.matmul(X,w)+b

    y+=torch.normal(0,0.01,y.shape)

    return X,y.view(-1,1)

features,labels=SyntheticData(true_w,true_b,num_examples)

# 使第一维成为两个Tensor的共同索引，所以两个Tensor的第一维size要相同
# 用来组成dataloader可处理的形式
dataset=data.TensorDataset(features,labels)
data_iter=data.dataloader.DataLoader(dataset,shuffle=True,batch_size=150)

net=nn.Sequential(nn.Linear(len(true_w),1,bias=True))
loss_f=nn.MSELoss()
opt=optim.SGD(net.parameters(),lr=0.01)

for epoch in range(100):
    e_loss=[]

    for X,y in data_iter:
        out=net(X)
        l=loss_f(out,y)

        e_loss.append(l.item())
        l.backward()
        opt.step()
        opt.zero_grad()
    
    print(f"{epoch+1} {sum(e_loss)}")

x=torch.tensor([108,2.666])
print(true_w,true_b)
for i in net.parameters():
    print(i)
</code></pre>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2021/11/23/d2l_3/" title="D2L: Softmax Regression"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: D2L: Softmax Regression</span></a><a class="button is-default" href="/2021/11/22/Transformer/" title="Transformer"><span class="has-text-weight-semibold">Next: Transformer</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="xiaoyu2018/xiaoyu2018.github.io" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/xiaoyu2018"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Xavier 2021</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>