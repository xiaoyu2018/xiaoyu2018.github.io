<!DOCTYPE html><html class="appearance-auto" lang="EN"><head><meta charset="UTF-8"><title>D2L: Matrix</title><meta name="description" content="while(true) me.Learn();"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/tom.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="
MatrixBasic knowledge
标量
x=torch.tensor([1.0])
y=torch.tensor([2.0])

print(x+y,x*y,x/y,x**y)


向量
x=torch.arange(4)
print(x,x[3],len(x),x.shape,x.size())


矩阵
x=torch.eye(20)
x=torch.arange(20).view(5,4)
print(x,x.T,x.t())
print(torch.arange(24).reshape(2,3,4))


运算
A=torch.arange(20,dtype=torch.float32).view(5,4)
B=A.clone() #创建A的副本给B，A与B指向不同地址
C=B  #B与.."><meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Xavier's blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Xavier's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">D2L: Matrix</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">点击返回顶部</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Matrix"><span class="toc-text">Matrix</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-knowledge"><span class="toc-text">Basic knowledge</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Derivative"><span class="toc-text">Derivative</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Automatic-derivative"><span class="toc-text">Automatic derivative</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Neural%20Network"><i class="tag post-item-tag">Neural Network</i></a><a href="/tags/Dive%20Into%20Deep%20Learning"><i class="tag post-item-tag">Dive Into Deep Learning</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">D2L: Matrix</h1><time class="has-text-grey" datetime="2021-11-22T08:23:46.324Z">2021-11-22</time><article class="mt-2 post-content"><p><img src="/images/d2l/1/cover.png" alt="$cover"></p>
<h1 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul>
<li><p>标量</p>
<pre><code class="python">x=torch.tensor([1.0])
y=torch.tensor([2.0])

print(x+y,x*y,x/y,x**y)
</code></pre>
</li>
<li><p>向量</p>
<pre><code class="python">x=torch.arange(4)
print(x,x[3],len(x),x.shape,x.size())
</code></pre>
</li>
<li><p>矩阵</p>
<pre><code class="python">x=torch.eye(20)
x=torch.arange(20).view(5,4)
print(x,x.T,x.t())
print(torch.arange(24).reshape(2,3,4))
</code></pre>
</li>
<li><p>运算</p>
<pre><code class="python">A=torch.arange(20,dtype=torch.float32).view(5,4)
B=A.clone() #创建A的副本给B，A与B指向不同地址
C=B  #B与C指向相同地址
# B，C都会改变
C[0,:] = 0
print(A,B,C)
# 矩阵加 点乘 每个元素求sin
print(A+B,A*B,torch.sin(A))
A=torch.arange(2*20).view(2,5,4).float()
print(A)
# 求和可指定维度,不指定则全部维度求和
print(A.sum(axis=0),A.sum([0,1]),A.sum())
# 求均值可指定维度,不指定则全部维度求均值
# keepdim=True代表不把求均值的那个维度丢掉，一般用来保证广播机制的正常运行
print(A.mean(axis=0,keepdim=True),A.mean([0,1]),A.mean())
# 求个数不可以指定维度
print(A.numel())
# 点积
X=torch.arange(4).float()
Y=torch.ones(4)
print(torch.dot(X,Y))
# 2范数,即向量长度，如果输入超过一维，会展成一维然后当成向量计算
print(torch.norm(X))
# 矩阵乘法
X=X.view(2,2)
Y=Y.view(2,2)
print(torch.mm(X,Y))
</code></pre>
</li>
</ul>
<h2 id="Derivative"><a href="#Derivative" class="headerlink" title="Derivative"></a><strong>Derivative</strong></h2><ul>
<li><p>向量上的导数<br>与向量相关的导数有以下四种形式<br><img src="/images/d2l/1/dv.png"></p>
<ul>
<li>y标量，x向量<br>y是由x中各分量计算得到的标量，最终得到y分别每个分量求导得出的向量，这也是梯度的计算过程<br><img src="/images/d2l/1/dv_1.png"></li>
<li>y向量，x标量<br>向量y的每个分量都是x的函数，每个分量分别对x求导最终得出一个向量<br><img src="/images/d2l/1/dv_2.png"></li>
<li>y向量，x向量<br>向量y的每个分量都是由x中各分量计算得到的标量，y的每个分量都分别对x的每个分量求导得出一个向量，最后得到一个矩阵<br><img src="/images/d2l/1/dv_3.png"></li>
</ul>
</li>
<li><p>矩阵上的导数<br>导数同样也可以被扩展到矩阵<br><img src="/images/d2l/1/dm.png"></p>
</li>
</ul>
<h2 id="Automatic-derivative"><a href="#Automatic-derivative" class="headerlink" title="Automatic derivative"></a><strong>Automatic derivative</strong></h2><ul>
<li><p>链式求法则<br>在神经网络中需要关注向量的链式求导，关键还是要把形状搞对<br><img src="/images/d2l/1/chain.png"> </p>
</li>
<li><p>自动求导<br>其含义是计算一个函数在指定值上的导数，它不同于符号求导和数值求导</p>
<ul>
<li><p>计算图<br>计算图本质上就等价于链式求导法则的求导过程，它将计算表示成一个无环图，下面是具体的例子<br><img src="/images/d2l/1/calc_graph.png"></p>
</li>
<li><p>反向传播<br>反向传播解决了自动求导的问题，它利用在前向传播时计算图中存储的计算中间结果，一步一步反向算出链式求导中各步导数<br><img src="/images/d2l/1/bp.png"></p>
</li>
<li><p>自动求导实现</p>
<pre><code class="python"># pytorch中做Tensor的计算时默认直接构造计算图
# 计算图中入度为0的节点还应该存储关于自身的梯度，需要手动设置requires_grad=True
x=torch.arange(4.0,requires_grad=True)
y=2*torch.dot(x,x)
print(y)
y.backward()
# 计算得出的梯度存在grad中
print(x.grad)

# 要注意pytorch默认会把同一个变量上的梯度累加
# 在另一轮计算梯度时需要先清除之前的值
x.grad.zero_()
y=x.sum()
y.backward()
print(x.grad)

# 在成batch地计算损失的过程中
# 最后将一个向量对向量求导的过程，转化为了标量对向量求导
x.grad.zero_()

# y是一个向量，把它对应为一个batch的损失
y=x*x
# 如果直接求导，会得到一个矩阵，这不是我们想要的
# 将batch中每一个损失加起来得到一个batch的损失后再求导
# 这也是pytorch中所做的
y.sum().backward()
print(x.grad)

# 将某些计算移动到计算图之外
# 这在想要固定网络中某些参数时是有用的
x.grad.zero_()
y=x*x
# u不被计入为计算图的一个单元，而是一个常数
u=y.detach()
z=u*x
z.sum().backward()
print(x.grad==u)

# 控制流也可以正确构成计算图并计算梯度
</code></pre>
</li>
</ul>
</li>
</ul>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><em></em><a class="button is-default" href="/2021/11/19/Gan0/" title="GAN #1"><span class="has-text-weight-semibold">下一页: GAN #1</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="xiaoyu2018/xiaoyu2018.github.io" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/xiaoyu2018"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Xavier 2021</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>