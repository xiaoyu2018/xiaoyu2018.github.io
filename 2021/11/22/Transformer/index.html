<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Transformer</title><meta name="description" content="while(true) me.Learn();"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/tom.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="
Attention Is All Your Need以往主流的序列转录(seq2seq)模型中常常基于包含Encoder与Decoder的复杂RNN或CNN，这些模型也会在Encoder与Decoder中使用Attention机制。Transformer仅仅使用了Attention机制，完全没用到循环和卷积，将循环层换为了Multi-headed Attetion。Transformer训练速度更快，预测能力更好。  

Advantages
RNN难以并行计算计算效率低，Transformer可并行。
RNN带有时序信息，但在序列较长时，早期的信息可能在后期丢失。Attention可通过在输入序列中加入index增加时序，并且不会存在信息丢失问题。
用CNN可以替换掉RNN实现并行计算，但由于感受野的限.."><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Xavier's blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Xavier's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Transformer</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Attention-Is-All-Your-Need"><span class="toc-text">Attention Is All Your Need</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Advantages"><span class="toc-text">Advantages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Architecture"><span class="toc-text">Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder"><span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention"><span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross-Attention-in-Transformer"><span class="toc-text">Cross Attention in Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feed-Forward"><span class="toc-text">Feed-Forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedding-and-Softmax"><span class="toc-text">Embedding and Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-text">Positional Encoding</span></a></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/papers"><i class="tag post-item-tag">papers</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a><a href="/tags/Seq2Seq-Model"><i class="tag post-item-tag">Seq2Seq-Model</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Transformer</h1><time class="has-text-grey" datetime="2021-11-22T08:36:52.958Z">2021-11-22</time><article class="mt-2 post-content"><p><img src="/images/transformer/transformer.jpg" alt="$cover"></p>
<h1 id="Attention-Is-All-Your-Need"><a href="#Attention-Is-All-Your-Need" class="headerlink" title="Attention Is All Your Need"></a>Attention Is All Your Need</h1><p>以往主流的序列转录(seq2seq)模型中常常基于包含Encoder与Decoder的复杂RNN或CNN，这些模型也会在Encoder与Decoder中使用Attention机制。<br>Transformer仅仅使用了Attention机制，完全没用到循环和卷积，将循环层换为了Multi-headed Attetion。Transformer训练速度更快，预测能力更好。  </p>
<hr>
<h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul>
<li>RNN难以并行计算计算效率低，Transformer可并行。</li>
<li>RNN带有时序信息，但在序列较长时，早期的信息可能在后期丢失。Attention可通过在输入序列中加入index增加时序，并且不会存在信息丢失问题。</li>
<li>用CNN可以替换掉RNN实现并行计算，但由于感受野的限制其依然存在难以对长序列进行建模的问题。Transformer中的Attention机制一次性看到所有的序列，消除了这一问题。</li>
<li>CNN可利用多个输出通道识别不一样的模式，在Transformer中使用Multi-headed Attetion，也实现了这样的特性。</li>
</ul>
<hr>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Transformer整体分为Encoder与Decoder两大部分。  </p>
<ul>
<li>Input Embedding/Output Embedding将词映射到向量。</li>
<li>Postoinal Encoding</li>
<li>Nx指有N个该块叠在一起。</li>
<li>Add表示残差连接，Norm表示正则处理。</li>
<li>在训练时，解码器的输入（outputs）是真实值(Ground Truth)；在测试时，输入（outputs）是前一时刻的输出。<br><img src="/images/transformer/architecture.png" alt="architecture"></li>
</ul>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>N=6，out_dim=input_dim=512。由两个子层组成，两个子层分别为Multi-headed Attetion和简单的MLP。每个子层使用残差连接和layer normalization。</p>
<blockquote>
<p>layer normalization and batch normalization</p>
<blockquote>
<p>batch:对每一个feature，将这个feature在一个batch中的所有数据的均值变为0方差变为1<br><img src="/images/transformer/batch_norm.png" alt="batch_norm"><br>layer:对每一个样本，将这个样本所有的特征均值变为0方差变为1<br><img src="/images/transformer/layer_norm.png" alt="layer_norm"><br>成sequence的normalization:<br>蓝为batch，黄为layer，取所有数据去做norm。阴影部分为样本实际长度（即该样本序列的seq_len），在实际长度外取全0。<br><img src="/images/transformer/seq_norm.png" alt="seq_norm"><br>使用layernormalization相对稳定一些。每个样本自己做均值方差再去norm。</p>
</blockquote>
</blockquote>
<hr>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>N=6,由三个子层组成，其中后两个子层Encoder一致，而第一个子层有所不同，其使用了Masked Multi-headed Attetion。在Decoder中还使用了自回归。当前的outputs输入是上一次的输出。在训练过程中，Decoder的输入outputs为ground truth，但在t时刻的输入不应包含t时刻之后的输入，所以第一个子层引入了masked机制。</p>
<hr>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><blockquote>
<p>Transformer使用了Scaled Dot-Product Attention。其将输入的一个seq进行融合输出一个等长的seq。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>在融合时，每一个输出都考虑了其对应位置输入元素和seq中其他元素的相关度。Wq与Wk是两个参数矩阵，原始输入做矩阵运算后得到两个向量q、k，最后用内积运算即得到了原始输入的相关性（内积运算代表了余弦相似度）。<br><img src="/images/transformer/dot-product.png" alt="dot-product"><br>同理可计算出a1与自身及序列中所有元素的相关性，全部计算完成后输入到softmax层进行归一化。<br><img src="/images/transformer/to-softmax.png" alt="to-softmax"><br>利用一个新的参数矩阵Wv获得v1-v4。v与相关性系数的带权和得到b1。<br><img src="/images/transformer/b1.png" alt="b1"><br>同理，可以得到b2、b3、b4。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>上述步骤可表示为矩阵运算。<br><img src="/images/transformer/use_matrix.png" alt="use_matrix"><br>最终的计算公式可如下表示。这里多了一个除以根号dk(input_dim)，这是因为向量(Transformer中dk=512)比较长时，内积绝对值可能会出现比较大的情况，这对梯度下降是不利的。<br><img src="/images/transformer/formula.png" alt="formula"> </p>
</blockquote>
<blockquote>
<p>Multi-headed Attetion可以看作是多通道的Attention。以2heads为例，计算过程如下，增加了多个参数矩阵。<br><img src="/images/transformer/multi-head.png" alt="multi-head"><br>在最后，将bi1与bi2一起其他的相应多通道b在特征维度拼接起来，为保证dim不变，利用一个新的参数矩阵Wo将输出元素的维度变为和输入相同。<br><img src="/images/transformer/concat.png" alt="concat"><br>一个线性层就可看作一个参数矩阵，所以上述两步操作可看作如下的过程。<br><img src="/images/transformer/liner-representatoin.png" alt="liner-representatoin"> </p>
</blockquote>
<hr>
<h3 id="Cross-Attention-in-Transformer"><a href="#Cross-Attention-in-Transformer" class="headerlink" title="Cross Attention in Transformer"></a>Cross Attention in Transformer</h3><p>Cross Attention指的是Encoder与Decoder之间的Attention机制。其V和K来自于Encoder，而Q来自于masked Attention。由于是masked，向Q输入的部分其seq_len会与V和K的不同，又因为其作为Q输入，所以cross-attention输出的seq_len会与之相同，所以Decoder的输入输出seq_len是相同的。<br><img src="/images/transformer/cross-attention.png" alt="cross-attention">  </p>
<hr>
<h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed-Forward"></a>Feed-Forward</h3><p>只有一个MLP分别去作用于seq中的每个词，<br>图中MLP权重是相同的，也不需要把Encoder的输出合并输入到大的MLP。因为这里只是想要把原始维度投影到想要的另一个维度，其信息融合已经在Encoder中做完了。<br><img src="/images/transformer/feed-forward.png" alt="feed-forward"></p>
<hr>
<h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>编码器要有embedding，解码器要有embedding，softmax层之前有一个Liner层，这三个层共享权重。</p>
<hr>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>为了给Attention加上时序信息，给输入加上位置信息。</p>
<hr>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2021/11/23/d2l_2/" title="D2L: Linear Regression"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: D2L: Linear Regression</span></a><a class="button is-default" href="/2021/11/22/CSAsync3/" title="C# Asynchronous programming #3"><span class="has-text-weight-semibold">Next: C# Asynchronous programming #3</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="xiaoyu2018/xiaoyu2018.github.io" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/xiaoyu2018"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Xavier 2021</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>