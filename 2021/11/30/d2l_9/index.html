<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>D2L: Sequential Model</title><meta name="description" content="while(true) me.Learn();"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/tom.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="
Sequential ModelBasic knowledge序列模型是考虑时间信息的模型

序列数据

数据带有时序结构，如电影的评价随时间变化
电影拿奖后评分上升
导演、演员负面报道后评分下降




统计工具  

将序列中每个元素看作随机变量，显然他们不是独立的
在实际操作中，时序序列一般只能正向建模去预测
要使用序列模型预测T时刻x的概率，核心是求T时刻的条件概率（似然），这里的f可看作神经网络，神经网络将训练集建模。自回归指的是用数据对见过的数据建模（因为最后预测也是在预测相同的数据），与非序列模型用数据对独立于数据的标签建模不同。
具体如何建模？
马尔科夫假设当前预测的数据只跟过去的tau个数据相关，tau是一个固定常数。假设x是标量数据，此时只需要将其看作回归问题，使用MLP把tau个x当.."><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Xavier's blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Xavier's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">D2L: Sequential Model</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Sequential-Model"><span class="toc-text">Sequential Model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-knowledge"><span class="toc-text">Basic knowledge</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation"><span class="toc-text">Implementation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%AD%A3%E5%BC%A6%E5%87%BD%E6%95%B0%E5%8A%A0%E4%B8%8A%E5%99%AA%E5%A3%B0%E7%94%9F%E6%88%90%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="toc-text">使用正弦函数加上噪声生成序列数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%86%E6%95%B0%E6%8D%AE%E6%98%A0%E5%B0%84%E4%B8%BA%E6%95%B0%E6%8D%AE%E5%AF%B9"><span class="toc-text">将数据映射为数据对</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%B5%8B%E8%AF%95%E9%9B%86%E9%A2%84%E6%B5%8B"><span class="toc-text">使用测试集预测</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%A2%84%E6%B5%8B%E5%80%BC%E8%BF%9B%E8%A1%8C%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B"><span class="toc-text">使用预测值进行多步预测</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">D2L: Sequential Model</h1><time class="has-text-grey" datetime="2021-11-30T08:41:29.526Z">2021-11-30</time><article class="mt-2 post-content"><p><img src="/images/d2l/1/cover.png" alt="$cover"></p>
<h1 id="Sequential-Model"><a href="#Sequential-Model" class="headerlink" title="Sequential Model"></a>Sequential Model</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><p>序列模型是考虑时间信息的模型</p>
<ul>
<li><p>序列数据</p>
<ul>
<li>数据带有时序结构，如电影的评价随时间变化<ul>
<li>电影拿奖后评分上升</li>
<li>导演、演员负面报道后评分下降</li>
</ul>
</li>
</ul>
</li>
<li><p>统计工具  </p>
<ul>
<li>将序列中每个元素看作随机变量，显然他们不是独立的<br><img src="/images/d2l/9/1.png"></li>
<li>在实际操作中，时序序列一般只能正向建模去预测<br><img src="/images/d2l/9/2.png"></li>
<li>要使用序列模型预测T时刻x的概率，核心是求T时刻的条件概率（似然），这里的f可看作神经网络，神经网络将训练集建模。自回归指的是用数据对见过的数据建模（因为最后预测也是在预测相同的数据），与非序列模型用数据对独立于数据的标签建模不同。<br><img src="/images/d2l/9/3.png"></li>
<li>具体如何建模？<ul>
<li>马尔科夫假设<br>当前预测的数据只跟过去的tau个数据相关，tau是一个固定常数。假设x是标量数据，此时只需要将其看作回归问题，使用MLP把tau个x当作特征训练得到t时刻标量x。MLP进行梯度优化的过程便是最大化似然概率的过程<br><img src="/images/d2l/9/4.png"></li>
<li>潜变量模型<br>引入一个可不断更新的潜变量用于概括历史信息，使得建模更加简单（RNN）<br><img src="/images/d2l/9/5.png"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul>
<li>马尔可夫假设+MLP<br>```python<br>import torch<br>from torch import nn<br>from torch.utils.data import TensorDataset<br>from torch.utils.data.dataloader import DataLoader<br>from torch import optim<br>import math</li>
</ul>
<h1 id="使用正弦函数加上噪声生成序列数据"><a href="#使用正弦函数加上噪声生成序列数据" class="headerlink" title="使用正弦函数加上噪声生成序列数据"></a>使用正弦函数加上噪声生成序列数据</h1><p>T=1000</p>
<p>time=torch.arange(1,T+1,dtype=torch.float32)<br>data=torch.sin(0.01*time)+torch.normal(0,0.1,time.shape)</p>
<h1 id="将数据映射为数据对"><a href="#将数据映射为数据对" class="headerlink" title="将数据映射为数据对"></a>将数据映射为数据对</h1><p>tau=4<br>labels=data[tau:].view(-1,1)<br>features=torch.zeros(T-tau,tau)<br>for i in range(tau):<br>    features[:,i]=data[i:T-tau+i]</p>
<p>train_dataset=TensorDataset(features[:600],labels[:600])<br>test_dataset=TensorDataset(features[600:],labels[600:])</p>
<p>train_iter=DataLoader(train_dataset,batch_size=16)<br>test_iter=DataLoader(test_dataset,batch_size=16)</p>
<p>net=nn.Sequential(<br>    nn.Linear(4,32),<br>    nn.Dropout(0.1),<br>    nn.ReLU(),<br>    nn.Linear(32,16),<br>    nn.ReLU(),<br>    nn.Linear(16,1)<br>    )<br>loss_f=nn.MSELoss()<br>opt=optim.Adam(net.parameters())</p>
<p>try:<br>    net.load_state_dict(torch.load(“9.params”))</p>
<p>except:<br>    for epoch in range(100):<br>        train_loss=[]<br>        test_loss=[]<br>        for X,y in train_iter:<br>            out=net(X)<br>            l=loss_f(out,y)</p>
<pre><code>        l.backward()
        train_loss.append(l.item())
        opt.step()
        opt.zero_grad()

    with torch.no_grad():
        for X,y in test_iter:
            out=net(X)
            l=loss_f(out,y)
            test_loss.append(l.item())

    print(f"{epoch+1},{sum(train_loss)}  {sum(test_loss)}")
torch.save(net.state_dict(),"9.params")
</code></pre>
<h1 id="使用测试集预测"><a href="#使用测试集预测" class="headerlink" title="使用测试集预测"></a>使用测试集预测</h1><p>from matplotlib import pyplot as plt<br>t=600<br>steps=396</p>
<p>plt.plot([i for i in range(t,t+steps)],net(features[600:]).view(-1).detach().numpy())</p>
<h1 id="使用预测值进行多步预测"><a href="#使用预测值进行多步预测" class="headerlink" title="使用预测值进行多步预测"></a>使用预测值进行多步预测</h1><p>win=[math.sin(i*0.01) for i in range(t-4,t)]<br>true=[]<br>pred=[]<br>for i in range(steps):<br>    X=torch.tensor(win,dtype=torch.float32)<br>    out=net(X)<br>    truth=math.sin((t+i)*0.01)<br>    true.append(truth);pred.append(out.item())<br>    # print(out.item(),t)<br>    win.pop(0)<br>    win.append(out)</p>
<p>plt.plot([i for i in range(t,t+steps)],pred)<br>plt.plot([i for i in range(t,t+steps)],true)<br>plt.show()<br>```</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2021/12/01/Gan0/" title="GAN #1"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: GAN #1</span></a><a class="button is-default" href="/2021/11/28/d2l_8/" title="D2L: Basic Pytorch"><span class="has-text-weight-semibold">Next: D2L: Basic Pytorch</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="xiaoyu2018/xiaoyu2018.github.io" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/xiaoyu2018"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Xavier 2022</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>