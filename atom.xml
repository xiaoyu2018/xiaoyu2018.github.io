<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xavier&#39;s blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-02-26T07:21:53.583Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Xavier</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>D2L: ResNet</title>
    <link href="http://example.com/2022/02/26/d2l_17/"/>
    <id>http://example.com/2022/02/26/d2l_17/</id>
    <published>2022-02-26T07:21:53.583Z</published>
    <updated>2022-02-26T07:21:53.583Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>加更多的层不一定总是改进精度</p><ul><li>新的层可能是使模型收敛范围偏差到一个不符合预期的区域</li><li>ResNet使各层更容易学会恒等变换，从而更容易使模型收敛范围达到Nested function classes<br><img src="/images/d2l/17/1.png"></li></ul></li><li><p>残差块</p><ul><li>基本的ResBlock结构如下，f(x)+x保证了包含原收敛范围<br><br><img src="/images/d2l/17/2.png"></li><li>具体使用时，ResBlock的设计细节<br><br><img src="/images/d2l/17/4.png"></li></ul></li><li><p>ResNet架构<br>一般来说现在的主流设计架构就是接入一个Stage（7x7Conv-3x3MP），之后再连接具体想要的网络架构，ResNet架构如下也是这种设计思想，具体架构如下<br>  <img src="/images/d2l/17/5.png"></p></li></ul><ul><li>Tricks<ul><li>实际应用中，Res34用的最多，达不到要求可以继续用Res50</li><li>Res152、Res101一般用来刷榜</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;ResNet&quot;&gt;&lt;a href=&quot;#ResNet&quot; class=&quot;headerlink&quot; title=&quot;ResNet&quot;&gt;&lt;/a&gt;ResNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-knowledge&quot; class</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Vison in Transformer</title>
    <link href="http://example.com/2022/02/21/VIT/"/>
    <id>http://example.com/2022/02/21/VIT/</id>
    <published>2022-02-21T12:07:31.165Z</published>
    <updated>2022-02-21T12:07:31.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><a href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale" class="headerlink" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"></a>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h1><p>从2012年AlexNet提出以来，卷积神经网络在计算机视觉领域一直占据统治地位。而本篇论文的研究表明，拥有足够多数据进行预训练的情况下，Transformer网络架构也可以把计算机视觉问题解决的很好。更进一步来说，这篇论文的提出打破了cv和nlp之间的壁垒，在多模态领域也产生了很大影响</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a><strong>Intro</strong></h2><ul><li>将每一张图片视作由许多16x16的patch组成，每个patch当作nlp领域中的单词，一张图片便可视作一个sequence</li><li>CV领域不需要局限于CNNs的结构，纯Transformer在预训练集足够大时在图像分类任务达到了目前CNNs的SOTA，并且需要更少的计算资源</li><li>使用Transformer架构，到目前为止还未出现增加数据和模型复杂度导致性能饱和的现象</li><li>CNNs具有人为规定的两个先验信息（局部性、平移不变性），而Transformer是缺少这样的信息的，所以在数据集不够的时候时VIT比CNNs的SOTA要差一点（因为VIT需要自己去学习两个先验信息），进一步扩大数据集后，效果达到SOTA</li></ul><h2 id="Problems-and-Purposes"><a href="#Problems-and-Purposes" class="headerlink" title="Problems and Purposes"></a><strong>Problems and Purposes</strong></h2><ul><li>受Transformer在nlp领域可扩展性的成功，本文想尽量少地修改Transformer架构，将这种可扩展性带到CV领域</li><li>如何把2d的图片变成1d的序列</li><li>输入transformer的长一般为512、1024这个量级，要考虑计算性能，序列过长复杂度无法接受</li><li>本文的解决方案：将原图划分为多个16x16地patch，再将patch组成sequence，大大减小了序列长度</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a><strong>Method</strong></h2><p><img src="/images/VIT/cover.jpg" alt="$cover"><br>将原始图片划分出多个patches，patches经过线性投射层展平为一维向量，为每个向量加入位置编码后输入Transformer Encoder。另外引入了一个额外的token（第0位向量），并以该对应的Transformer Encoder输出作为分类的依据</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale&quot;&gt;&lt;a href=&quot;#An-Image-is-Worth-16x16-Words-Transformers-for-</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Computer Vision" scheme="http://example.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Batch Normalization</title>
    <link href="http://example.com/2022/02/21/d2l_16/"/>
    <id>http://example.com/2022/02/21/d2l_16/</id>
    <published>2022-02-21T06:36:14.190Z</published>
    <updated>2022-02-21T06:36:14.190Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>问题<ul><li>损失出现在最后，由BP算法和梯度消失，后面的层训练的会更快</li><li>数据在最前面，前面的层训练的慢且前面的层变化后面的层也要跟着变（抽取的底层信息变化让后面的层要重新学），所以后面的层要重新学习很多次，导致收敛变慢</li><li>考虑在学习底部层时避免变化顶部层</li></ul></li><li>批量归一化<ul><li>固定小批量里面的均值和方差，然后再做额外的调整（可学习的参数gama和beta）<br><img src="/images/d2l/16/1.png"></li><li>是线性变换</li><li>作用在<ul><li>全连接层和卷积层输出后，激活函数前</li><li>全连接层和卷积层输入前</li></ul></li><li>对于全连接层作用于特征维</li><li>对于卷积层作用于通道维（将每一个像素都当作一个样本，通道数就是一个样本的特征数）</li></ul></li><li>批量归一化在做什么？<ul><li>最初的论文是想用它来减少内部协变量转移（使每一层的输出分布变化不那么剧烈）</li><li>后续有论文指出，批量归一化可能只是在小批量中加入噪声控制模型复杂度</li></ul></li><li>总结<ul><li>批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放</li><li>批量归一化可以加速收敛（可以设置更大的学习率），一般不改变模型精度</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch<br><br>net=nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),nn.BatchNorm2d(<span class="hljs-number">6</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),nn.BatchNorm2d(<span class="hljs-number">16</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.BatchNorm1d(<span class="hljs-number">120</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.BatchNorm1d(<span class="hljs-number">84</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters(),lr=<span class="hljs-number">1.0</span>)<br><br><span class="hljs-keyword">import</span> d2l<br><br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,nn.CrossEntropyLoss(),</span><br><span class="hljs-comment">#     optim.Adam(net.parameters()),</span><br><span class="hljs-comment">#     net,train_iter,save_name="LeNet_bn",</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"))</span><br>d2l.evaluate(<br>    net,test_iter,nn.CrossEntropyLoss(),<br>    param_path=<span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/LeNet_bn_10"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Batch-Normalization&quot;&gt;&lt;a href=&quot;#Batch-Normalization&quot; class=&quot;headerlink&quot; title=&quot;Batch Normalization&quot;&gt;&lt;/a&gt;Batch Normalization&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Basic SQL</title>
    <link href="http://example.com/2022/02/21/sql/"/>
    <id>http://example.com/2022/02/21/sql/</id>
    <published>2022-02-21T06:31:45.970Z</published>
    <updated>2022-02-21T06:31:45.970Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-SQL"><a href="#Basic-SQL" class="headerlink" title="Basic SQL"></a>Basic SQL</h1><p>数据库语言可分为两个部分，DDL和DML。</p><ul><li>DDL（Data Definitoin Language）用于描述数据库中要存储的现实世界实体，操作数据库的结构等</li><li>DML（Data Manipulation Language）用于数据库操作，操作数据库存储的对象或数据</li></ul><h2 id="SQL-DDL-数据定义"><a href="#SQL-DDL-数据定义" class="headerlink" title="SQL DDL:数据定义"></a><strong>SQL DDL:数据定义</strong></h2><h3 id="数据库相关"><a href="#数据库相关" class="headerlink" title="数据库相关"></a>数据库相关</h3><ul><li>查询所有数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">show</span> databases;<br></code></pre></td></tr></tbody></table></figure></li><li>创建新数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> database 数据库名称;<br></code></pre></td></tr></tbody></table></figure></li><li>删除数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">drop</span> database 数据库名称;<br></code></pre></td></tr></tbody></table></figure><h3 id="表相关"><a href="#表相关" class="headerlink" title="表相关"></a>表相关</h3></li><li>查询当前数据库下所有表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">show</span> tables;<br></code></pre></td></tr></tbody></table></figure></li><li>创建新表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql">   <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> 表名称<br>   (<br>列名称<span class="hljs-number">1</span> 数据类型[(最大位数) 约束]，<br>列名称<span class="hljs-number">2</span> 数据类型[(最大位数) 约束]，<br>列名称<span class="hljs-number">3</span> 数据类型[(最大位数) 约束],<br>...    <br>   )<br></code></pre></td></tr></tbody></table></figure></li><li>删除表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">drop</span> <span class="hljs-keyword">table</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改已有表的列  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 在表中添加列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名称<br><span class="hljs-keyword">add</span> 列名称 数据类型;<br><br># 删除表中的列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名称<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">column</span> 列名称;<br></code></pre></td></tr></tbody></table></figure><h2 id="SQL-DML-数据操作"><a href="#SQL-DML-数据操作" class="headerlink" title="SQL DML:数据操作"></a><strong>SQL DML:数据操作</strong></h2></li><li>从表中获取数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 获取表中某一列数据<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br># 或获取表中所有数据<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 更新符合要求的行中某列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称 <span class="hljs-operator">=</span> 新值 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br># 更新符合要求的行中若干列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称<span class="hljs-number">1</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">2</span> <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br></code></pre></td></tr></tbody></table></figure></li><li>删除表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 删除表中的某行<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 值;<br># 删除所有行（在不删除表的情况下删除所有的行。这意味着表的结构、属性和索引都是完整的）<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称;<br><span class="hljs-keyword">delete</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>向表中插入数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称 <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br># 指定要插入的列<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称(列<span class="hljs-number">1</span>,列<span class="hljs-number">2</span>,...) <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br></code></pre></td></tr></tbody></table></figure></li><li>去重后从表中获取某列中值  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>根据条件获取表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">and</span> 列 运算符 值;<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">or</span> 列 运算符 值;<br></code></pre></td></tr></tbody></table></figure></li><li>根据指定的列对结果集进行排序  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br># 先按列名称<span class="hljs-number">1</span>排序，然后以列名称<span class="hljs-number">3</span>排序<br><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span>,列名称<span class="hljs-number">3</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)],列名称<span class="hljs-number">3</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Basic-SQL&quot;&gt;&lt;a href=&quot;#Basic-SQL&quot; class=&quot;headerlink&quot; title=&quot;Basic SQL&quot;&gt;&lt;/a&gt;Basic SQL&lt;/h1&gt;&lt;p&gt;数据库语言可分为两个部分，DDL和DML。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DDL（Dat</summary>
      
    
    
    
    <category term="Database" scheme="http://example.com/categories/Database/"/>
    
    
    <category term="Sql" scheme="http://example.com/tags/Sql/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://example.com/2022/01/29/Transformer/"/>
    <id>http://example.com/2022/01/29/Transformer/</id>
    <published>2022-01-29T06:19:03.569Z</published>
    <updated>2022-01-29T06:19:03.569Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention-Is-All-Your-Need"><a href="#Attention-Is-All-Your-Need" class="headerlink" title="Attention Is All Your Need"></a>Attention Is All Your Need</h1><p>以往主流的序列转录(seq2seq)模型中常常基于包含Encoder与Decoder的复杂RNN或CNN，这些模型也会在Encoder与Decoder中使用Attention机制。<br>Transformer仅仅使用了Attention机制，完全没用到循环和卷积，将循环层换为了Multi-headed Attetion。Transformer训练速度更快，预测能力更好。  </p><hr><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>RNN难以并行计算计算效率低，Transformer可并行。</li><li>RNN带有时序信息，但在序列较长时，早期的信息可能在后期丢失。Attention可通过在输入序列中加入index增加时序，并且不会存在信息丢失问题。</li><li>用CNN可以替换掉RNN实现并行计算，但由于感受野的限制其依然存在难以对长序列进行建模的问题。Transformer中的Attention机制一次性看到所有的序列，消除了这一问题。</li><li>CNN可利用多个输出通道识别不一样的模式，在Transformer中使用Multi-headed Attetion，也实现了这样的特性。</li></ul><hr><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Transformer整体分为Encoder与Decoder两大部分。  </p><ul><li>Input Embedding/Output Embedding将词映射到向量。</li><li>Postoinal Encoding</li><li>Nx指有N个该块叠在一起。</li><li>Add表示残差连接，Norm表示正则处理。</li><li>在训练时，解码器的输入（outputs）是真实值(Ground Truth)；在测试时，输入（outputs）是前一时刻的输出。<br><img src="/images/transformer/architecture.png" alt="architecture"></li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>N=6，out_dim=input_dim=512。由两个子层组成，两个子层分别为Multi-headed Attetion和简单的MLP。每个子层使用残差连接和layer normalization。</p><blockquote><p>layer normalization and batch normalization</p><blockquote><p>batch:对每一个feature，将这个feature在一个batch中的所有数据的均值变为0方差变为1<br><img src="/images/transformer/batch_norm.png" alt="batch_norm"><br>layer:对每一个样本，将这个样本所有的特征均值变为0方差变为1<br><img src="/images/transformer/layer_norm.png" alt="layer_norm"><br>成sequence的normalization:<br>蓝为batch，黄为layer，取所有数据去做norm。阴影部分为样本实际长度（即该样本序列的seq_len），在实际长度外取全0。<br><img src="/images/transformer/seq_norm.png" alt="seq_norm"><br>使用layernormalization相对稳定一些。每个样本自己做均值方差再去norm。</p></blockquote></blockquote><hr><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>N=6,由三个子层组成，其中后两个子层Encoder一致，而第一个子层有所不同，其使用了Masked Multi-headed Attetion。在Decoder中还使用了自回归。当前的outputs输入是上一次的输出。在训练过程中，Decoder的输入outputs为ground truth，但在t时刻的输入不应包含t时刻之后的输入，所以第一个子层引入了masked机制。</p><hr><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><blockquote><p>Transformer使用了Scaled Dot-Product Attention。其将输入的一个seq进行融合输出一个等长的seq。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>在融合时，每一个输出都考虑了其对应位置输入元素和seq中其他元素的相关度。Wq与Wk是两个参数矩阵，原始输入做矩阵运算后得到两个向量q、k，最后用内积运算即得到了原始输入的相关性（内积运算代表了余弦相似度）。<br><img src="/images/transformer/dot-product.png" alt="dot-product"><br>同理可计算出a1与自身及序列中所有元素的相关性，全部计算完成后输入到softmax层进行归一化。<br><img src="/images/transformer/to-softmax.png" alt="to-softmax"><br>利用一个新的参数矩阵Wv获得v1-v4。v与相关性系数的带权和得到b1。<br><img src="/images/transformer/b1.png" alt="b1"><br>同理，可以得到b2、b3、b4。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>上述步骤可表示为矩阵运算。<br><img src="/images/transformer/use_matrix.png" alt="use_matrix"><br>最终的计算公式可如下表示。这里多了一个除以根号dk(input_dim)，这是因为向量(Transformer中dk=512)比较长时，内积绝对值可能会出现比较大的情况，这对梯度下降是不利的。<br><img src="/images/transformer/formula.png" alt="formula"> </p></blockquote><blockquote><p>Multi-headed Attetion可以看作是多通道的Attention。以2heads为例，计算过程如下，增加了多个参数矩阵。<br><img src="/images/transformer/multi-head.png" alt="multi-head"><br>在最后，将bi1与bi2一起其他的相应多通道b在特征维度拼接起来，为保证dim不变，利用一个新的参数矩阵Wo将输出元素的维度变为和输入相同。<br><img src="/images/transformer/concat.png" alt="concat"><br>一个线性层就可看作一个参数矩阵，所以上述两步操作可看作如下的过程。<br><img src="/images/transformer/liner-representatoin.png" alt="liner-representatoin"> </p></blockquote><hr><h3 id="Cross-Attention-in-Transformer"><a href="#Cross-Attention-in-Transformer" class="headerlink" title="Cross Attention in Transformer"></a>Cross Attention in Transformer</h3><p>Cross Attention指的是Encoder与Decoder之间的Attention机制。其V和K来自于Encoder，而Q来自于masked Attention。由于是masked，向Q输入的部分其seq_len会与V和K的不同，又因为其作为Q输入，所以cross-attention输出的seq_len会与之相同，所以Decoder的输入输出seq_len是相同的。<br><img src="/images/transformer/cross-attention.png" alt="cross-attention">  </p><hr><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed-Forward"></a>Feed-Forward</h3><p>只有一个MLP分别去作用于seq中的每个词，<br>图中MLP权重是相同的，也不需要把Encoder的输出合并输入到大的MLP。因为这里只是想要把原始维度投影到想要的另一个维度，其信息融合已经在Encoder中做完了。<br><img src="/images/transformer/feed-forward.png" alt="feed-forward"></p><hr><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>编码器要有embedding，解码器要有embedding，softmax层之前有一个Liner层，这三个层共享权重。</p><hr><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>为了给Attention加上时序信息，给输入加上位置信息。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Attention-Is-All-Your-Need&quot;&gt;&lt;a href=&quot;#Attention-Is-All-Your-Need&quot; class=&quot;headerlink&quot; title=&quot;Attention Is All Your Need&quot;&gt;&lt;/a&gt;Attentio</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Seq2Seq-Model" scheme="http://example.com/tags/Seq2Seq-Model/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="http://example.com/2022/01/29/resnet/"/>
    <id>http://example.com/2022/01/29/resnet/</id>
    <published>2022-01-29T06:18:59.404Z</published>
    <updated>2022-01-29T06:18:59.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><p>深层的神经网络通常很难进行训练，本文使用了一个残差学习网络结构来训练比以往的神经网络要深得多的模型。残差网络容易训练，并且在深层神经网络中表现出来较好的准确率。</p><p><img src="/images/resnet/plain_error.png" alt="plain_error"><br>在未使用残差网络的模型中，当网络层数变多时，训练误差以及测试误差均会升高。</p><h2 id="Is-learning-better-networks-as-easy-as-stacking-more-layers"><a href="#Is-learning-better-networks-as-easy-as-stacking-more-layers" class="headerlink" title="Is learning better networks as easy as stacking more layers ?"></a><strong>Is learning better networks as easy as stacking more layers ?</strong></h2><ul><li>当网络变得特别深时，会出现梯度爆炸或梯度消失</li><li>传统的解决方法是：参数在初始化时要做的好一点，不要太大也不要太小；加入一些Batch Normalization Layers</li><li>传统的解决方法使得神经网络能够收敛，但是网络的精度却变得更差，而这并非是模型变得复杂后导致的过拟合问题，因为模型的训练误差也变高了</li><li>正常来说，如果在一个浅层的神经网络后直接加入更多的层，这些层只做identity mapping，那么这个深层神经网络的误差绝不会高于浅层的神经网络，但是传统的神经网络模型并未找到这样的解（或更好的解）</li><li>如果深层网络后面的层都是是恒等映射，那么模型就可以转化为一个浅层网络</li></ul><h2 id="Deep-residual-learning-framework"><a href="#Deep-residual-learning-framework" class="headerlink" title="Deep residual learning framework"></a><strong>Deep residual learning framework</strong></h2><ul><li>将残差块的输入与块内最后一个神经网络层的线性输出求和后在进行激活，得到残差块的输出<br><img src="/images/resnet/residual_block.png" alt="plain_error"></li><li>残差块只简单地做了shortcut connections，没有引入额外的训练参数，不会增加网络复杂度</li><li>已有的神经网络很难拟合潜在的恒等映射函数H(x) = x，但是ResNet将残差块设计为H(x) = F(x) + x，其直接把恒等映射作为网络的一部分，只要F(x) = 0，便得到了恒等映射。而此时F(x) = H(x) - x 称为残差函数，就是当前残差块的学习目标（学习出这样一个F(x)函数满足如图输出）</li><li>值得一提的是，一个残差块中应该至少有两层（中间要包含一个非线性激活），否则就会出现如下情况，这显然是没有用的工作<br><img src="/images/resnet/no_use.png" alt="plain_error"></li></ul><h2 id="Deeper-bottleneck-architecture"><a href="#Deeper-bottleneck-architecture" class="headerlink" title="Deeper bottleneck architecture"></a><strong>Deeper bottleneck architecture</strong></h2><ul><li>当神经网络层数进一步增多时，参数的增长会带来很大的计算开销。此时可以考虑使用1*1的卷积核暂时减少通道数来减少整个网络的数据规模<br><img src="/images/resnet/deeper.png" alt="plain_error"></li></ul><h2 id="Analysis-of-Deep-Residual-Networks"><a href="#Analysis-of-Deep-Residual-Networks" class="headerlink" title="Analysis of Deep Residual Networks"></a><strong>Analysis of Deep Residual Networks</strong></h2><ul><li>基本BP算法流程如下<br><img src="/images/resnet/bp.jpg" alt="plain_error"></li><li>残差块的反向传播过程较好地解释了残差网络避免梯度消失原因，具体推导过程如下</li><li>推导中忽略偏置项和激活函数<br><img src="/images/resnet/res_bp.png" alt="plain_error"></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Residual-Network&quot;&gt;&lt;a href=&quot;#Residual-Network&quot; class=&quot;headerlink&quot; title=&quot;Residual Network&quot;&gt;&lt;/a&gt;Residual Network&lt;/h1&gt;&lt;p&gt;深层的神经网络通常很难进行训</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Residual-learning" scheme="http://example.com/tags/Residual-learning/"/>
    
  </entry>
  
  <entry>
    <title>GAN #1</title>
    <link href="http://example.com/2022/01/29/Gan0/"/>
    <id>http://example.com/2022/01/29/Gan0/</id>
    <published>2022-01-29T06:18:55.732Z</published>
    <updated>2022-01-29T06:18:55.732Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Generative-Adversarial-Nets"><a href="#Generative-Adversarial-Nets" class="headerlink" title="Generative Adversarial Nets"></a>Generative Adversarial Nets</h1><p>Gan利用对抗的方法，提出了生成式模型的新框架。它需要同时训练两个模型（生成模型G和判别模型D），G用于捕获数据的分布，而D需要判别出一个样本是来自训练集还是生成模型。生成模型的目标是尽最大可能让判别模型犯错（无法成功判别数据的来源），G和D在本文中被定义为多层感知机，整个系统通过反向传播来进行训练。</p><h2 id="AN-analogy-to-GAN"><a href="#AN-analogy-to-GAN" class="headerlink" title="AN analogy to GAN"></a><strong>AN analogy to GAN</strong></h2><p>在生成对抗网络框架中，生成模型与判别互相对抗。可以把生成模型类比为造假币者，判别模型类比为警察。生成模型试图制造假币骗过判别模型，而判别模型努力区分假币。二者在这样的对抗中不断学习提升各自的水平，直到生成模型的造的假币和真的一模一样，判别器无法区分。另外，警察进步不能过大或过小。进步过大时，造假者直接被一锅端，无法继续造假钞；进步过小时，造假者不需进步也能骗过警察，则没有动力进步</p><h2 id="Adversarial-nets"><a href="#Adversarial-nets" class="headerlink" title="Adversarial nets"></a><strong>Adversarial nets</strong></h2><ul><li><p>对抗网络认为数据集代表着一个联合分布，每一个样本都可以由高维随机变量表示。假设数据集是许多张2*2大小的黑白图片，每张图片有四个像素点，于是将该分布看作四维随机变量的分布，每一维代表着一个像素的取值（黑白只取0或1）<br><img src="/images/gan/mrv.jpg"></p></li><li><p>在让生成模型学习数据集代表了分布之前，首先定义一个符合高斯分布的先验噪声Z，生成模型就是要学习把Z映射成为数据集代表的分布，MLP理论上可以拟合这样一个函数来完成目标。生成器以Z为输入，输出一个尽量符合数据集分布的样本  </p></li><li><p>判别器也是一个MLP，判别器以数据集或生成器生产的样本为输入，输出一个标量（0：生成器生成的样本，1：真实样本，判别器的输出介于[0,1]），判别样本的真伪。判别器要学习一个二分类任务  </p></li><li><p>D与G对于同一个目标函数采取相反的优化方式：<br><img src="/images/gan/obj_func.png"><br>在不同分布里采样计算后得到期望和，生成器和鉴别器分别要调整参数最大化和最小化这一期望和，体现了对抗的过程，D尽量区分生成的数据和真实数据，G尽量使得生成的数据和真实数据难以区分</p></li><li><p>下图展示了对抗网络训练过程中，各个成分的变化：<br><img src="/images/gan/graph_proc.png"><br>Z为高斯噪声，绿色线代表G生成数据的概率密度函数，黑色线代表真实数据的概率密度函数，蓝色代表D输出标量的函数<br>（a）G将Z随即映射为另一个分布，与真实分布存在一定差距，此时D未经学习，分类能力弱<br>（b）固定G，训练D，D分类能力明显上升<br>（c）固定D，训练G，G学习到把Z向真实数据分布映射<br>（d）反复多轮后，G映射的分布与真实数据相同，判别器无法区分</p></li><li><p>对抗网络算法流程如下：<br><img src="/images/gan/algorithm.png"><br>先更新D，再更新G，G只与目标函数中后半段有关。k是一个超参数，不能太大也不能大小。取太大判别器训练得太好，取太小判别器变化太小</p></li></ul><h2 id="Theoretical-results"><a href="#Theoretical-results" class="headerlink" title="Theoretical results"></a><strong>Theoretical results</strong></h2><ul><li>理论上，对抗网络存在全局最优解：生成器映射的分布等于真实数据分布</li><li>对抗网络算法可以求解目标函数</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Generative-Adversarial-Nets&quot;&gt;&lt;a href=&quot;#Generative-Adversarial-Nets&quot; class=&quot;headerlink&quot; title=&quot;Generative Adversarial Nets&quot;&gt;&lt;/a&gt;Gener</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Generative-Model" scheme="http://example.com/tags/Generative-Model/"/>
    
  </entry>
  
  <entry>
    <title>D2L: GoogLeNet</title>
    <link href="http://example.com/2022/01/29/d2l_15/"/>
    <id>http://example.com/2022/01/29/d2l_15/</id>
    <published>2022-01-29T06:18:33.434Z</published>
    <updated>2022-01-29T06:18:33.434Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>Inception块<ul><li>4个路径从不同层面抽取信息，然后再输出通道合并，最终输出高宽与输入相等，要把更多的通道数留给比较重要的通道<br><img src="/images/d2l/15/1.png"></li><li>要达到相同的输出通道数，Inception块与直接的3x3或5x5卷积相比，参数和计算复杂度更低</li></ul></li><li>GoogLeNet<ul><li>5个stage（高宽减半一次就是一个stage），9个Inception块<br><img src="/images/d2l/15/2.png"></li></ul></li><li>Inception后续具有多个变种<ul><li>Inception-BN(v2)：使用batch normalization</li><li>Inception-v3：修改了inception块</li><li>Inception-v4：使用了残差连接</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inception</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>        self.relu=nn.ReLU()<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        p1 = self.relu(self.p1_1(x))<br>        p2 = self.relu(self.p2_2(self.relu(self.p2_1(x))))<br>        p3 = self.relu(self.p3_2(self.relu(self.p3_1(x))))<br>        p4 = self.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 以通道维拼接张量</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br><br>b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>,<br>                                           padding=<span class="hljs-number">1</span>))<br><br>b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>), nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,resize=<span class="hljs-number">96</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"),</span><br><span class="hljs-comment">#     save_name="GoogLeNet"</span><br><span class="hljs-comment"># )</span><br><br>d2l.evaluate(<br>    net,test_iter,loss_f,<br>    <span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/GoogLeNet_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GoogLeNet&quot;&gt;&lt;a href=&quot;#GoogLeNet&quot; class=&quot;headerlink&quot; title=&quot;GoogLeNet&quot;&gt;&lt;/a&gt;GoogLeNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-know</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Convolution and Pooling</title>
    <link href="http://example.com/2022/01/29/d2l_10/"/>
    <id>http://example.com/2022/01/29/d2l_10/</id>
    <published>2022-01-29T06:18:27.246Z</published>
    <updated>2022-01-29T06:18:27.246Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Convolution-and-Pooling"><a href="#Convolution-and-Pooling" class="headerlink" title="Convolution and Pooling"></a>Convolution and Pooling</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>卷积</p><ul><li>平移不变性和局部性是在图片中寻找某种模式的原则。<ul><li>因为模式不会随着其在图片中位置改变而改变，所以一个识别器（卷积核）被设计为具有平移不变性（即参数只与输入的像素值有关，而与像素在图片的位置无关），去学习图片中的一种模式</li><li>模式与其相邻的局部相关，识别器每次仅去看图片的一部分</li><li>对全连接层使用平移不变性和局部性得到卷积层</li></ul></li></ul></li><li><p>卷积层<br><img src="/images/d2l/10/1.png"></p><ul><li>不同卷积核（值）会对图片带来不同的效果，当某种效果对任务有帮助时，网络很有可能就会学习出这种卷积核<br><img src="/images/d2l/10/2.png"></li></ul></li><li><p>填充与步幅</p><ul><li>在输入的四周加入额外的行和列以控制卷积后的输出图像大小，卷积核大小一般选奇数，能上下对称地填充图片来保证输入输出图片大小不变<br><img src="/images/d2l/10/3.png"></li><li>增大卷积步幅，快速缩小图片</li><li>总结<br><img src="/images/d2l/10/4.png"></li></ul></li><li><p>通道</p><ul><li>每个通道有自己的卷积核，输入通道不同通道的对应卷积后直接相加后再加偏置项，最后输出一个单通道</li><li>多输出通道就是多个上述操作输出的多个单通道</li><li>每个输出通道可以识别特定的模式，输入通道识别并组合（加权相加）输入中的模式</li></ul></li><li><p>池化</p><ul><li>池化层缓解卷积对位置的敏感性</li><li>池化层不学习任何参数，有最大池化、平均池化等</li><li>池化层也可以调整填充与步幅</li><li>经过池化层不会改变通道数</li></ul></li><li><p>tricks</p><ul><li>填充：一般填充就是为了使图小大小不变</li><li>步幅：一般设置为1，计算量太大时增大步幅</li><li>最终图像大小：一般为3x3、5x5、7x7</li><li>1x1卷积层：不识别空间模式，而是用来改变通道数</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Convolution-and-Pooling&quot;&gt;&lt;a href=&quot;#Convolution-and-Pooling&quot; class=&quot;headerlink&quot; title=&quot;Convolution and Pooling&quot;&gt;&lt;/a&gt;Convolution and P</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: LeNet</title>
    <link href="http://example.com/2022/01/29/d2l_11/"/>
    <id>http://example.com/2022/01/29/d2l_11/</id>
    <published>2022-01-29T06:18:26.253Z</published>
    <updated>2022-01-29T06:18:26.253Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>最早是用于手写数字识别，识别信件上的邮政编码</li><li>网络结构<br><img src="/images/d2l/11/1.png" alt="$cover"></li><li>提出了一个数据集：MNIST<ul><li>5w个训练数据</li><li>1w个测试数据</li><li>图像大小 28x28</li><li>10类</li></ul></li><li>总结<ul><li>LeNet是早期成功的神经网络</li><li>先使用卷积层学习图片空间信息</li><li>然后使用全连接层转换到类别空间</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-comment"># 定义Reshape层</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Reshape</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br><br>net=nn.Sequential(<br>    Reshape(),<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br><br>x=torch.rand((<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>),dtype=torch.float32)<br><br><span class="hljs-comment"># 观察每一层输出的tensor尺寸</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    x=layer(x)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">':\t'</span>,x.size())<br><br><span class="hljs-comment"># 在Fashion-MINST上的表现</span><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     25,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     param_name="LeNet",device=torch.device("cuda:0")</span><br><span class="hljs-comment">#     )</span><br>d2l.evaluate(net,test_iter,loss_f,<span class="hljs-string">".\params\LeNet_25"</span>)   <br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LeNet&quot;&gt;&lt;a href=&quot;#LeNet&quot; class=&quot;headerlink&quot; title=&quot;LeNet&quot;&gt;&lt;/a&gt;LeNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-knowledge&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: AlexNet</title>
    <link href="http://example.com/2022/01/29/d2l_12/"/>
    <id>http://example.com/2022/01/29/d2l_12/</id>
    <published>2022-01-29T06:18:25.147Z</published>
    <updated>2022-01-29T06:18:25.147Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>在深度学习之前：<ul><li>核方法<ul><li>特征提取</li><li>选择核函数</li><li>凸优化问题</li><li>漂亮的定理</li></ul></li><li>几何学<ul><li>抽取特征</li><li>将计算机视觉问题描述为几何问题（如多相机）</li><li>凸优化</li><li>漂亮的定理</li><li>建立假设模型，若假设满足，效果会很好</li></ul></li><li>特征工程<ul><li>特征工程（人工特征提取）是关键，不太关心机器学习模型</li><li>特征描述子（SIFT、SURF）</li><li>视觉词袋（聚类）</li><li>最后一般用SVM</li></ul></li></ul></li><li>AlexNet赢得了2012年ImageNet竞赛的冠军，引起了深度学习的热潮，其本质上是一个更深更大的LeNet<ul><li>主要改进：<ul><li>丢弃法</li><li>ReLU</li><li>MaxPooling</li><li>数据增强（截取、调亮度、调色温等）</li><li>更深更大</li></ul></li></ul></li><li>网络结构与复杂度<br><img src="/images/d2l/12/1.png"></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AlexNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1=nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,kernel_size=<span class="hljs-number">11</span>,stride=<span class="hljs-number">4</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv2=nn.Conv2d(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>)<br>        self.conv3=nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv4=nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">384</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv5=nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">256</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.liner1=nn.Linear(<span class="hljs-number">6400</span>,<span class="hljs-number">4096</span>)<br>        self.liner2=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">4096</span>)<br>        self.liner3=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)<br><br>        self.pool=nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>)<br>        self.flatten=nn.Flatten()<br>        self.relu=nn.ReLU()<br>        self.dropout=nn.Dropout(p=<span class="hljs-number">0.5</span>)<br>    <br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        <br>        x=self.relu(self.pool(self.conv1(x)))<br>        <br>        x=self.relu(self.pool(self.conv2(x)))<br>        x=self.relu(self.conv3(x))<br>        x=self.relu(self.conv4(x))<br>        x=self.relu(self.conv5(x))<br>        x=self.pool(x)<br>        x=self.flatten(x)<br>        x=self.dropout((self.liner1(x)))<br>        x=self.dropout((self.liner2(x)))<br>        x=self.liner3(x)<br><br>        <span class="hljs-keyword">return</span> x<br><br>net=AlexNet().to(torch.device(<span class="hljs-string">"cuda:0"</span>))<br><br><span class="hljs-comment"># x=torch.rand((1,1,224,224)).cuda()</span><br><span class="hljs-comment"># print(net(x).size())</span><br><br><span class="hljs-comment"># 读取Fashion-MNIST，将图片直接拉伸为224x224</span><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>,<span class="hljs-number">224</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     25,nn.CrossEntropyLoss(),</span><br><span class="hljs-comment">#     optim.Adam(net.parameters()),</span><br><span class="hljs-comment">#     net,train_iter,save_name="AlexNet",</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"))</span><br>d2l.evaluate(<br>    net,test_iter,nn.CrossEntropyLoss(),<br>    param_path=<span class="hljs-string">"D:\code\machine_learning\limu_d2l\params\AlexNet_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AlexNet&quot;&gt;&lt;a href=&quot;#AlexNet&quot; class=&quot;headerlink&quot; title=&quot;AlexNet&quot;&gt;&lt;/a&gt;AlexNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-knowledge&quot; c</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: NiN</title>
    <link href="http://example.com/2022/01/29/d2l_14/"/>
    <id>http://example.com/2022/01/29/d2l_14/</id>
    <published>2022-01-29T06:18:22.343Z</published>
    <updated>2022-01-29T06:18:22.343Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>全连接层的问题：<ul><li>全连接层参数比卷积层的参数多很多，导致很多的内存（显存）及计算带宽占用</li><li>全连接层容易带来过拟合</li></ul></li><li>NiN思想：完全不要全连接层</li><li>NiN块：<ul><li>一个卷积层后跟两个起到全连接层的作用的卷积层</li><li>起到全连接层的作用的卷积层为1x1步幅为1无填充的卷积层</li></ul></li><li>NiN架构<ul><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层</li><li>最后使用全局平均池化层得到输出（通道数是类别数）<br><img src="/images/d2l/14/1.png"></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NiNBlock</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,in_channels,out_channels,</span></span><br><span class="hljs-params"><span class="hljs-function">        kernel_size,stride,padding</span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv=nn.Conv2d(<br>            in_channels,out_channels,kernel_size,<br>            stride=stride,padding=padding<br>        )<br>        self.f1=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">1</span>)<br>        self.f2=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">1</span>)<br>        self.relu=nn.ReLU()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        x=self.relu(self.conv(x))<br>        x=self.relu(self.f1(x))<br>        x=self.relu(self.f2(x))<br>        <span class="hljs-keyword">return</span> x<br><br>nin_net=nn.Sequential(<br>    NiNBlock(<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,<span class="hljs-number">11</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),<br>    NiNBlock(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),<br>    NiNBlock(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),nn.Dropout(),<br>    NiNBlock(<span class="hljs-number">384</span>,<span class="hljs-number">10</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>    <span class="hljs-comment"># 目标输出size为1x1，也就是全局池化</span><br>    nn.AdaptiveAvgPool2d(<span class="hljs-number">1</span>),<br>    nn.Flatten()<br>)<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(nin_net.parameters())<br><br><span class="hljs-keyword">import</span> d2l<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,resize=<span class="hljs-number">224</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,loss_f,opt,nin_net,train_iter,</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"),</span><br><span class="hljs-comment">#     save_name="NIN"</span><br><span class="hljs-comment"># )</span><br>d2l.evaluate(<br>    nin_net,test_iter,loss_f,<br>    <span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/NIN_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NiN&quot;&gt;&lt;a href=&quot;#NiN&quot; class=&quot;headerlink&quot; title=&quot;NiN&quot;&gt;&lt;/a&gt;NiN&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-knowledge&quot; class=&quot;headerlink</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: VGGNet</title>
    <link href="http://example.com/2022/01/29/d2l_13/"/>
    <id>http://example.com/2022/01/29/d2l_13/</id>
    <published>2022-01-29T06:18:17.521Z</published>
    <updated>2022-01-29T06:18:17.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>AlexNet的设计很随意，如何变大变深无规律性，VGG探讨了如何对CNN进行扩展</li><li>如何更深更大？<ul><li>更多全连接层（太贵）</li><li>更多的卷积层</li><li>将卷积层组合成块（VGG）</li></ul></li><li>VGG块<ul><li>使用小卷积核深网络比大小卷积核浅网络效果好</li><li>3x3卷积层（n层、m通道）</li><li>2x2最大池化层<br><img src="/images/d2l/13/2.png"></li></ul></li><li>VGG架构<ul><li>多个VGG块后接全连接层</li><li>不同次数的重复块得到不同架构（VGG-16、VGG-19等）<br><img src="/images/d2l/13/1.png"></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> dropout, nn,optim<br><span class="hljs-keyword">import</span> d2l<br><span class="hljs-comment"># 返回VGG块</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_block</span>(<span class="hljs-params">num_convs,in_channels,out_channels</span>):</span><br>    layers=[]<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(<br>            nn.Conv2d(in_channels,out_channels,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>            )<br>        layers.append(nn.ReLU())<br>        in_channels=out_channels<br><br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>    <br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_architecture</span>(<span class="hljs-params">num_blocks,in_channels</span>):</span><br>    blocks=[]<br>    out_channels=<span class="hljs-number">16</span><br>    blocks.append(vgg_block(<span class="hljs-number">1</span>,in_channels,out_channels))<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks-<span class="hljs-number">1</span>):<br>        in_channels=out_channels<br>        out_channels*=<span class="hljs-number">2</span><br>        blocks.append(vgg_block(<span class="hljs-number">1</span>,in_channels,out_channels))<br>    <br>    <span class="hljs-keyword">return</span> nn.Sequential(*blocks)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_5</span>(<span class="hljs-params">in_channels</span>):</span><br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        vgg_architecture(<span class="hljs-number">5</span>,in_channels),<br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">1024</span>),<br>        nn.Dropout(),<br>        nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">512</span>),<br>        nn.Dropout(),<br>        nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>    )<br><br>vgg=vgg_5(<span class="hljs-number">1</span>).to(torch.device(<span class="hljs-string">"cuda:0"</span>))<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">64</span>,<span class="hljs-number">224</span>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(vgg.parameters())<br><br>d2l.train(<span class="hljs-number">10</span>,loss_f,opt,vgg,train_iter,save_name=<span class="hljs-string">"vgg_5"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;VGGNet&quot;&gt;&lt;a href=&quot;#VGGNet&quot; class=&quot;headerlink&quot; title=&quot;VGGNet&quot;&gt;&lt;/a&gt;VGGNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-knowledge&quot; class</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Sequential Model</title>
    <link href="http://example.com/2022/01/29/d2l_9/"/>
    <id>http://example.com/2022/01/29/d2l_9/</id>
    <published>2022-01-29T06:17:59.274Z</published>
    <updated>2022-01-29T06:17:59.274Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sequential-Model"><a href="#Sequential-Model" class="headerlink" title="Sequential Model"></a>Sequential Model</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><p>序列模型是考虑时间信息的模型</p><ul><li><p>序列数据</p><ul><li>数据带有时序结构，如电影的评价随时间变化<ul><li>电影拿奖后评分上升</li><li>导演、演员负面报道后评分下降</li></ul></li></ul></li><li><p>统计工具  </p><ul><li>将序列中每个元素看作随机变量，显然他们不是独立的<br><img src="/images/d2l/9/1.png"></li><li>在实际操作中，时序序列一般只能正向建模去预测<br><img src="/images/d2l/9/2.png"></li><li>要使用序列模型预测T时刻x的概率，核心是求T时刻的条件概率（似然），这里的f可看作神经网络，神经网络将训练集建模。自回归指的是用数据对见过的数据建模（因为最后预测也是在预测相同的数据），与非序列模型用数据对独立于数据的标签建模不同。<br><img src="/images/d2l/9/3.png"></li><li>具体如何建模？<ul><li>马尔科夫假设<br>当前预测的数据只跟过去的tau个数据相关，tau是一个固定常数。假设x是标量数据，此时只需要将其看作回归问题，使用MLP把tau个x当作特征训练得到t时刻标量x。MLP进行梯度优化的过程便是最大化似然概率的过程<br><img src="/images/d2l/9/4.png"></li><li>潜变量模型<br>引入一个可不断更新的潜变量用于概括历史信息，使得建模更加简单（RNN）<br><img src="/images/d2l/9/5.png"></li></ul></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>马尔可夫假设+MLP<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 使用正弦函数加上噪声生成序列数据</span><br>T=<span class="hljs-number">1000</span><br><br>time=torch.arange(<span class="hljs-number">1</span>,T+<span class="hljs-number">1</span>,dtype=torch.float32)<br>data=torch.sin(<span class="hljs-number">0.01</span>*time)+torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.1</span>,time.shape)<br><br><span class="hljs-comment"># 将数据映射为数据对</span><br>tau=<span class="hljs-number">4</span><br>labels=data[tau:].view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>features=torch.zeros(T-tau,tau)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau):<br>    features[:,i]=data[i:T-tau+i]<br><br>train_dataset=TensorDataset(features[:<span class="hljs-number">600</span>],labels[:<span class="hljs-number">600</span>])<br>test_dataset=TensorDataset(features[<span class="hljs-number">600</span>:],labels[<span class="hljs-number">600</span>:])<br><br>train_iter=DataLoader(train_dataset,batch_size=<span class="hljs-number">16</span>)<br>test_iter=DataLoader(test_dataset,batch_size=<span class="hljs-number">16</span>)<br><br>net=nn.Sequential(<br>    nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>),<br>    nn.Dropout(<span class="hljs-number">0.1</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">16</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">16</span>,<span class="hljs-number">1</span>)<br>    )<br>loss_f=nn.MSELoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-keyword">try</span>:<br>    net.load_state_dict(torch.load(<span class="hljs-string">"9.params"</span>))<br><br><span class="hljs-keyword">except</span>:<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>        train_loss=[]<br>        test_loss=[]<br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            out=net(X)<br>            l=loss_f(out,y)<br><br>            l.backward()<br>            train_loss.append(l.item())<br>            opt.step()<br>            opt.zero_grad()<br><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> test_iter:<br>                out=net(X)<br>                l=loss_f(out,y)<br>                test_loss.append(l.item())<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>,<span class="hljs-subst">{<span class="hljs-built_in">sum</span>(train_loss)}</span>  <span class="hljs-subst">{<span class="hljs-built_in">sum</span>(test_loss)}</span>"</span>)<br>    torch.save(net.state_dict(),<span class="hljs-string">"9.params"</span>)<br><br><span class="hljs-comment"># 使用测试集预测</span><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>t=<span class="hljs-number">600</span><br>steps=<span class="hljs-number">396</span><br><br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],net(features[<span class="hljs-number">600</span>:]).view(-<span class="hljs-number">1</span>).detach().numpy())<br><br><span class="hljs-comment"># 使用预测值进行多步预测</span><br><br>win=[math.sin(i*<span class="hljs-number">0.01</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t-<span class="hljs-number">4</span>,t)]<br>true=[]<br>pred=[]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):<br>    X=torch.tensor(win,dtype=torch.float32)<br>    out=net(X)<br>    truth=math.sin((t+i)*<span class="hljs-number">0.01</span>)<br>    true.append(truth);pred.append(out.item())<br>    <span class="hljs-comment"># print(out.item(),t)</span><br>    win.pop(<span class="hljs-number">0</span>)<br>    win.append(out)<br><br><br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],pred)<br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],true)<br>plt.show()<br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Sequential-Model&quot;&gt;&lt;a href=&quot;#Sequential-Model&quot; class=&quot;headerlink&quot; title=&quot;Sequential Model&quot;&gt;&lt;/a&gt;Sequential Model&lt;/h1&gt;&lt;h2 id=&quot;Basic-kno</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Basic Pytorch</title>
    <link href="http://example.com/2022/01/29/d2l_8/"/>
    <id>http://example.com/2022/01/29/d2l_8/</id>
    <published>2022-01-29T06:17:55.849Z</published>
    <updated>2022-01-29T06:17:55.849Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-Pytorch"><a href="#Basic-Pytorch" class="headerlink" title="Basic Pytorch"></a>Basic Pytorch</h1><ul><li>模型构造</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>net=nn.Sequential(nn.Linear(<span class="hljs-number">20</span>,<span class="hljs-number">256</span>),nn.ReLU(),nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>))<br>X=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>))<br><span class="hljs-built_in">print</span>(net(X))<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.rand_weight=torch.randn((<span class="hljs-number">20</span>,<span class="hljs-number">64</span>),requires_grad=<span class="hljs-literal">False</span>,dtype=torch.float32)<br>        self.hidden=nn.Sequential(nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>),nn.ReLU(),nn.Linear(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>))<br>        self.out=nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        X=X@self.rand_weight+<span class="hljs-number">1</span><br>        X=nn.ReLU()(self.hidden(X))<br>        X=self.out(X)<br>        <span class="hljs-comment"># 下面这个过程流不会计入计算图</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">while</span> torch.<span class="hljs-built_in">abs</span>(X).<span class="hljs-built_in">sum</span>()&gt;<span class="hljs-number">1</span>:<br>                X/=<span class="hljs-number">2</span><br>        <span class="hljs-keyword">return</span> X.<span class="hljs-built_in">sum</span>()<br><br>net=MLP()<br><span class="hljs-built_in">print</span>(net(X))<br></code></pre></td></tr></tbody></table></figure><ul><li>参数管理</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>net=nn.Sequential(nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">8</span>),nn.ReLU(),nn.Linear(<span class="hljs-number">8</span>,<span class="hljs-number">1</span>))<br>X=torch.rand(size=(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>))<br><br><span class="hljs-comment"># 访问参数</span><br><span class="hljs-comment"># 访问网络中每一层，以及如何访问层中的参数</span><br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">0</span>].state_dict(),net[<span class="hljs-number">1</span>],net[<span class="hljs-number">2</span>].bias.data)<br><span class="hljs-comment"># 网络中的参数包括data和grad两部分，在这没做反向传播呢，所以梯度为None</span><br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>].weight.grad==<span class="hljs-literal">None</span>)<br><span class="hljs-comment"># 将整个网络信息打印</span><br><span class="hljs-built_in">print</span>(net)<br><br><span class="hljs-comment"># 初始化参数</span><br><span class="hljs-comment"># pytorch已经为我们做了比较好的默认初始化</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_xavier</span>(<span class="hljs-params">m</span>):</span><br>    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">type</span>(m)==nn.Linear):<br>        nn.init.xavier_normal_(m.weight)<br>        nn.init.zeros_(m.bias)<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_42</span>(<span class="hljs-params">m</span>):</span><br>    nn.init.constant_(m.weight,<span class="hljs-number">42</span>)<br>    nn.init.constant_(m.bias,<span class="hljs-number">42</span>)<br><span class="hljs-comment"># 将初始化函数应用到net每一个子层，不止可以用在初始化上</span><br>net[<span class="hljs-number">0</span>].apply(init_xavier)<br><span class="hljs-built_in">print</span>([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> net[<span class="hljs-number">0</span>].parameters()])<br>net[<span class="hljs-number">2</span>].apply(init_42)<br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>].weight,net[<span class="hljs-number">2</span>].bias)<br><br><span class="hljs-comment"># 更暴力的方法</span><br>net[<span class="hljs-number">0</span>].weight.data[:]+=<span class="hljs-number">100</span><br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">0</span>].weight.data)<br><br><span class="hljs-comment"># 参数绑定</span><br><span class="hljs-comment"># 让某几层共享同样的参数</span><br>shared=nn.Linear(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>)<br>net=nn.Sequential(nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">8</span>),nn.ReLU(),shared,nn.ReLU(),shared,nn.Sigmoid(),nn.Linear(<span class="hljs-number">8</span>,<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>]==shared)<br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>].weight.data==net[<span class="hljs-number">4</span>].weight.data)<br></code></pre></td></tr></tbody></table></figure><ul><li>自定义层</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment"># 自定义层和自定义模型没本质区别</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CenteredLayer</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        <br>        <span class="hljs-keyword">return</span> X-X.mean()<br><br>layer=CenteredLayer()<br><span class="hljs-built_in">print</span>(layer(torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],dtype=torch.float32)))<br><br><span class="hljs-comment"># 带有参数的层</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PrameterizedLayer</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_dim,out_dim</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.w=nn.Parameter(torch.randn((in_dim,out_dim)))<br>        self.b=nn.Parameter(torch.randn((out_dim)))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        <br>        <span class="hljs-keyword">return</span> X@self.w.data+self.b.data<br><br>net=PrameterizedLayer(<span class="hljs-number">2</span>,<span class="hljs-number">8</span>)<br><span class="hljs-built_in">print</span>(net(torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]],dtype=torch.float32)))<br></code></pre></td></tr></tbody></table></figure><ul><li>读写文件</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>x=torch.arange(<span class="hljs-number">4</span>)<br><span class="hljs-comment"># 加载和保存张量</span><br>torch.save(x,<span class="hljs-string">'x_file'</span>)<br>y=torch.load(<span class="hljs-string">'x_file'</span>)<br><span class="hljs-built_in">print</span>(x==y)<br><br><span class="hljs-comment"># 加载和保存张量组成的数据结构</span><br>a=torch.arange(<span class="hljs-number">5</span>)<br>b=torch.arange(<span class="hljs-number">5</span>)<br>torch.save([a,b],<span class="hljs-string">"list"</span>)<br>torch.save({<span class="hljs-string">'a'</span>:a,<span class="hljs-string">'b'</span>:b},<span class="hljs-string">"dict"</span>)<br><span class="hljs-comment"># 读取到内存会保持原数据结构</span><br><span class="hljs-built_in">print</span>(torch.load(<span class="hljs-string">"list"</span>))<br><span class="hljs-built_in">print</span>(torch.load(<span class="hljs-string">"dict"</span>))<br><br><span class="hljs-comment"># 加载和保存模型的参数</span><br>net=nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>),nn.Flatten(),nn.Linear(<span class="hljs-number">288</span>,<span class="hljs-number">64</span>))<br><span class="hljs-built_in">print</span>(net(torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.5</span>,size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">16</span>,<span class="hljs-number">16</span>))))<br><span class="hljs-built_in">print</span>(net.state_dict())<br>torch.save(net.state_dict(),<span class="hljs-string">'net.params'</span>)<br>new_net=nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>),nn.Flatten(),nn.Linear(<span class="hljs-number">288</span>,<span class="hljs-number">64</span>))<br>new_net.load_state_dict(torch.load(<span class="hljs-string">'net.params'</span>))<br><span class="hljs-built_in">print</span>(new_net==net)<br><br>X=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.5</span>,size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">16</span>,<span class="hljs-number">16</span>))<br><span class="hljs-built_in">print</span>(new_net(X)==net(X))<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Basic-Pytorch&quot;&gt;&lt;a href=&quot;#Basic-Pytorch&quot; class=&quot;headerlink&quot; title=&quot;Basic Pytorch&quot;&gt;&lt;/a&gt;Basic Pytorch&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;模型构造&lt;/li&gt;
&lt;/ul&gt;
&lt;fig</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Numerical Stability &amp; Initialization</title>
    <link href="http://example.com/2022/01/29/d2l_7/"/>
    <id>http://example.com/2022/01/29/d2l_7/</id>
    <published>2022-01-29T06:17:52.732Z</published>
    <updated>2022-01-29T06:17:52.732Z</updated>
    
    <content type="html"><![CDATA[<h1 id="D2L-Numerical-Stability-amp-Initialization"><a href="#D2L-Numerical-Stability-amp-Initialization" class="headerlink" title="D2L: Numerical Stability &amp; Initialization"></a>D2L: Numerical Stability &amp; Initialization</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>数值的稳定性<ul><li>神经网络的梯度<br>求某一层的参数的梯度，直接就对损失函数关于该层参数求导,然后通过链式法则，化成d-t次的矩阵乘法<br><img src="/images/d2l/7/chain_d.png"></li><li>梯度爆炸与梯度消失<br>上述连续的乘法运算会带来两个问题：梯度爆炸与梯度消失<br><img src="/images/d2l/7/e_v.png"><br>梯度爆炸带来的问题：梯度值超过计算机可表示大小、对学习率敏感<br>梯度消失带来的问题：梯度值变为0（超出计算可表示精度的小浮点数）、无论如何选择学习率训练都没有进展、神经网络无法做到更深</li></ul></li><li>模型初始化<ul><li>如何让训练更加稳定？（不产生梯度消失和梯度爆炸）<br>要让梯度值保持在合理的范围内，一般有如下方法：<ul><li>将乘法变为加法（ResNet、LSTM）</li><li>归一化（梯度归一化、梯度裁剪）</li><li>选定合适的激活函数</li><li>合理的初始参数</li></ul></li><li>让每层的方差是一个常数<br><img src="/images/d2l/7/bn.png"></li><li>合理的权重初始化<ul><li>需要在一个合理值区间里随机初始参数<ul><li>远离最优解的地方损失函数很复杂（梯度很大）</li><li>最优解附近比较平缓 </li></ul></li></ul></li><li>Xavier初始化<br>使得输入空间的方差和输出空间的方差尽量相等</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;D2L-Numerical-Stability-amp-Initialization&quot;&gt;&lt;a href=&quot;#D2L-Numerical-Stability-amp-Initialization&quot; class=&quot;headerlink&quot; title=&quot;D2L: Num</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Weight Decay &amp; Dropout</title>
    <link href="http://example.com/2022/01/29/d2l_6/"/>
    <id>http://example.com/2022/01/29/d2l_6/</id>
    <published>2022-01-29T06:17:49.417Z</published>
    <updated>2022-01-29T06:17:49.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Weight-Decay-amp-Dropout"><a href="#Weight-Decay-amp-Dropout" class="headerlink" title="Weight Decay &amp; Dropout"></a>Weight Decay &amp; Dropout</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>权重衰退<br>权重衰退可以控制模型复杂度，使其复杂度不会太大，从而一定程度上避免过拟合<ul><li>使用均方范数作为硬性限制<br>通过限制参数的取值范围来控制模型容量，具体有如下例子<br>在限制参数向量范数的情况下优化损失函数，一般不会使用这种正则方式<br><img src="/images/d2l/6/hard_res.png"></li><li>使用均方范数作为柔性限制<br>上述硬性限制有一个等价方案，具体如下，这就是一般的正则化方法，其作用同样也是使得参数被限制在一个较小的范围<br><img src="/images/d2l/6/soft_res.png"><br>下面是正则项对最优解影响的一个演示<br>坐标轴分别是w的各个分量，圆线是等高线。<br>正则项给了另外一个梯度，把原始的损失函数算出的最优解往原点拉，必然会导致W的取值范围变小从而使模型复杂度降低，也就减小了过拟合。另外一种理解，正则项加入后优化目标就不再全局最优点了，所以肯定会减小训练集拟合程度，也就减小了过拟合<br>考虑为什么λ控制了正则项的重要程度，因为求偏导时λ会变成梯度前的常数项<br><img src="/images/d2l/6/effect.png"></li><li>参数更新<br>带正则项后参数更新过程如下，这也说明了为什么这种方法叫权重衰退：更新前先把权重减小，然后继续更新梯度<br><img src="/images/d2l/6/update.png"></li><li>注意<br>权重衰减也只在训练过程中使用，用来限制训练过程中的参数，在最终的验证过程中，指标还是原先的损失函数</li></ul></li><li>丢弃法<ul><li>动机<br>一个好的模型需要对输入数据加入扰动鲁棒，使用有噪音的数据等价于Tikhonov正则。丢弃法就是在层之间加入噪音，丢弃法也可以看作一个正则</li><li>无偏差地加入噪音<br>丢弃法就是加入了无偏噪音<br><img src="/images/d2l/6/noise.png"></li><li>使用丢弃法  <ul><li>对其发作用域隐藏层地输出上，即随机将某些神经元的输出置零且其它输出按上述公式增大</li><li>丢弃法只在训练过程中使用，测试和验证过程中不使用，这样保证了确定的输出</li><li>丢弃法常用于全连接层</li></ul></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>权重衰减从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 训练数据设置比较小，容易过拟合</span><br>num_train=<span class="hljs-number">20</span><br>num_test=<span class="hljs-number">100</span><br>num_inputs=<span class="hljs-number">200</span><br>batch_size=<span class="hljs-number">5</span><br><br>true_w,true_b=torch.ones(num_inputs)*<span class="hljs-number">0.01</span>,<span class="hljs-number">0.05</span><br><br>train_data=synthetic_data(true_w,true_b,num_train)<br>train_iter=data_loader(train_data,batch_size)<br>test_data=synthetic_data(true_w,true_b,num_test)<br>test_iter=data_loader(test_data,batch_size)<br><br><span class="hljs-comment"># for X,y in train_iter:</span><br><span class="hljs-comment">#     print(X,y)</span><br>w=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=true_w.shape,requires_grad=<span class="hljs-literal">True</span>)<br>b=torch.zeros(<span class="hljs-number">1</span>,requires_grad=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># L2正则</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L2_penalty</span>(<span class="hljs-params">lambada,w</span>):</span><br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(w**<span class="hljs-number">2</span>)/<span class="hljs-number">2</span>*lambada<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">liner_reg</span>(<span class="hljs-params">w,b,X</span>):</span><br>    <span class="hljs-keyword">return</span> torch.matmul(X,w)+b<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net</span>(<span class="hljs-params">X</span>):</span><br>    <span class="hljs-keyword">return</span> liner_reg(w,b,X)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">squared_loss_L2</span>(<span class="hljs-params">y_hat,y</span>):</span><br>    loss=(y_hat.view(y.shape)-y)**<span class="hljs-number">2</span>/<span class="hljs-number">2</span>/<span class="hljs-built_in">len</span>(y)<br>    <span class="hljs-keyword">return</span> (loss+L2_penalty(<span class="hljs-number">0.5</span>,w)).<span class="hljs-built_in">sum</span>()<br><br>opt=optim.SGD([w,b],lr=<span class="hljs-number">0.01</span>)<br><br><br><br>train(<span class="hljs-number">100</span>,squared_loss_L2,opt,net,train_iter,test_iter)<br></code></pre></td></tr></tbody></table></figure><ul><li>权重衰减简洁实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">net=nn.Sequential(nn.Linear(num_inputs,<span class="hljs-number">1</span>))<br>loss_f=nn.MSELoss()<br><span class="hljs-comment"># weight_decay代表了L2范数前面的λ系数</span><br><span class="hljs-comment"># 权重衰减系数很小时，在当前数据集下过拟合非常明显</span><br><span class="hljs-comment"># 权重衰减系数很大时，欠拟合则会非常明显</span><br>opt=optim.SGD(net.parameters(),lr=<span class="hljs-number">0.01</span>,weight_decay=<span class="hljs-number">1.2</span>)<br><br>train(<span class="hljs-number">100</span>,loss_f,opt,net,train_iter,test_iter)<br><span class="hljs-comment"># 其它范数的正则pytorch没有直接的实现</span><br><span class="hljs-comment"># 手动实现也很简单</span><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Weight-Decay-amp-Dropout&quot;&gt;&lt;a href=&quot;#Weight-Decay-amp-Dropout&quot; class=&quot;headerlink&quot; title=&quot;Weight Decay &amp;amp; Dropout&quot;&gt;&lt;/a&gt;Weight Decay</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Modle Selection</title>
    <link href="http://example.com/2022/01/29/d2l_5/"/>
    <id>http://example.com/2022/01/29/d2l_5/</id>
    <published>2022-01-29T06:17:45.836Z</published>
    <updated>2022-01-29T06:17:45.836Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Modle-Selection"><a href="#Modle-Selection" class="headerlink" title="Modle Selection"></a>Modle Selection</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>训练误差和泛化误差  <ul><li>训练误差：模型在训练数据上的误差（不太关心）</li><li>泛化误差：模型在新数据上的误差（很关心）</li></ul></li><li>验证数据集和测试数据集<ul><li>验证数据集<br>用于在训练过程中评估模型好坏的数据集，一般从训练集中划分出一部分，验证数据集不能作为训练集让模型训练，用来动态调整模型超参数</li><li>测试数据集<br>模型最终训练完毕后，使用测试集测试模型泛化能力，<strong>不能使用测试集来调整模型超参数</strong>，大多数情况下不会被打上标签</li></ul></li><li>K-折交叉验证<br>通常情况下，我们都没有足够富裕的数据去从训练集中划分验证集，这是使用K-折交叉验证能较简单的解决问题<ul><li>思想：一般情况将K折交叉验证用于模型调优，找到使得模型泛化性能最优的超参值。找到后，在全部训练集上重新训练模型，并使用独立验证集对模型性能做出最终评价。</li><li>算法：K折就将训练集分为K块，训练代价为原来的K倍<ol><li>将原始数据集划分为相等的K部分（“折”）</li><li>将第i部分作为验证集，其余作为训练集</li><li>训练模型，计算模型在验证集上的准确率</li><li>每次用不同的部分i作为验证集，重复步骤2和3 K次</li><li>将平均准确率作为使用当前超参时的模型准确率</li><li>找到一个较好的超参数后，再用全部训练集训练模型，并在一个全新的验证集上验证，不用调超参数，达到一个较好的验证准确率时，直接去测试</li></ol></li></ul></li><li>过拟合和欠拟合<br><img src="/images/d2l/5/fitting.png"><ul><li>模型容量的影响<br>数据集复杂程度应该与模型复杂程度正相关，否则就会出现过拟合与欠拟合。举例来说，当模型很复杂而数据很简单时，模型可以直接就把所有数据记住而丧失泛化能力；而模型过于简单时，如感知机模型，无法正确划分异或的数据<br>模型足够复杂时，有其他手段减少过拟合；模型太简单没前途<br><img src="/images/d2l/5/capacity.png"></li></ul></li><li>估计模型容量<br>模型种类确定时（如神经网络），模型容量由两个因素估计：参数个数、参数取值范围 </li><li>数据复杂度<br>有多个重要因素：<ul><li>样本个数</li><li>特征个数</li><li>时间、空间结构</li><li>多样性</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Modle-Selection&quot;&gt;&lt;a href=&quot;#Modle-Selection&quot; class=&quot;headerlink&quot; title=&quot;Modle Selection&quot;&gt;&lt;/a&gt;Modle Selection&lt;/h1&gt;&lt;h2 id=&quot;Basic-Knowled</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Perceptron</title>
    <link href="http://example.com/2022/01/29/d2l_4/"/>
    <id>http://example.com/2022/01/29/d2l_4/</id>
    <published>2022-01-29T06:17:40.810Z</published>
    <updated>2022-01-29T06:17:40.810Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>感知机  <ul><li>模型<br>感知机只比线性分类多了一个激活函数，激活函数为单层感知机带来了分类能力，为多层感知机带来了非线性因素<br><img src="/images/d2l/4/perceptron.png"></li><li>训练<br>训练感知机等价于批量大小为1的梯度下降，按顺序逐个取样本，与随机梯度下降不同<br><img src="/images/d2l/4/train_p.png"></li><li>单层感知机无法解决异或问题，他只能产生线性分割面，这导致了第一次AI寒冬</li></ul></li><li>多层感知机<ul><li>多层感知机由多个感知机组成，分为输入层、隐藏层、输出层，层内不连接，层间全连接<br><img src="/images/d2l/4/mlp.png"></li><li>每个感知机输出后要经过一个非线性的激活函数，否则多层感知机等价于单层感知机</li><li>常用激活函数：Sigmoiod、Tanh、ReLU，性能都没太大区别，ReLU计算更容易，如果没有特别的想法，用ReLU就行</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><br>train_iter,_=LoadData(<span class="hljs-number">256</span>)<br>num_in,num_out,num_h=<span class="hljs-number">784</span>,<span class="hljs-number">10</span>,<span class="hljs-number">256</span><br><span class="hljs-comment"># 隐层参数</span><br>w1=torch.randn(num_in,num_h,requires_grad=<span class="hljs-literal">True</span>)<br>b1=torch.zeros(num_h,requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 输出层参数</span><br>w2=torch.randn(num_h,num_out,requires_grad=<span class="hljs-literal">True</span>)<br>b2=torch.zeros(num_out,requires_grad=<span class="hljs-literal">True</span>)<br>params=[w1,b2,w1,b2]<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.SGD(params,lr=<span class="hljs-number">0.001</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReLU</span>(<span class="hljs-params">X</span>):</span><br>    a=torch.zeros_like(X)<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">max</span>(X,a)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Net</span>(<span class="hljs-params">X:Tensor</span>):</span><br>    X=X.view(-<span class="hljs-number">1</span>,num_in)<br>    <span class="hljs-comment"># @被重载为了矩阵乘法</span><br>    X=ReLU(X@w1+b1)<br>    <span class="hljs-keyword">return</span> X@w2+b2<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Train</span>():</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>        loss=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            X=X.view(-<span class="hljs-number">1</span>,<span class="hljs-number">784</span>)<br>            out=Net(X)<br>            l=loss_f(out,y)<br><br>            l.backward()<br>            opt.step()<br>            opt.zero_grad()<br>            loss=l.item()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch}</span>,<span class="hljs-subst">{loss}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Perceptron&quot;&gt;&lt;a href=&quot;#Perceptron&quot; class=&quot;headerlink&quot; title=&quot;Perceptron&quot;&gt;&lt;/a&gt;Perceptron&lt;/h1&gt;&lt;h2 id=&quot;Basic-Knowledge&quot;&gt;&lt;a href=&quot;#Basic-</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Softmax Regression</title>
    <link href="http://example.com/2022/01/29/d2l_3/"/>
    <id>http://example.com/2022/01/29/d2l_3/</id>
    <published>2022-01-29T06:17:32.432Z</published>
    <updated>2022-01-29T06:17:32.432Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>softmax操作子<br>将输出变为一个概率分布（保证非负性与归一性）<br><img src="/images/d2l/3/softmax.png"></li><li>交叉熵损失<br>用于衡量两个概率分布的区别,将softmax输出的分布与one-hot形式的标签作为两个分布<br><img src="/images/d2l/3/cross_entropy.png"></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>数据集</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LoadData</span>(<span class="hljs-params">batch_size,resize=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-comment"># 用于把图片转为Tensor，会自动归一化</span><br>    trans=[transforms.ToTensor()]<br>    <span class="hljs-keyword">if</span> resize:<br>        trans.insert(<span class="hljs-number">0</span>,transforms.Resize(resize))<br>    trans=transforms.Compose(trans)<br>    <br>    train_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">True</span>,<br>        transform=trans,download=<span class="hljs-literal">True</span><br>    )<br>    test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        transform=trans,download=<span class="hljs-literal">True</span><br>    )<br><br>    <span class="hljs-comment"># print(test_data[0])</span><br>    <br>    <span class="hljs-keyword">return</span> (data.DataLoader(train_data,batch_size=batch_size,shuffle=<span class="hljs-literal">True</span>),<br>        data.DataLoader(test_data,batch_size=batch_size,shuffle=<span class="hljs-literal">False</span>))<br>    train_iter,test_iter=LoadData(<span class="hljs-number">256</span>) <br></code></pre></td></tr></tbody></table></figure><ul><li>从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将图片展平</span><br>num_inputs=<span class="hljs-number">1</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br><span class="hljs-comment"># 共有10类</span><br>num_outputs=<span class="hljs-number">10</span><br><br>w=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,size=(num_inputs,num_outputs),requires_grad=<span class="hljs-literal">True</span>)<br>b=torch.zeros(num_outputs,requires_grad=<span class="hljs-literal">True</span>)<br><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Softmax</span>(<span class="hljs-params">X</span>):</span><br>    <span class="hljs-comment"># 成batch分子组成的向量</span><br>    X_exp=torch.exp(X)<br>    <span class="hljs-comment"># 成batch个分母</span><br>    partition=X_exp.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>,keepdim=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 应用广播机制</span><br>    <span class="hljs-keyword">return</span> X_exp/partition<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">CrossEntropy</span>(<span class="hljs-params">y_hat,y</span>):</span><br>    <span class="hljs-comment"># 只有y为下标那一项起作用</span><br>    <span class="hljs-comment"># 按正常one_hot编码为0的项直接就没算了</span><br>    <span class="hljs-comment"># 一个batch的y_hat都取对应y中的值为下标那个</span><br>    <span class="hljs-comment"># 最终得到了整个batch中所有样本的损失</span><br>    <span class="hljs-keyword">return</span> -torch.log(y_hat[<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(y_hat)),y]).<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Net</span>(<span class="hljs-params">w,b,X</span>):</span><br>    <span class="hljs-keyword">return</span> Softmax(torch.matmul(X,w)+b)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Sgd</span>(<span class="hljs-params">params,lr</span>):</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> params:<br>            i-=lr*i.grad<br>            i.grad.zero_()<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Train</span>():</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>        loss=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            X=X.view(-<span class="hljs-number">1</span>,<span class="hljs-number">784</span>)<br>            out=Net(w,b,X)<br>            l=CrossEntropy(out,y)<br><br>            l.backward()<br>            <span class="hljs-comment"># 学习率设置这么小是因为在损失函数或优化器中</span><br>            <span class="hljs-comment"># 没有除以batch_size,导致求的梯度会很大</span><br>            Sgd([w,b],<span class="hljs-number">0.0001</span>)<br>            loss=l.item()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch}</span>,<span class="hljs-subst">{loss}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure><ul><li>简洁实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">train_iter,test_iter=LoadData(<span class="hljs-number">256</span>)<br><br><span class="hljs-comment"># 将图片展平</span><br>num_inputs=<span class="hljs-number">1</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br><span class="hljs-comment"># 共有10类</span><br>num_outputs=<span class="hljs-number">10</span><br><br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-comment"># Flatten() 保留第零维度，其他全部展平</span><br>net=nn.Sequential(nn.Flatten(),nn.Linear(num_inputs,num_outputs))<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>    loss=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>        out=net(X)<br>        l=loss_f(out,y)<br>        l.backward()<br>        opt.step()<br>        opt.zero_grad()<br>        loss=l.item()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch}</span>,<span class="hljs-subst">{loss}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Softmax-Regression&quot;&gt;&lt;a href=&quot;#Softmax-Regression&quot; class=&quot;headerlink&quot; title=&quot;Softmax Regression&quot;&gt;&lt;/a&gt;Softmax Regression&lt;/h1&gt;&lt;h2 id=&quot;B</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
</feed>
