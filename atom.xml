<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xavier&#39;s blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-03-11T14:37:46.163Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Xavier</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Git Skills</title>
    <link href="http://example.com/2022/03/11/Git/"/>
    <id>http://example.com/2022/03/11/Git/</id>
    <published>2022-03-11T14:37:46.163Z</published>
    <updated>2022-03-11T14:37:46.163Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Git-Skills"><a href="#Git-Skills" class="headerlink" title="Git Skills"></a>Git Skills</h1><ul><li><p>初始化本地git仓库  </p>  <figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git init [仓库名]<br></code></pre></td></tr></tbody></table></figure><p>  不指定仓库名时在当前文件夹下创建本地git仓库。初始化仓库后生成隐藏文件夹，包含本地仓库各种信息。</p></li><li><p>关联远端git仓库</p>  <figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git remote add &lt;远端仓库名称&gt; &lt;远端仓库地址&gt;<br></code></pre></td></tr></tbody></table></figure><p>  远端仓库名称随便起，供本地仓库之后使用，可以关联多个。</p></li><li><p>创建/切换分支</p>  <figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git branch &lt;分支名&gt; <br>git switch &lt;分支名&gt;<br></code></pre></td></tr></tbody></table></figure><p>  本地分支与远端仓库分支一一对应。<br>  向远端仓库push时如果远端仓库没有创建对应分支，则应先创建同名分支</p>  <figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git push --set-upstream origin &lt;分支名&gt;<br></code></pre></td></tr></tbody></table></figure></li><li><p>拉取远端仓库到本地</p>  <figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git pull [远程主机名] [远程分支名] <br>git fetch [远程主机名] [远程分支名] <br></code></pre></td></tr></tbody></table></figure><p>  不指定远端主机名和远端分支名时，拉取默认远端主机的同名分支的代码。<br>  git pull会将代码直接与本地仓库合并<br>  git fetch 不会直接合并，需要手动合并</p></li><li><p>将本地仓库推送至远端仓库</p>  <figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git push [远程主机名] [远程分支名]<br></code></pre></td></tr></tbody></table></figure><p>  不指定远端主机名和远端分支名时，推送至默认远端主机的同名分支。 </p></li><li><p>快速关联github与本地仓库</p><ol><li>在github新建仓库</li><li>在本地git clone 新建的仓库</li><li>此时本地仓库已经与远端仓库自动关联，且具有相同分支</li></ol></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Git-Skills&quot;&gt;&lt;a href=&quot;#Git-Skills&quot; class=&quot;headerlink&quot; title=&quot;Git Skills&quot;&gt;&lt;/a&gt;Git Skills&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初始化本地git仓库  &lt;/p&gt;
  &lt;figure cl</summary>
      
    
    
    
    <category term="notes" scheme="http://example.com/categories/notes/"/>
    
    
    <category term="git" scheme="http://example.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>You Only Look Once #2</title>
    <link href="http://example.com/2022/03/09/YOLOv2/"/>
    <id>http://example.com/2022/03/09/YOLOv2/</id>
    <published>2022-03-09T07:05:44.589Z</published>
    <updated>2022-03-09T07:05:44.589Z</updated>
    
    <content type="html"><![CDATA[<h1 id="YOLO9000-Better-Faster-Stronger"><a href="#YOLO9000-Better-Faster-Stronger" class="headerlink" title="YOLO9000: Better, Faster, Stronger"></a>YOLO9000: Better, Faster, Stronger</h1><h2 id="Improvement"><a href="#Improvement" class="headerlink" title="Improvement"></a><strong>Improvement</strong></h2><ul><li><p>Yolov1还存在着诸多缺陷：</p><ul><li>相比于其他两阶段目标检测算法，Yolov1虽然速度更快，但准确率更低</li><li>将图片中全部目标检出的能力较差，尤其是密集且小的物体</li><li>Yolov1对目标定位的能力更差</li></ul></li><li><p>Yolov2对上一版本进行了许多改进，并取得了不错的效果</p><ul><li>改进效果主要体现在：Better（更准确）、Faster（更快）、Stronger（能检测更多种类的目标）</li><li>改进方法主要有：Batch Normalization、High Resolution classifier、Anchor、Dimension、Direct Location、Fine-grained Features、Multi-scale Training</li></ul></li></ul><h2 id="Prime-Algorithm"><a href="#Prime-Algorithm" class="headerlink" title="Prime Algorithm"></a><strong>Prime Algorithm</strong></h2><ul><li>Batch Normalization</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;YOLO9000-Better-Faster-Stronger&quot;&gt;&lt;a href=&quot;#YOLO9000-Better-Faster-Stronger&quot; class=&quot;headerlink&quot; title=&quot;YOLO9000: Better, Faster, Stro</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Computer Vision" scheme="http://example.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Asynchronous programming</title>
    <link href="http://example.com/2022/03/08/AsyncAwait/"/>
    <id>http://example.com/2022/03/08/AsyncAwait/</id>
    <published>2022-03-08T13:19:10.111Z</published>
    <updated>2022-03-08T13:19:10.111Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Asynchronous-programming"><a href="#Asynchronous-programming" class="headerlink" title="Asynchronous programming"></a>Asynchronous programming</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a>Basic Knowledge</h2><ul><li><p>异步编程与多线程编程</p><ul><li>异步编程一般在单线程（或线程池）上实现并发执行，不涉及线程切换，减小了维护多线程的开销。而多线程编程在多核处理器上做并行执行，需要考虑线程间同步以及线程切换等问题</li><li>异步编程适合于I/O密集型操作，而多线程编程适合于计算密集型工作。这是因为在I/O过程中线程会被阻塞但依然要维护其占用的内存资源，并且还有进行线程切换开销；而异步编程则避免了这些问题，将I/O操作封装为异步函数，cpu在执行到I/O操作时向DMA发送指令后直接执行其他代码，当I/O操作结束后执行回调</li></ul></li><li><p>async/await 结构</p><ul><li>async与await被许多语言都设置为了异步编程的语法糖，有多种实现（如python中的coroutine、c#中的Task、js中的promise）。</li><li>async/await 结构可分成三部分：<ol><li>调用方法：该方法调用异步方法，在异步方法执行其任务的时候继续执行该方法下其他代码</li><li>异步方法：在执行完成前立即返回调用方法，在调用方法继续执行的过程中完成任务</li><li>await 表达式：用于异步方法内部，指出需要异步执行的任务(在await之前的代码还都是同步执行)。一个异步方法可以包含多个 await 表达式，当异步方法中不包含await表达式时，将会同步执行（顺序执行）异步方法</li></ol></li></ul></li><li><p>async/await执行过程</p><ol><li>调用方法执行到由async修饰的异步方法，进入该方法先同步执行</li><li>顺序执行到await修饰的语句，调用该语句后立即返回原调用方法</li><li>在原调用方法中继续执行后面的代码，同时异步方法也在执行await的语句</li></ol></li><li><p>注意</p><ul><li>调用方法和异步方法可能是并行的，也可能是并发的，这方面不需要程序员考虑，只需要知道异步方法的执行不影响调用方法的执行</li><li>异步方法正常的返回值并不是方法内指明的返回值（python中返回coroutine、c#中返回Task、js中返回promise），但用await修饰会直接返回异步方法内指明的返回值</li><li>如果异步方法还未执行完，而在调用方法中就要使用异步方法的result，则会死等到异步方法执行完毕</li><li>异步方法一般回调联合使用</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><ul><li><p>c#中实现的异步编程基于Task，底层是线程池</p>  <figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><br>App app = <span class="hljs-keyword">new</span> App();<br>app.Run();<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">NeuralNetwork</span><br>{<br>    <span class="hljs-keyword">public</span> <span class="hljs-built_in">double</span> Acc { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }<br>    <span class="hljs-keyword">public</span> <span class="hljs-built_in">double</span> Loss { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-built_in">string</span> _name;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">NeuralNetwork</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> name</span>)</span><br>    {<br>        _name = name;<br>    }<br><br>    <span class="hljs-comment">//训练结束后调用回调函数</span><br>    <span class="hljs-comment">//由于是异步方法，其不会阻塞界面线程</span><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> <span class="hljs-keyword">void</span> <span class="hljs-title">TrainAsync</span>(<span class="hljs-params">Action&lt;<span class="hljs-built_in">double</span>, <span class="hljs-built_in">double</span>&gt; action</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">"我之前还是同步执行"</span>);<br>        <span class="hljs-keyword">await</span> Task.Delay(<span class="hljs-number">3000</span>);<br>        Acc = <span class="hljs-number">0.9f</span>;<br>        Loss = <span class="hljs-number">0.00466f</span>;<br>        action(Acc, Loss);<br>    }<br><br>    <span class="hljs-comment">//虽然也是异步方法，但方法完成后不会通知调用方法，在此场景下还是低效</span><br>    <span class="hljs-comment">//在不需要回调的情境下可以使用</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task&lt;Tuple&lt;<span class="hljs-built_in">double</span>, <span class="hljs-built_in">double</span>&gt;&gt; TrainAsync()<br>    {<br>        Console.WriteLine(<span class="hljs-string">"我之前还是同步执行"</span>);<br><br>        <span class="hljs-keyword">await</span> Task.Delay(<span class="hljs-number">3000</span>);<br>        Acc = <span class="hljs-number">0.9f</span>;<br>        Loss = <span class="hljs-number">0.00466f</span>;<br><br>        <span class="hljs-keyword">return</span> Tuple.Create(Acc, Loss);<br>    }<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">App</span><br>{<br>    <span class="hljs-keyword">private</span> NeuralNetwork _network = <span class="hljs-keyword">new</span> NeuralNetwork(<span class="hljs-string">"Cnn"</span>);<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">RunWithCallBack</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">30</span>; i++)<br>        {<br>            Thread.Sleep(<span class="hljs-number">500</span>);<br>            Console.WriteLine(<span class="hljs-string">"running..."</span>);<br><br>            <span class="hljs-comment">//模拟训练模型，需要长时间操作</span><br>            <span class="hljs-keyword">if</span> (i == <span class="hljs-number">3</span>)<br>                <span class="hljs-comment">//进入到函数中，执行到调用await后立即返回到RunWithCallBack继续执行</span><br>                _network.TrainAsync(<span class="hljs-keyword">this</span>.Show);<br>        }<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Run</span>(<span class="hljs-params"></span>)</span><br>    {<br>        Task&lt;Tuple&lt;<span class="hljs-built_in">double</span>, <span class="hljs-built_in">double</span>&gt;&gt; t = <span class="hljs-literal">null</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">30</span>; i++)<br>        {<br><br>            Thread.Sleep(<span class="hljs-number">500</span>);<br>            Console.WriteLine(<span class="hljs-string">"running..."</span>);<br>            <br><br>            <span class="hljs-comment">//模拟训练模型，需要长时间操作</span><br>            <span class="hljs-keyword">if</span> (i == <span class="hljs-number">3</span>)<br>            {<br>                <span class="hljs-comment">//如果在这里直接输出，则依然会产生阻塞的效果</span><br>                <span class="hljs-comment">//Console.WriteLine(_network.TrainAsync().Result);</span><br><br>                <span class="hljs-comment">//使用await，await返回的直接就是_network.TrainAsync().Result</span><br>                t = _network.TrainAsync();<br><br>            }<br><br>            <span class="hljs-comment">//在第十次刷新时检查是否训练完成，没完成继续等</span><br>            <span class="hljs-keyword">if</span> (i == <span class="hljs-number">10</span>)<br>                Console.WriteLine(t.Result);<br><br>        }<br><br><br>    }<br><br>    <span class="hljs-comment">//界面展示精度与损失</span><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Show</span>(<span class="hljs-params"><span class="hljs-built_in">double</span> acc, <span class="hljs-built_in">double</span> loss</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"acc:<span class="hljs-subst">{acc}</span>,loss:<span class="hljs-subst">{loss}</span>"</span>);<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure></li><li><p>python实现的异步编程使用了协程概念，与JS一样底层都是单线程</p>  <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#asyncio包帮助实现异步编程</span><br><span class="hljs-comment">#将所有异步和同步函数加入一个事件循环，在这个循环内按异步定义为每个函数分配时间片</span><br><span class="hljs-keyword">import</span> asyncio<br><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">RunNN</span>(<span class="hljs-params">f</span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"===="</span>)<br>    <span class="hljs-keyword">await</span> asyncio.sleep(<span class="hljs-number">2</span>)<br>    <br>    loss=<span class="hljs-number">0.0045</span><br>    acc=<span class="hljs-number">0.97</span><br>    f(loss,acc)<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">app</span>():</span><br>    <span class="hljs-keyword">while</span>(<span class="hljs-literal">True</span>):<br>        <span class="hljs-keyword">await</span> asyncio.sleep(<span class="hljs-number">0.5</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">"running..."</span>)<br>        <br><br>asyncio.gather(app(), RunNN(<span class="hljs-keyword">lambda</span> x, y: <span class="hljs-built_in">print</span>(<span class="hljs-string">f"loss:<span class="hljs-subst">{x}</span>,acc<span class="hljs-subst">{y}</span>"</span>)))<br><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    <span class="hljs-keyword">await</span> asyncio.gather(app(), RunNN(<span class="hljs-keyword">lambda</span> x, y: <span class="hljs-built_in">print</span>(<span class="hljs-string">f"loss:<span class="hljs-subst">{x}</span>,acc<span class="hljs-subst">{y}</span>"</span>)))<br><br>asyncio.run(main())<br><br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Asynchronous-programming&quot;&gt;&lt;a href=&quot;#Asynchronous-programming&quot; class=&quot;headerlink&quot; title=&quot;Asynchronous programming&quot;&gt;&lt;/a&gt;Asynchronous p</summary>
      
    
    
    
    <category term="Parallelism and concurrency" scheme="http://example.com/categories/Parallelism-and-concurrency/"/>
    
    
    <category term="Asynchronous-programming" scheme="http://example.com/tags/Asynchronous-programming/"/>
    
  </entry>
  
  <entry>
    <title>MultiThreading programming #3</title>
    <link href="http://example.com/2022/03/08/MultiThread3/"/>
    <id>http://example.com/2022/03/08/MultiThread3/</id>
    <published>2022-03-08T11:11:15.016Z</published>
    <updated>2022-03-08T11:11:15.016Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Asynchronous-programming-3"><a href="#Asynchronous-programming-3" class="headerlink" title="Asynchronous programming #3"></a>Asynchronous programming #3</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><h3 id="Use-Thread-in-Applications"><a href="#Use-Thread-in-Applications" class="headerlink" title="Use Thread in Applications"></a><strong>Use Thread in Applications</strong></h3><ul><li>在带界面的WPF、UWP、WinForm等程序种，若主线程执行耗时的操作，就会导致整个程序无响应。因为主线程同时还要处理消息循环，而渲染和鼠标键盘事件处理等工作都是在消息循环中执行的</li><li>针对这种耗时的操作，一种流行的做法使启用一个worker线程，执行完操作后再更新到UI</li><li>富客户端应用 的线程模型通常是：<ul><li>UI控件只能从创建他们的线程来进行访问（通常是主线程）</li><li>当想从worker线程更新UI时，应该把请求交给UI线程</li></ul></li></ul><h3 id="Synchronization-Contexts"><a href="#Synchronization-Contexts" class="headerlink" title="Synchronization Contexts"></a><strong>Synchronization Contexts</strong></h3><ul><li>在System.ComponentModel下有一个抽象类：SynchronizationContext，它使得Thread Marshaling得到泛化<ul><li>Thread Marshaling：把一些数据的所有权从一个线程交给了另一个线程</li></ul></li></ul><h3 id="Thread-Pool"><a href="#Thread-Pool" class="headerlink" title="Thread Pool"></a><strong>Thread Pool</strong></h3><ul><li><p>当开始一个线程时，将花费数百微秒来组织一些内容（如一个新的局部变量栈），产生了开销</p></li><li><p>线程池可以节省这些开销：</p><ul><li>预先创建一个可循环使用的线程的池来减少创建新线程的开销</li></ul></li><li><p>线程池对于搞笑的并行编程和细粒度的并发是必不可少的</p></li><li><p>关于c#中的线程池需要注意以下几点：</p><ul><li>不可以设置池线程的<code>Name</code></li><li>池线程都是后台线程</li><li>阻塞池线程可能使性能降低</li><li>池线程优先级可以被自由的更改，当它被释放回池的时候优先级将被还原为正常状态</li><li>可以通过<code>IsThreadPoolThread</code>属性来判断是否为池线程</li><li><code>Task</code>也使用线程池</li></ul></li><li><p>线程池中的整洁</p><ul><li>线程池提供了另一个功能，即确保不会产生CPU超额订阅（活跃的线程数超过CPU核数），超额订阅对性能影响很大</li><li>CLR通过对任务排队并对其启动进行节流限制来避免线程池中的超额订阅</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Asynchronous-programming-3&quot;&gt;&lt;a href=&quot;#Asynchronous-programming-3&quot; class=&quot;headerlink&quot; title=&quot;Asynchronous programming #3&quot;&gt;&lt;/a&gt;Asynchr</summary>
      
    
    
    
    <category term="Parallelism and concurrency" scheme="http://example.com/categories/Parallelism-and-concurrency/"/>
    
    
    <category term="MultiThreading-programming" scheme="http://example.com/tags/MultiThreading-programming/"/>
    
  </entry>
  
  <entry>
    <title>MultiThreading programming #2</title>
    <link href="http://example.com/2022/03/08/MultiThread2/"/>
    <id>http://example.com/2022/03/08/MultiThread2/</id>
    <published>2022-03-08T11:09:08.108Z</published>
    <updated>2022-03-08T11:09:08.108Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MultiThreading-programming-2"><a href="#MultiThreading-programming-2" class="headerlink" title="MultiThreading programming #2"></a>MultiThreading programming #2</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><h3 id="Thread-Safety"><a href="#Thread-Safety" class="headerlink" title="Thread Safety"></a><strong>Thread Safety</strong></h3><ul><li>本地状态与共享状态<ul><li>Local 本地独立：CLR为每个线程分配自己的内存栈，以便使本地变量保持独立</li><li>Shared 共享：如果多个线程引用到了同一个对象实例，那么他们就共享了数据；被Lambad表达式或匿名委托捕获的本地变量，会被编译器转化为字段（field），所以也会被共享；静态字段也会在线程间共享</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-comment">/*本地变量不共享*/</span><br>        <span class="hljs-comment">//打印六次hello...</span><br>        Thread thread = <span class="hljs-keyword">new</span> Thread(GO);<br>        thread.Start();<br>        GO();<br>        <span class="hljs-comment">/*同一实例共享*/</span><br>        <span class="hljs-comment">//打印一次Done！</span><br>        ThreadTest test = <span class="hljs-keyword">new</span> ThreadTest();<br>        <span class="hljs-keyword">new</span> Thread(test.Test).Start();<br>        Thread.Sleep(<span class="hljs-number">1000</span>);<br>        test.Test();<br>        <span class="hljs-comment">/*匿名委托共享*/</span><br>        <span class="hljs-comment">//打印一次Done</span><br>        <span class="hljs-built_in">bool</span> done = <span class="hljs-literal">false</span>;<br>        ThreadStart action = () =&gt;<br>            {<br>                <span class="hljs-keyword">if</span> (!done)<br>                {<br>                    <span class="hljs-comment">//此处还是会有输出多次的风险</span><br>                    done = <span class="hljs-literal">true</span>;<br>                    System.Console.WriteLine(<span class="hljs-string">"Done"</span>);<br>                }<br>            };<br>        <span class="hljs-keyword">new</span> Thread(action).Start();<br>        Thread.Sleep(<span class="hljs-keyword">new</span> TimeSpan(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>));<br>        action.Invoke();<br>        <span class="hljs-comment">/*静态字段共享*/</span><br>        <span class="hljs-comment">//打印一次Done！</span><br>        ThreadTest test1 = <span class="hljs-keyword">new</span> ThreadTest();<br>        ThreadTest test2 = <span class="hljs-keyword">new</span> ThreadTest();<br>        <span class="hljs-keyword">new</span> Thread(test1.AnotherTest).Start();<br>        Thread.Sleep(<span class="hljs-number">10</span>);<br>        test2.AnotherTest();<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">GO</span>(<span class="hljs-params"></span>)</span><br>    {   <br>        <span class="hljs-comment">//cycle是本地变量，属于本地状态</span><br>        <span class="hljs-comment">//在每个线程的内存栈上，都会创建cycle的 独立副本</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> cycle = <span class="hljs-number">0</span>;  cycle&lt; <span class="hljs-number">3</span>; cycle++)<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"hello..."</span>);<br>        }<br>    }<br><br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">ThreadTest</span><br>{<br>    <span class="hljs-keyword">private</span> <span class="hljs-built_in">bool</span> _done;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-built_in">bool</span> done;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">AnotherTest</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-comment">//此处还是会有输出多次的风险</span><br>        <span class="hljs-keyword">if</span>(!done)<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"DONE!"</span>);<br>            done = <span class="hljs-literal">true</span>;<br>        }<br>    }<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Test</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-comment">//此处还是会有输出多次的风险</span><br>        <span class="hljs-keyword">if</span>(!_done)<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"Done!"</span>);<br>            _done = <span class="hljs-literal">true</span>;<br>        }<br>    }<br><br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>线程安全<ul><li>上述后涉及线程共享数据的代码是缺乏线程安全的，其实际输出无法确定，理论上Done有可能会打印两次，因为一个线程可能正在评估if，而另一个语句没来得及调整<code>done</code>为<code>true</code></li><li>保证线程安全：消除代码执行过程中的不确定性<ol><li>尽可能的避免使用共享状态以保证线程安全</li><li>使用<code>lock</code>语句加锁，在读取和写入共享数据的时候，通过使用互斥锁，就可以修复前面代码中的问题，当两个线程同时竞争一个锁的时候（锁可以基于任何引用类型对象），一个线程会等待或阻塞，直到锁重新变成可用状态</li></ol></li><li>然而，lock也并非线程安全的银弹，lock也会引起一些其他问题（如死锁）</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <span class="hljs-keyword">static</span> <span class="hljs-keyword">readonly</span> <span class="hljs-built_in">object</span> locker=<span class="hljs-keyword">new</span> <span class="hljs-built_in">object</span>();<br>    <span class="hljs-keyword">static</span> <span class="hljs-built_in">bool</span> done;<br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-keyword">new</span> Thread(GO).Start();<br>        GO();<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">GO</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-comment">// 锁要基于引用类型的变量</span><br>        <span class="hljs-comment">// 线程安全</span><br>        <span class="hljs-keyword">lock</span>(locker)<br>        {<br>            <span class="hljs-keyword">if</span> (!done)<br>            {<br>                System.Console.WriteLine(<span class="hljs-string">"Done"</span>);<br>                Thread.Sleep(<span class="hljs-number">1000</span>);<br>                done = <span class="hljs-literal">true</span>;<br>            }<br>        }<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="Transfer-Parameter"><a href="#Transfer-Parameter" class="headerlink" title="Transfer Parameter"></a><strong>Transfer Parameter</strong></h3><ul><li>如果想往线程启动方法里传递参数，最简单的方式就是使用lambda表达式，在里面使用参数调用方法</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>    {<br>        <br>        <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>        {<br>            <span class="hljs-keyword">new</span> Thread(() =&gt; { Print(<span class="hljs-string">"hello world"</span>); }).Start();<br><br>        }<br><br>        <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Print</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> s</span>)</span><br>        {<br>            System.Console.WriteLine(s);<br>        }<br>    }<br></code></pre></td></tr></tbody></table></figure><ul><li>还可以使用<code>Thread.Start</code>方法来传递参数，类似委托中<code>Invoke</code>时的传参<ul><li>Thread的重载构造函数可以接受下列两个委托之一作为参数：<ol><li><code>public delegate void ThreadStart();</code></li><li><code>public delegate void ParameterizedThreadStart(object obj);</code></li></ol></li><li>第二种委托接收带参数的方法名</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        Thread thread = <span class="hljs-keyword">new</span> Thread(Print);<br>        thread.Start(<span class="hljs-string">"hello world"</span>);<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Print</span>(<span class="hljs-params"><span class="hljs-built_in">object</span> s</span>)</span><br>    {<br>        s = s <span class="hljs-keyword">as</span> <span class="hljs-built_in">string</span>;<br>        System.Console.WriteLine(s);<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>需要注意的是lambda表达式（匿名委托）传的参数会当作字段，即就算传的是值类型的变量也会得到其地址并被线程共享；而第二种传参强制要求了引用变量，也会被线程共享</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{  <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-comment">// Thread1();</span><br>        Thread2();<br>    }<br><br>    <span class="hljs-comment">// 线程共享了同一个局部变量i</span><br>    <span class="hljs-comment">// 会出现重复的数</span><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Thread1</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++)<br>        {<br>            <span class="hljs-keyword">new</span> Thread(() =&gt; { System.Console.WriteLine(i); }).Start();<br>        }<br>    }<br>    <span class="hljs-comment">// 每个线程获得不同局部变量的地址</span><br>    <span class="hljs-comment">// 不会出现重复的数</span><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Thread2</span>(<span class="hljs-params"></span>)</span><br>    {<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++)<br>        {<br>            <span class="hljs-built_in">int</span> temp = i;<br>            <span class="hljs-keyword">new</span> Thread(() =&gt; { System.Console.WriteLine(temp); }).Start();<br>        }<br>    }<br>    <br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="Exception"><a href="#Exception" class="headerlink" title="Exception"></a><strong>Exception</strong></h3><ul><li>异常处理块种的线程抛出异常时，不会被捕获，解决方案是传入线程的方法中处理异常</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{  <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-comment">// 正常捕获</span><br>        <span class="hljs-keyword">new</span> Thread(Go2).Start();<br>        <span class="hljs-comment">// 无法捕获</span><br>        <span class="hljs-keyword">try</span><br>        {<br>            <span class="hljs-keyword">new</span> Thread(Go1).Start();<br>        }<br>        catch<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"Exceptoin!"</span>);<br>        }<br>    }   <br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Go1</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> Exception();<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Go2</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">try</span><br>        {<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> Exception();<br>        }<br><br>        catch<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"catch sucessfully"</span>);<br>        }<br>    }<br>    <br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="Foreground-Threads-and-Background-Threads"><a href="#Foreground-Threads-and-Background-Threads" class="headerlink" title="Foreground Threads and Background Threads"></a><strong>Foreground Threads and Background Threads</strong></h3><ul><li>默认情况下，手动创建的线程就是前台线程</li><li>只要有前台线程在运行，那么应用程序就会一直处于活动状态<ul><li>后台线程运行不会保持应用程序的活动状态</li><li>一旦所有前台线程停止，应用程序随即停止，后台线程也会立即终止</li></ul></li><li>线程的前台、后台与它的优先级无关</li><li>通过<code>IsBackground</code>属性判断线程是否是后台线程</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{  <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        Thread thread = <span class="hljs-keyword">new</span> Thread(() =&gt; { Console.WriteLine(Console.ReadLine());});<br>        <br>        <span class="hljs-comment">// 如果将thread设置为后台线程，当主线程（前台线程）执行完之后程序立即结束</span><br>        <span class="hljs-keyword">if</span>(args.Length&gt;<span class="hljs-number">0</span>)<br>            thread.IsBackground = <span class="hljs-literal">true</span>;<br><br>        thread.Start();<br>    }   <br>    <br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>如果在退出前想要等待后台线程执行完毕，可以考虑使用<code>Join</code></li><li>应用程序无法正常退出的一个常见原因就是还有活跃的前台线程</li></ul><h3 id="Priority-of-Threading"><a href="#Priority-of-Threading" class="headerlink" title="Priority of Threading"></a><strong>Priority of Threading</strong></h3><ul><li>线程的优先级（<code>Thread</code>中<code>Priority</code>属性）决定了相对于操作系统中其他活跃线程所占的执行时间</li><li>优先级划分：<ul><li><code>enum ThreadPriority{Lowest,BelowNormal,Normal,AboveNormal,Highest}</code></li></ul></li><li>提升线程优先级：<ul><li>提升线程优先级时要特别注意，因为他可能饿死其他线程</li><li>如果想让某线程的优先级比其他进程中的线程优先级高，那么就必须提升进程优先级</li></ul></li><li>手动提升进程/线程优先级适用于只做少量工作且需要较低延迟的非UI进程</li><li>对于需要大量计算的应用程序（尤其是带UI的），手动提升进程/线程优先级可能会使其他进程/线程饿死，从而降低整个计算机的速度</li></ul><h3 id="Signaling"><a href="#Signaling" class="headerlink" title="Signaling"></a><strong>Signaling</strong></h3><ul><li>某个线程在收到其他线程发来通知之前一直处于等待状态，发送通知的过程就称为Signaling，不同于信号量机制</li><li>最简单的信号结构就是<code>ManualResetEvent</code><ul><li>调用其上<code>WaitOne</code>方法会阻塞当前线程，直到另一个线程通过调用<code>Set</code>方法开启信号</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{ <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-keyword">var</span> signal = <span class="hljs-keyword">new</span> ManualResetEvent(<span class="hljs-literal">false</span>);<br><br>        <span class="hljs-keyword">new</span> Thread(() =&gt;<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"Waiting for signal..."</span>);<br>            <span class="hljs-comment">// 因为在实例化时设置了false主线程打开信号之前处于阻塞状态</span><br>            <span class="hljs-comment">// 成功接收到信号时不会将signal状态重置为nonusignaled</span><br>            signal.WaitOne();<br>            <span class="hljs-comment">// 获得信号后直接关闭该信号量</span><br>            System.Console.WriteLine(<span class="hljs-string">"Got signal!"</span>);<br>        }).Start();<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++)<br>        {<br>            System.Console.WriteLine(i);<br>        }<br>        Thread.Sleep(<span class="hljs-number">1000</span>);<br>        signal.Set();<br>    }   <br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>调用完<code>Set</code>后信号会处于signaled状态，通过调用<code>Reset</code>将信号重新变为nonsignaled状态</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MultiThreading-programming-2&quot;&gt;&lt;a href=&quot;#MultiThreading-programming-2&quot; class=&quot;headerlink&quot; title=&quot;MultiThreading programming #2&quot;&gt;&lt;/a&gt;M</summary>
      
    
    
    
    <category term="Parallelism and concurrency" scheme="http://example.com/categories/Parallelism-and-concurrency/"/>
    
    
    <category term="MultiThreading-programming" scheme="http://example.com/tags/MultiThreading-programming/"/>
    
  </entry>
  
  <entry>
    <title>MultiThreading programming #1</title>
    <link href="http://example.com/2022/03/08/MultiThread1/"/>
    <id>http://example.com/2022/03/08/MultiThread1/</id>
    <published>2022-03-08T11:09:04.906Z</published>
    <updated>2022-03-08T11:09:04.906Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MultiThreading-programming-1"><a href="#MultiThreading-programming-1" class="headerlink" title="MultiThreading programming #1"></a>MultiThreading programming #1</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><hr><h3 id="What-is-Thread"><a href="#What-is-Thread" class="headerlink" title="What is Thread ?"></a><strong>What is Thread ?</strong></h3><ul><li>线程是一个可执行路径，每一个线程可以独立于其他线程执行</li><li>每个线程在均进程(Process)内执行，在操作系统中，进行提供了程序运行的独立环境</li><li>单线程应用，在进程的独立环境中只跑一个线程，该线程具有独占权</li><li>多线程应用，单个进程中跑多个线程，多个线程共享当前的执行环境（尤其是内存）<ul><li>共享：多个线程共同占有某种资源，如一个线程在后台读取数据，另一个线程在数据到达后进行展示。  </li></ul></li></ul><p>下面是c#中最简单的异步编程实例：</p><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        Thread.CurrentThread.Name = <span class="hljs-string">"Main Thread..."</span>;<br>        <span class="hljs-comment">//开辟一个新线程            </span><br>        Thread thread = <span class="hljs-keyword">new</span> Thread(WriteY);<br>        thread.Name = <span class="hljs-string">"Y Thread..."</span>;<br>        thread.Start();<br>        Thread.Sleep(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//同时在主线程上做一些工作</span><br>        System.Console.WriteLine(Thread.CurrentThread.Name);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">1000</span>; i++)<br>        {<br>            System.Console.Write(<span class="hljs-string">"x"</span>);<br>        }<br>        <br><br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">WriteY</span>(<span class="hljs-params"></span>)</span><br>    {<br>        System.Console.WriteLine(Thread.CurrentThread.Name);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">1000</span>; i++)<br>        {<br>            System.Console.Write(<span class="hljs-string">"y"</span>);<br>        }<br>        <br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>在单核计算机上，操作系统必须为每个线程分配<strong>时间片</strong>（Windows下通常为20ms）来模拟并发，从而在本例中，会输出重复的x块与y块</li><li>在多核或多处理器的计算机上，使用c#创建的多线程可以真正意义上并行执行。然而，在本例中由于控制台程序处理并发请求机制的微妙性，仍然会得到重复的x块与y块<br><img src="/images/CSAsynchronousProgramming/first_example.png" alt="first_example"></li><li>c#中线程的一些属性：<ul><li>线程一旦开始执行，属性<code>IsAlive</code>就变为<code>true</code>，线程结束就变为<code>false</code></li><li>线程结束的条件：线程构造器中传入的委托结束了执行</li><li>线程一旦结束，便无法重启</li><li>每个线程都有一个<code>Name</code>属性，通常用于调试，<code>Name</code>只能设置一次，多次更改会抛出异常</li><li>静态属性<code>Thread.CurrentThread</code>，指向当前执行的线程</li></ul></li></ul><h3 id="Join-and-Sleep"><a href="#Join-and-Sleep" class="headerlink" title="Join and Sleep"></a><strong>Join and Sleep</strong></h3><ul><li>在线程A中调用另一个线程B实例的<code>Join</code>方法，线程A便会等待线程B执行结束后继续执行。</li><li>调用<code>Jion</code>方法时，可在参数中设置一个超时(毫秒/<code>TimeSpan</code>)<ul><li>使用有超时的重载方法时返回值<code>bool</code>类型，<code>true</code>：线程结束；<code>false</code>：超时</li></ul></li><li><code>Thread.Sleep</code>方法会暂停当前的线程，并等待一段时间<ul><li><code>Thread.Sleep(0)</code>会导致县城立即放弃本身当前时间片，自动将cpu转交给其他线程</li><li><code>Thread.Yield</code>与<code>Thread.Sleep</code>做同样的事情，不同的是<code>Thread.Yield</code>只会把执行交给同一处理器的其他线程</li><li>当等待<code>Sleep</code>和<code>Join</code>时，线程处于阻塞状态</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>    {<br>        <span class="hljs-keyword">static</span> Thread thread1;<br>        <span class="hljs-keyword">static</span> Thread thread2;<br>        <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>        {<br>            thread1 = <span class="hljs-keyword">new</span> Thread(ThreadProc);<br>            thread1.Name = <span class="hljs-string">"t1"</span>;<br>            thread1.Start();<br><br>            thread2 = <span class="hljs-keyword">new</span> Thread(ThreadProc);<br>            thread2.Name = <span class="hljs-string">"t2"</span>;<br>            thread2.Start();<br>        }<br><br><br>        <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">ThreadProc</span>(<span class="hljs-params"></span>)</span><br>        {<br>            System.Console.WriteLine(<span class="hljs-string">$"\nCurrent thread:<span class="hljs-subst">{Thread.CurrentThread.Name}</span>"</span>);<br>            <span class="hljs-keyword">if</span>(Thread.CurrentThread.Name==<span class="hljs-string">"t1"</span>&amp;&amp;<br>            ((thread2.ThreadState&amp;ThreadState.Unstarted)==<span class="hljs-number">0</span>))<br>                <span class="hljs-keyword">if</span>(thread2.Join(<span class="hljs-number">2000</span>))<br>                    System.Console.WriteLine(<span class="hljs-string">"t2 has been terminated"</span>);<br>                <span class="hljs-keyword">else</span><br>                    System.Console.WriteLine(<span class="hljs-string">"overtime"</span>);<br><br>            <span class="hljs-comment">//TimeSpan(0,0,4) 0h，0m，4s</span><br>            Thread.Sleep(<span class="hljs-keyword">new</span> TimeSpan(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">4</span>));<br>            System.Console.WriteLine(<span class="hljs-string">$"\nCurrent thread:<span class="hljs-subst">{Thread.CurrentThread.Name}</span>"</span>);<br>            System.Console.WriteLine(<span class="hljs-string">$"Thread1:<span class="hljs-subst">{thread1.ThreadState}</span>"</span>);<br>            System.Console.WriteLine(<span class="hljs-string">$"Thread2:<span class="hljs-subst">{thread2.ThreadState}</span>"</span>);<br><br>        }<br>    }<br></code></pre></td></tr></tbody></table></figure><p>在一次执行中，输出结果如下：<br><img src="/images/CSAsynchronousProgramming/join_sleep.png" alt="join_sleep"><br>在此次运行过程中，线程<code>t2</code>首先进入<code>ThreadProc</code>，之后开始Sleep，时间片交给线程<code>t1</code>，if判断成立，<code>t1</code>进入阻塞状态等待<code>t2</code>运行结束。<code>t2</code>Sleep结束后开始运行，运行结束后，<code>t1</code>继续运行直到结束</p><h3 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a><strong>Blocking</strong></h3><ul><li>如果线程的执行由于某种原因导致暂停，那么就认为该线程被阻塞了<ul><li>如<code>Sleep</code>、<code>Join</code></li></ul></li><li>被阻塞的线程会立即将其时间片交给其他线程，从此不再消耗处理器时间，知道满足阻塞结束条件为止</li><li>可以通过<code>ThreadState</code>属性来判断线程是否处于阻塞状态</li><li><code>ThreadState</code>是一个<em>flags enum</em>，通过按位与/或来合并数据项<br><img src="/images/CSAsynchronousProgramming/thread_state.png" alt="thread_state"><br><code>ThreadState</code>变化图如下：<br><img src="/images/CSAsynchronousProgramming/transform.png" alt="transform"><br>常用的状态只有四个：<code>Unstarted</code>、<code>Running</code>、<code>WaitSleepJoin</code>和<code>Stopped</code></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> ThreadState <span class="hljs-title">Foo</span>(<span class="hljs-params">ThreadState ts</span>)</span><br>        {<br>            <span class="hljs-keyword">return</span> ts &amp; (<br>                ThreadState.Stopped |<br>                ThreadState.Unstarted |<br>                ThreadState.WaitSleepJoin<br>            );<br>        }<br></code></pre></td></tr></tbody></table></figure><ul><li><p>当遇到下列四种情况解除阻塞：</p><ul><li>阻塞条件被满足</li><li>操作超时(如果设置了超时)</li><li>通过<code>Thread.Interrupt()</code>进行打断</li><li>通过<code>Thread.Abort()</code>进行中止</li></ul></li><li><p>上下文切换</p><ul><li>当线程阻塞或解除阻塞时，操作系统执行上下文切换。这会产生少量开销，通常为1或2微秒</li></ul></li><li><p>I/O密集型与CPU密集型</p><ul><li>花费大部分时间等待某事发生的操作称为I/O密集型，通常此事指输入/输出，但不是硬性要求，如<code>Thread.Sleep()</code>被视为I/O密集型</li><li>相反，一个花费大部分时间执行CPU密集型的操作成为CPU密集型</li></ul></li><li><p>阻塞与忙等待</p><ul><li>阻塞是在当前线程上同步的等待，<code>Console.ReadLine()</code>、<code>Thread.Sleep()</code>、<code>Thread.Join()</code>都是阻塞操作</li><li>忙等待以周期性的在一个循环里打转，也是同步的<br><code>while(DataTime.Now&lt;nextStartTime)</code></li><li>还有一种异步的操作，在操作完成后触发回调</li><li>如果条件很快得到满足（在几微秒之内）,短暂的忙等待更为适合，因为他避免了上下文切换的开销</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MultiThreading-programming-1&quot;&gt;&lt;a href=&quot;#MultiThreading-programming-1&quot; class=&quot;headerlink&quot; title=&quot;MultiThreading programming #1&quot;&gt;&lt;/a&gt;M</summary>
      
    
    
    
    <category term="Parallelism and concurrency" scheme="http://example.com/categories/Parallelism-and-concurrency/"/>
    
    
    <category term="MultiThreading-programming" scheme="http://example.com/tags/MultiThreading-programming/"/>
    
  </entry>
  
  <entry>
    <title>You Only Look Once #1</title>
    <link href="http://example.com/2022/03/02/YOLOv1/"/>
    <id>http://example.com/2022/03/02/YOLOv1/</id>
    <published>2022-03-02T14:48:22.580Z</published>
    <updated>2022-03-02T14:48:22.580Z</updated>
    
    <content type="html"><![CDATA[<h1 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection"></a>You Only Look Once: Unified, Real-Time Object Detection</h1><h2 id="Prime-Algorithm"><a href="#Prime-Algorithm" class="headerlink" title="Prime Algorithm"></a><strong>Prime Algorithm</strong></h2><ul><li><p>概述<br><img src="/images/yolo/2.png"></p><ul><li><p>YOLO将输入图片划分为SxS个网格。每个网格预测B个边缘框，表示为(x,y,h,w,c)，分别代表中心点坐标、边缘框高宽和边缘框置信度(图片中越粗的线c越大)；同时，每个网格还要预测一组类别概率（softmax分类概率）来确定目标的类别</p></li><li><p>上述过程训练阶段和测试阶段具体方法有所不同，下文将两个阶段分别讨论</p></li></ul></li><li><p>预测阶段  </p><ul><li><p>在预测阶段，网络直接接受3x448x448的图片作为输入。图片经24层卷积提取特征，再经两个全连接层回归得到7x7x30（S=7，B=2，类别数=30-Bx5=20）的张量作为输出，即输出49个网格生成的98个边缘框的信息及每个网格所属20个类别分别的概率（<em>全部的信息都是由神经网络预测得出的</em>）。这也表明了YOLOv1最多只能检测SxS个目标，且难以检测小而密集的目标<br><img src="/images/yolo/1.png"></p></li><li><p>在获取YOLO提供的预测信息后，需要进行一系列的后处理（置信度过滤和非极大值抑制）得到目标检测结果。下图为SxS个网格生成的BxSxS个边缘框，颜色代表了其预测类别（一个网格生成的所有边缘框同属一个类别），粗细代表了其置信度，经后处理得到最终结果<br><img src="/images/yolo/3.png"></p></li><li><p>计算全概率：将每个边缘框的置信度与其所属网格的类别概率相乘得到BxSxS个类别数维的全概率向量</p></li><li><p>NMS：对所有类别分别做如下操作，将最大全概率的边缘框与其他所有非零的边缘框计算IoU，如果超过了threshold（如0.5），则认为他们重复预测了同一目标，将概率小的边缘框在这一类别的概率置零，再继续将第二高全概率的边缘框与其后（降序）所有其他边缘框比较并重复上述过程。如此反复，直到找不到新的非零概率边缘框。有二十个类别的情况下会做二十次的NMS<br><img src="/images/yolo/6.png"></p></li><li><p>完整后处理过程：将全概率向量中数值小于threshold的直接置零，然后按照某一类别将向量降序排列，并用NMS非极大值抑制删除多余边缘框，最后对每一个全概率向量（对应每一个边缘框）找出它最大概率的类别（非零），画出该边缘框并表明这一类别。如果向量此时为零向量，则代表该边缘框不包含任何目标，不画出该边缘框<br><img src="/images/yolo/7.png"></p></li></ul></li><li><p>训练阶段</p><ul><li><p>在训练阶段，训练集提供了已经标注好的ground truth，我们要让YOLO的输出去拟合ground truth。</p></li><li><p>真实边缘框的中心点落在哪一个网格内，就应当由哪一个网格负责去预测出一个边缘框来拟合该真实边缘框，并且此网格的类别也被决定。每一个网格将预测B个边缘框，选择与真实边缘框IoU大的去拟合，小的边缘框置信度越小越好<br><img src="/images/yolo/8.png"></p></li><li><p>对于没有真实边缘框中心点落入的网格，其预测边缘框的置信度越小越好</p></li><li><p>在训练时，也是整张图片作为输入，预测的中心点坐标值是0-1之间的数，表明了在某一个网格的坐标</p></li><li><p>损失函数如下（lambda是权重，因为不包含目标的边缘框一般远多于包含的边缘框）<br><img src="/images/yolo/9.png"></p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;You-Only-Look-Once-Unified-Real-Time-Object-Detection&quot;&gt;&lt;a href=&quot;#You-Only-Look-Once-Unified-Real-Time-Object-Detection&quot; class=&quot;heade</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Computer Vision" scheme="http://example.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Object Detection</title>
    <link href="http://example.com/2022/03/01/d2l_20/"/>
    <id>http://example.com/2022/03/01/d2l_20/</id>
    <published>2022-03-01T13:02:59.359Z</published>
    <updated>2022-03-01T13:02:59.359Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><p>目标检测任务与图像分类不同，目标检测要在图片中识别出多个不同的目标并表明每个目标的位置</p><ul><li><p>边缘框</p><ul><li>边缘框表示了目标的位置，有两种坐标表示法<ul><li>(左上x，左上y，右下x，右下y)</li><li>(左上x，左上y，边框宽，边框高)</li></ul></li></ul></li><li><p>锚框</p><ul><li>锚框在目标检测算法中很常用，许多目标检测算法都用到了这项技术</li><li>锚框是对边缘框的一个猜测：<ul><li>提出多个被称为锚框的区域</li><li>预测每个锚框中是否含有目标物体</li><li>如果有，则继续预测从这个锚框到真实边缘框的偏移</li></ul></li><li>使用交并比（IoU）来计算两个框的相似度(预测框和标签框)<br><img src="/images/d2l/20/1.png"></li><li>赋予锚框标号<ul><li>在训练时，每个锚框都是一个训练样本，每次读取一张图片都要进行一次赋予锚框标号的操作</li><li>将每个锚框要么标注为背景，要么标注为与真实边缘框相关</li><li>可能生成大量锚框导致负样本过多</li><li>锚框可固定生成，或根据图片生成，甚至随机</li><li>假设一个图片有4个标签边缘框，生成了9个锚框，一种赋予标号的算法如下图<br><img src="/images/d2l/20/2.png"></li></ul></li><li>在预测时，使用非极大值抑制（NMS）输出<br><img src="/images/d2l/20/3.png"></li></ul></li><li><p>目标检测常用算法</p><ol><li><p>Faster R-CNN<br>图片进入一个CNN后分成两条路线，一条路线进入RPN（负责生成锚框），另一条路线经过RoI pooling（将不同大小的锚框提取为同一大小）后连接至全连接层，最后做出分类以及边缘框预测。Faster R-CNN相对其他算法来说还是很慢，但精度很高，适合刷榜。<br><img src="/images/d2l/20/4.png"></p></li><li><p>SSD（单发多框检测）<br>SSD由一个基础的网络来抽取特征，然后多个卷积层块来减半高宽。每段都会生成锚框，底部段拟合小物体，顶部段拟合大物体。每个锚框都会预测类别和边缘框<br><img src="/images/d2l/20/5.png"></p></li><li><p>YOLO<br>SSD中锚框有大量重叠，YOLO将图片均匀分成SxS个锚框，每个锚框预测B个边缘框。</p></li></ol></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li><p>边缘框实现</p>  <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>img=Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">"./1.jpg"</span>)<br>plt.imshow(img)<br>plt.show()<br>fig=plt.imshow(img)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">box_corner_to_center</span>(<span class="hljs-params">boxes</span>):</span><br>    <span class="hljs-string">"""(左上，右下)转换到(中间，宽度，高度)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        boxs : 一组边缘框 </span><br><span class="hljs-string">    """</span><br>    x1, y1, x2, y2 = boxes[:, <span class="hljs-number">0</span>], boxes[:, <span class="hljs-number">1</span>], boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">3</span>]<br>    cx = (x1 + x2) / <span class="hljs-number">2</span><br>    cy = (y1 + y2) / <span class="hljs-number">2</span><br>    w = x2 - x1<br>    h = y2 - y1<br>    <span class="hljs-comment"># 将新坐标堆叠起来变为二维Tensor，注意与cat不同</span><br>    boxes = torch.stack((cx, cy, w, h))<br>    <span class="hljs-comment"># 返回转置</span><br>    <span class="hljs-keyword">return</span> boxes.T<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">box_center_to_corner</span>(<span class="hljs-params">boxes</span>):</span><br>    <span class="hljs-string">"""(中间，宽度，高度)转换到(左上，右下)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        boxs : 一组边缘框 </span><br><span class="hljs-string">    """</span><br>    cx, cy, w, h = boxes[:, <span class="hljs-number">0</span>], boxes[:, <span class="hljs-number">1</span>], boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">3</span>]<br>    x1 = cx - <span class="hljs-number">0.5</span> * w<br>    y1 = cy - <span class="hljs-number">0.5</span> * h<br>    x2 = cx + <span class="hljs-number">0.5</span> * w<br>    y2 = cy + <span class="hljs-number">0.5</span> * h<br>    boxes = torch.stack((x1, y1, x2, y2))<br>    <span class="hljs-keyword">return</span> boxes.T<br><br>x=torch.tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]])<br>x=box_corner_to_center(x)<br><span class="hljs-built_in">print</span>(x)<br>x=box_center_to_corner(x)<br><span class="hljs-built_in">print</span>(x)<br><br><br>box1=torch.tensor([<span class="hljs-number">360</span>,<span class="hljs-number">170</span>,<span class="hljs-number">650</span>,<span class="hljs-number">650</span>])<br>box2=torch.tensor([<span class="hljs-number">120</span>,<span class="hljs-number">210</span>,<span class="hljs-number">350</span>,<span class="hljs-number">290</span>])<br><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_bboxes</span>(<span class="hljs-params">fig,bboxes,color</span>):</span><br>    <span class="hljs-string">"""显示带边缘框的图片</span><br><span class="hljs-string"></span><br><span class="hljs-string">    """</span> <br>    <span class="hljs-keyword">for</span> b,c <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(bboxes,color):<br>        fig.axes.add_patch(_bbox_to_rect(b, c))<br>    plt.show()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_bbox_to_rect</span>(<span class="hljs-params">bbox, color</span>):</span><br>        <span class="hljs-keyword">return</span> plt.Rectangle(xy=(bbox[<span class="hljs-number">0</span>], bbox[<span class="hljs-number">1</span>]), width=bbox[<span class="hljs-number">2</span>] - bbox[<span class="hljs-number">0</span>],<br>                                height=bbox[<span class="hljs-number">3</span>] - bbox[<span class="hljs-number">1</span>], fill=<span class="hljs-literal">False</span>,<br>                                edgecolor=color, linewidth=<span class="hljs-number">2</span>)<br><br><br><br>show_bboxes(fig,[box1,box2],[<span class="hljs-string">'red'</span>,<span class="hljs-string">'green'</span>])<br><br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Object-Detection&quot;&gt;&lt;a href=&quot;#Object-Detection&quot; class=&quot;headerlink&quot; title=&quot;Object Detection&quot;&gt;&lt;/a&gt;Object Detection&lt;/h1&gt;&lt;h2 id=&quot;Basic-kno</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Fine Tune</title>
    <link href="http://example.com/2022/02/27/d2l_19/"/>
    <id>http://example.com/2022/02/27/d2l_19/</id>
    <published>2022-02-27T06:59:36.485Z</published>
    <updated>2022-02-27T06:59:36.485Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine Tune"></a>Fine Tune</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>一般神经网络的架构都分为两块：</p><ul><li>特征抽取部分</li><li>线性分类部分<br><img src="/images/d2l/19/1.png"></li></ul></li><li><p>微调  </p><ul><li><p>一个模型特征提取部分可以作为上游模型训练其他任务<br><img src="/images/d2l/19/2.png"></p></li><li><p>pre-train的过程一般是一个大数据集上的正常训练任务，而在fine-tune的过程中使用更强的正则化、更小的学习率、更少的epoch</p></li><li><p>原数据集和目标数据集要相似，且原数据集比目标数据集要大</p></li></ul></li><li><p>微调的一些技巧</p><ul><li>重用分类器权重：原数据集中也可能有下游任务数据的部分标号，可以用预训练模型分类器中对应标号的对应向量来初始化下游任务分类部分</li><li>固定前面的层：神经网络中靠近输入的层更加通用，可以固定底部的一些层的参数不参与更新</li><li>对于个人或小企业来说，通常不会从头开始训练模型，而是进行微调</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models,transforms<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> d2l<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">64</span>)<br><br>train_aug=transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    transforms.RandomHorizontalFlip(),<br>    <span class="hljs-comment"># transforms.ToTensor(), #在load时已经将PIL转化为了Tensor</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>,<span class="hljs-number">0.456</span>,<span class="hljs-number">0.406</span>],[<span class="hljs-number">0.229</span>,<span class="hljs-number">0.224</span>,<span class="hljs-number">0.225</span>]),<br>    <br>])<br><br>test_aug=transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    <span class="hljs-comment"># transforms.ToTensor(),</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>,<span class="hljs-number">0.456</span>,<span class="hljs-number">0.406</span>],[<span class="hljs-number">0.229</span>,<span class="hljs-number">0.224</span>,<span class="hljs-number">0.225</span>])<br>])<br><br><br><span class="hljs-comment"># 指定预训练模型</span><br>finetune_res=models.resnet18(pretrained=<span class="hljs-literal">True</span>,progress=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 将分类器部分重新设计，in_features属性记录了原模型的本层的输入特征数</span><br>finetune_res.fc=nn.Linear(finetune_res.fc.in_features,<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 初始化新分类器参数</span><br>nn.init.xavier_uniform_(finetune_res.fc.weight)<br><br>loss_f=nn.CrossEntropyLoss()<br><span class="hljs-comment"># 增加正则化并且学习率应该设得很小</span><br>opt=optim.Adam(finetune_res.parameters(),weight_decay=<span class="hljs-number">0.1</span>,lr=<span class="hljs-number">5e-5</span>)<br><br><span class="hljs-comment"># 不需要迭代很多轮</span><br>d2l.train(<br>    <span class="hljs-number">5</span>,loss_f,opt,finetune_res,train_iter,<br>    save_name=<span class="hljs-string">"res18_pretrained"</span>,device=torch.device(<span class="hljs-string">"cuda:0"</span>),<br>    aug=train_aug<br>    )<br><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Fine-Tune&quot;&gt;&lt;a href=&quot;#Fine-Tune&quot; class=&quot;headerlink&quot; title=&quot;Fine Tune&quot;&gt;&lt;/a&gt;Fine Tune&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-know</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Data Augmentation</title>
    <link href="http://example.com/2022/02/26/d2l_18/"/>
    <id>http://example.com/2022/02/26/d2l_18/</id>
    <published>2022-02-26T12:30:53.085Z</published>
    <updated>2022-02-26T12:30:53.085Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><p>我们收集的训练数据通常很难覆盖到未来可能部署的全部场景（比如人脸识别的应用可能会部署到不同摄像头状况、天气、时间等的场景）。数据增强则在一个已有的数据上做数据变换，起到增大数据集的作用，使其有更好的多样性。</p><ul><li><p>数据增强</p><ul><li>数据增强只在训练时进行</li><li>一般采用在线生成的方式</li><li>数据增强假设测试环境中会出现增强后的数据，如果测试环境和训练集高度一致则没有必要做数据增强</li></ul></li><li><p>数据增强方法</p><ul><li>翻转（上下、左右翻转）</li><li>切割（随机高宽比、大小、位置切割一块，然后再变为固定大小）</li><li>颜色（色调、饱和度、亮度）</li><li>其他（高斯模糊、锐化、遮挡等）</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pytorch在提供transforms模块中提供了很多数据增广函数</span><br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_images</span>(<span class="hljs-params">imgs, num_rows, num_cols, titles=<span class="hljs-literal">None</span>, scale=<span class="hljs-number">1.5</span></span>):</span>  <br>    <br>    figsize = (num_cols * scale, num_rows * scale)<br>    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)<br>    axes = axes.flatten()<br>    <span class="hljs-keyword">for</span> i, (ax, img) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(axes, imgs)):<br>        <span class="hljs-keyword">if</span> torch.is_tensor(img):<br>            ax.imshow(transforms.ToPILImage()(img))<br>        <span class="hljs-keyword">else</span>:<br>            ax.imshow(img)<br>        ax.axes.get_xaxis().set_visible(<span class="hljs-literal">False</span>)<br>        ax.axes.get_yaxis().set_visible(<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">if</span> titles:<br>            ax.set_title(titles[i])<br>    plt.show()<br>    <span class="hljs-keyword">return</span> axes<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">4</span>,(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br>X,y=<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(test_iter))<br><br><span class="hljs-comment"># 显示tensor形式的图片</span><br><span class="hljs-comment"># X=X.view(4,224,224)</span><br><br>show_images(X,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,scale=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 数据增广</span><br><span class="hljs-comment"># trans=transforms.RandomCrop((20,20))</span><br><span class="hljs-comment"># trans=transforms.RandomHorizontalFlip()</span><br><span class="hljs-comment"># trans=transforms.GaussianBlur(5)</span><br>trans=transforms.RandomErasing(<span class="hljs-number">1</span>)<br>show_images(trans(X),<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,scale=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 显示使用PIL读进内存的图片</span><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br>test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        download=<span class="hljs-literal">True</span>,transform=transforms.ToTensor()<br>    )<br><br><span class="hljs-built_in">print</span>(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape)<br><span class="hljs-comment"># 将Tensor形式转换为PIL形式</span><br>image=transforms.ToPILImage()(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>image.show()<br><span class="hljs-keyword">import</span> torchvision<br>test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        download=<span class="hljs-literal">True</span><br>    )<br><span class="hljs-comment"># 第i张图片test_data[i][0]，test_data[i][1]是第一张图片的标签</span><br>d2l.show_images([test_data[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>)], <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, scale=<span class="hljs-number">0.8</span>)<br><br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">r"./1.jpg"</span>)<br><span class="hljs-built_in">print</span>(image)<br><span class="hljs-comment"># 将PIL形式转换为Tensor形式</span><br><span class="hljs-built_in">print</span>(transforms.ToTensor()(image))<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Data-Augmentation&quot;&gt;&lt;a href=&quot;#Data-Augmentation&quot; class=&quot;headerlink&quot; title=&quot;Data Augmentation&quot;&gt;&lt;/a&gt;Data Augmentation&lt;/h1&gt;&lt;h2 id=&quot;Basic</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: ResNet</title>
    <link href="http://example.com/2022/02/26/d2l_17/"/>
    <id>http://example.com/2022/02/26/d2l_17/</id>
    <published>2022-02-26T10:01:45.328Z</published>
    <updated>2022-02-26T10:01:45.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>加更多的层不一定总是改进精度</p><ul><li>新的层可能是使模型收敛范围偏差到一个不符合预期的区域</li><li>ResNet使各层更容易学会恒等变换，从而更容易使模型收敛范围达到Nested function classes<br><img src="/images/d2l/17/1.png"></li></ul></li><li><p>残差块</p><ul><li>基本的ResBlock结构如下，f(x)+x保证了包含原收敛范围<br><br><img src="/images/d2l/17/2.png"></li><li>具体使用时，ResBlock的设计细节<br><br><img src="/images/d2l/17/4.png"></li></ul></li><li><p>ResNet架构<br>一般来说现在的主流设计架构就是接入一个Stage（7x7Conv-3x3MP），之后再连接具体想要的网络架构，ResNet架构如下也是这种设计思想，具体架构如下<br>  <img src="/images/d2l/17/5.png"></p></li></ul><ul><li>Tricks<ul><li>实际应用中，Res34用的最多，达不到要求可以继续用Res50</li><li>Res152、Res101一般用来刷榜</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Residual</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,in_channels,out_channels,</span></span><br><span class="hljs-params"><span class="hljs-function">        use_1x1conv=<span class="hljs-literal">False</span>,stride=<span class="hljs-number">1</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1=nn.Conv2d(in_channels,out_channels,<span class="hljs-number">3</span>,stride,<span class="hljs-number">1</span>)<br>        self.conv2=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.bn1=nn.BatchNorm2d(out_channels)<br>        self.bn2=nn.BatchNorm2d(out_channels)<br>        <span class="hljs-comment"># inplace更省内存(显存)</span><br>        self.relu=nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.conv3=<span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span>(use_1x1conv):<br>            self.conv3=nn.Conv2d(in_channels,out_channels,<span class="hljs-number">1</span>,stride)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        Y=self.relu(self.bn1(self.conv1(X)))<br>        Y=self.bn2(self.conv2(Y))<br><br>        <span class="hljs-keyword">if</span>(self.conv3):<br>            X=self.conv3(X)<br>        <span class="hljs-keyword">return</span> self.relu(Y+X)<br><br>s1=nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),nn.BatchNorm2d(<span class="hljs-number">64</span>),nn.ReLU(),nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet_block</span>(<span class="hljs-params">input_channels, num_channels, num_residuals,</span></span><br><span class="hljs-params"><span class="hljs-function">                 first_block=<span class="hljs-literal">False</span></span>):</span><br>    blk = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>            blk.append(<br>                Residual(input_channels, num_channels, use_1x1conv=<span class="hljs-literal">True</span>,<br>                         stride=<span class="hljs-number">2</span>))<br>        <span class="hljs-keyword">else</span>:<br>            blk.append(Residual(num_channels, num_channels))<br>    <span class="hljs-keyword">return</span> blk<br><br>s2=nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">2</span>,<span class="hljs-literal">True</span>))<br>s3=nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>,<span class="hljs-number">2</span>))<br>s4=nn.Sequential(*resnet_block(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">2</span>))<br>s5=nn.Sequential(*resnet_block(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">2</span>))<br><br>device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>res_net=nn.Sequential(<br>    s1,s2,s3,s4,s5,<br>    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>)<br><br>x=torch.rand((<span class="hljs-number">20</span>,<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><span class="hljs-built_in">print</span>(res_net(x).shape)<br><br>opt=optim.Adam(res_net.parameters())<br>train_iter,val_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,nn.CrossEntropyLoss(),opt,</span><br><span class="hljs-comment">#     res_net,train_iter,save_name="res_net"</span><br><span class="hljs-comment">#     )</span><br><br>d2l.evaluate(res_net,val_iter,nn.CrossEntropyLoss(),<span class="hljs-string">"./params/res_net_2"</span>,device=torch.device(<span class="hljs-string">"cuda:0"</span>))<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;ResNet&quot;&gt;&lt;a href=&quot;#ResNet&quot; class=&quot;headerlink&quot; title=&quot;ResNet&quot;&gt;&lt;/a&gt;ResNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-knowledge&quot; class</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Vison in Transformer</title>
    <link href="http://example.com/2022/02/21/VIT/"/>
    <id>http://example.com/2022/02/21/VIT/</id>
    <published>2022-02-21T12:07:31.165Z</published>
    <updated>2022-02-21T12:07:31.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><a href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale" class="headerlink" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"></a>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h1><p>从2012年AlexNet提出以来，卷积神经网络在计算机视觉领域一直占据统治地位。而本篇论文的研究表明，拥有足够多数据进行预训练的情况下，Transformer网络架构也可以把计算机视觉问题解决的很好。更进一步来说，这篇论文的提出打破了cv和nlp之间的壁垒，在多模态领域也产生了很大影响</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a><strong>Intro</strong></h2><ul><li>将每一张图片视作由许多16x16的patch组成，每个patch当作nlp领域中的单词，一张图片便可视作一个sequence</li><li>CV领域不需要局限于CNNs的结构，纯Transformer在预训练集足够大时在图像分类任务达到了目前CNNs的SOTA，并且需要更少的计算资源</li><li>使用Transformer架构，到目前为止还未出现增加数据和模型复杂度导致性能饱和的现象</li><li>CNNs具有人为规定的两个先验信息（局部性、平移不变性），而Transformer是缺少这样的信息的，所以在数据集不够的时候时VIT比CNNs的SOTA要差一点（因为VIT需要自己去学习两个先验信息），进一步扩大数据集后，效果达到SOTA</li></ul><h2 id="Problems-and-Purposes"><a href="#Problems-and-Purposes" class="headerlink" title="Problems and Purposes"></a><strong>Problems and Purposes</strong></h2><ul><li>受Transformer在nlp领域可扩展性的成功，本文想尽量少地修改Transformer架构，将这种可扩展性带到CV领域</li><li>如何把2d的图片变成1d的序列</li><li>输入transformer的长一般为512、1024这个量级，要考虑计算性能，序列过长复杂度无法接受</li><li>本文的解决方案：将原图划分为多个16x16地patch，再将patch组成sequence，大大减小了序列长度</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a><strong>Method</strong></h2><p><img src="/images/VIT/cover.jpg" alt="$cover"><br>将原始图片划分出多个patches，patches经过线性投射层展平为一维向量，为每个向量加入位置编码后输入Transformer Encoder。另外引入了一个额外的token（第0位向量），并以该对应的Transformer Encoder输出作为分类的依据</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale&quot;&gt;&lt;a href=&quot;#An-Image-is-Worth-16x16-Words-Transformers-for-</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Computer Vision" scheme="http://example.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Batch Normalization</title>
    <link href="http://example.com/2022/02/21/d2l_16/"/>
    <id>http://example.com/2022/02/21/d2l_16/</id>
    <published>2022-02-21T06:36:14.190Z</published>
    <updated>2022-02-21T06:36:14.190Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>问题<ul><li>损失出现在最后，由BP算法和梯度消失，后面的层训练的会更快</li><li>数据在最前面，前面的层训练的慢且前面的层变化后面的层也要跟着变（抽取的底层信息变化让后面的层要重新学），所以后面的层要重新学习很多次，导致收敛变慢</li><li>考虑在学习底部层时避免变化顶部层</li></ul></li><li>批量归一化<ul><li>固定小批量里面的均值和方差，然后再做额外的调整（可学习的参数gama和beta）<br><img src="/images/d2l/16/1.png"></li><li>是线性变换</li><li>作用在<ul><li>全连接层和卷积层输出后，激活函数前</li><li>全连接层和卷积层输入前</li></ul></li><li>对于全连接层作用于特征维</li><li>对于卷积层作用于通道维（将每一个像素都当作一个样本，通道数就是一个样本的特征数）</li></ul></li><li>批量归一化在做什么？<ul><li>最初的论文是想用它来减少内部协变量转移（使每一层的输出分布变化不那么剧烈）</li><li>后续有论文指出，批量归一化可能只是在小批量中加入噪声控制模型复杂度</li></ul></li><li>总结<ul><li>批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放</li><li>批量归一化可以加速收敛（可以设置更大的学习率），一般不改变模型精度</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch<br><br>net=nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),nn.BatchNorm2d(<span class="hljs-number">6</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),nn.BatchNorm2d(<span class="hljs-number">16</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.BatchNorm1d(<span class="hljs-number">120</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.BatchNorm1d(<span class="hljs-number">84</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters(),lr=<span class="hljs-number">1.0</span>)<br><br><span class="hljs-keyword">import</span> d2l<br><br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,nn.CrossEntropyLoss(),</span><br><span class="hljs-comment">#     optim.Adam(net.parameters()),</span><br><span class="hljs-comment">#     net,train_iter,save_name="LeNet_bn",</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"))</span><br>d2l.evaluate(<br>    net,test_iter,nn.CrossEntropyLoss(),<br>    param_path=<span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/LeNet_bn_10"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Batch-Normalization&quot;&gt;&lt;a href=&quot;#Batch-Normalization&quot; class=&quot;headerlink&quot; title=&quot;Batch Normalization&quot;&gt;&lt;/a&gt;Batch Normalization&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Basic SQL</title>
    <link href="http://example.com/2022/02/21/sql/"/>
    <id>http://example.com/2022/02/21/sql/</id>
    <published>2022-02-21T06:31:45.970Z</published>
    <updated>2022-02-21T06:31:45.970Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-SQL"><a href="#Basic-SQL" class="headerlink" title="Basic SQL"></a>Basic SQL</h1><p>数据库语言可分为两个部分，DDL和DML。</p><ul><li>DDL（Data Definitoin Language）用于描述数据库中要存储的现实世界实体，操作数据库的结构等</li><li>DML（Data Manipulation Language）用于数据库操作，操作数据库存储的对象或数据</li></ul><h2 id="SQL-DDL-数据定义"><a href="#SQL-DDL-数据定义" class="headerlink" title="SQL DDL:数据定义"></a><strong>SQL DDL:数据定义</strong></h2><h3 id="数据库相关"><a href="#数据库相关" class="headerlink" title="数据库相关"></a>数据库相关</h3><ul><li>查询所有数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">show</span> databases;<br></code></pre></td></tr></tbody></table></figure></li><li>创建新数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> database 数据库名称;<br></code></pre></td></tr></tbody></table></figure></li><li>删除数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">drop</span> database 数据库名称;<br></code></pre></td></tr></tbody></table></figure><h3 id="表相关"><a href="#表相关" class="headerlink" title="表相关"></a>表相关</h3></li><li>查询当前数据库下所有表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">show</span> tables;<br></code></pre></td></tr></tbody></table></figure></li><li>创建新表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql">   <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> 表名称<br>   (<br>列名称<span class="hljs-number">1</span> 数据类型[(最大位数) 约束]，<br>列名称<span class="hljs-number">2</span> 数据类型[(最大位数) 约束]，<br>列名称<span class="hljs-number">3</span> 数据类型[(最大位数) 约束],<br>...    <br>   )<br></code></pre></td></tr></tbody></table></figure></li><li>删除表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">drop</span> <span class="hljs-keyword">table</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改已有表的列  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 在表中添加列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名称<br><span class="hljs-keyword">add</span> 列名称 数据类型;<br><br># 删除表中的列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名称<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">column</span> 列名称;<br></code></pre></td></tr></tbody></table></figure><h2 id="SQL-DML-数据操作"><a href="#SQL-DML-数据操作" class="headerlink" title="SQL DML:数据操作"></a><strong>SQL DML:数据操作</strong></h2></li><li>从表中获取数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 获取表中某一列数据<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br># 或获取表中所有数据<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 更新符合要求的行中某列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称 <span class="hljs-operator">=</span> 新值 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br># 更新符合要求的行中若干列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称<span class="hljs-number">1</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">2</span> <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br></code></pre></td></tr></tbody></table></figure></li><li>删除表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 删除表中的某行<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 值;<br># 删除所有行（在不删除表的情况下删除所有的行。这意味着表的结构、属性和索引都是完整的）<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称;<br><span class="hljs-keyword">delete</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>向表中插入数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称 <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br># 指定要插入的列<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称(列<span class="hljs-number">1</span>,列<span class="hljs-number">2</span>,...) <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br></code></pre></td></tr></tbody></table></figure></li><li>去重后从表中获取某列中值  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>根据条件获取表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">and</span> 列 运算符 值;<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">or</span> 列 运算符 值;<br></code></pre></td></tr></tbody></table></figure></li><li>根据指定的列对结果集进行排序  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br># 先按列名称<span class="hljs-number">1</span>排序，然后以列名称<span class="hljs-number">3</span>排序<br><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span>,列名称<span class="hljs-number">3</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)],列名称<span class="hljs-number">3</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Basic-SQL&quot;&gt;&lt;a href=&quot;#Basic-SQL&quot; class=&quot;headerlink&quot; title=&quot;Basic SQL&quot;&gt;&lt;/a&gt;Basic SQL&lt;/h1&gt;&lt;p&gt;数据库语言可分为两个部分，DDL和DML。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DDL（Dat</summary>
      
    
    
    
    <category term="Database" scheme="http://example.com/categories/Database/"/>
    
    
    <category term="Sql" scheme="http://example.com/tags/Sql/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://example.com/2022/01/29/Transformer/"/>
    <id>http://example.com/2022/01/29/Transformer/</id>
    <published>2022-01-29T06:19:03.569Z</published>
    <updated>2022-01-29T06:19:03.569Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention-Is-All-Your-Need"><a href="#Attention-Is-All-Your-Need" class="headerlink" title="Attention Is All Your Need"></a>Attention Is All Your Need</h1><p>以往主流的序列转录(seq2seq)模型中常常基于包含Encoder与Decoder的复杂RNN或CNN，这些模型也会在Encoder与Decoder中使用Attention机制。<br>Transformer仅仅使用了Attention机制，完全没用到循环和卷积，将循环层换为了Multi-headed Attetion。Transformer训练速度更快，预测能力更好。  </p><hr><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>RNN难以并行计算计算效率低，Transformer可并行。</li><li>RNN带有时序信息，但在序列较长时，早期的信息可能在后期丢失。Attention可通过在输入序列中加入index增加时序，并且不会存在信息丢失问题。</li><li>用CNN可以替换掉RNN实现并行计算，但由于感受野的限制其依然存在难以对长序列进行建模的问题。Transformer中的Attention机制一次性看到所有的序列，消除了这一问题。</li><li>CNN可利用多个输出通道识别不一样的模式，在Transformer中使用Multi-headed Attetion，也实现了这样的特性。</li></ul><hr><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Transformer整体分为Encoder与Decoder两大部分。  </p><ul><li>Input Embedding/Output Embedding将词映射到向量。</li><li>Postoinal Encoding</li><li>Nx指有N个该块叠在一起。</li><li>Add表示残差连接，Norm表示正则处理。</li><li>在训练时，解码器的输入（outputs）是真实值(Ground Truth)；在测试时，输入（outputs）是前一时刻的输出。<br><img src="/images/transformer/architecture.png" alt="architecture"></li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>N=6，out_dim=input_dim=512。由两个子层组成，两个子层分别为Multi-headed Attetion和简单的MLP。每个子层使用残差连接和layer normalization。</p><blockquote><p>layer normalization and batch normalization</p><blockquote><p>batch:对每一个feature，将这个feature在一个batch中的所有数据的均值变为0方差变为1<br><img src="/images/transformer/batch_norm.png" alt="batch_norm"><br>layer:对每一个样本，将这个样本所有的特征均值变为0方差变为1<br><img src="/images/transformer/layer_norm.png" alt="layer_norm"><br>成sequence的normalization:<br>蓝为batch，黄为layer，取所有数据去做norm。阴影部分为样本实际长度（即该样本序列的seq_len），在实际长度外取全0。<br><img src="/images/transformer/seq_norm.png" alt="seq_norm"><br>使用layernormalization相对稳定一些。每个样本自己做均值方差再去norm。</p></blockquote></blockquote><hr><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>N=6,由三个子层组成，其中后两个子层Encoder一致，而第一个子层有所不同，其使用了Masked Multi-headed Attetion。在Decoder中还使用了自回归。当前的outputs输入是上一次的输出。在训练过程中，Decoder的输入outputs为ground truth，但在t时刻的输入不应包含t时刻之后的输入，所以第一个子层引入了masked机制。</p><hr><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><blockquote><p>Transformer使用了Scaled Dot-Product Attention。其将输入的一个seq进行融合输出一个等长的seq。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>在融合时，每一个输出都考虑了其对应位置输入元素和seq中其他元素的相关度。Wq与Wk是两个参数矩阵，原始输入做矩阵运算后得到两个向量q、k，最后用内积运算即得到了原始输入的相关性（内积运算代表了余弦相似度）。<br><img src="/images/transformer/dot-product.png" alt="dot-product"><br>同理可计算出a1与自身及序列中所有元素的相关性，全部计算完成后输入到softmax层进行归一化。<br><img src="/images/transformer/to-softmax.png" alt="to-softmax"><br>利用一个新的参数矩阵Wv获得v1-v4。v与相关性系数的带权和得到b1。<br><img src="/images/transformer/b1.png" alt="b1"><br>同理，可以得到b2、b3、b4。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>上述步骤可表示为矩阵运算。<br><img src="/images/transformer/use_matrix.png" alt="use_matrix"><br>最终的计算公式可如下表示。这里多了一个除以根号dk(input_dim)，这是因为向量(Transformer中dk=512)比较长时，内积绝对值可能会出现比较大的情况，这对梯度下降是不利的。<br><img src="/images/transformer/formula.png" alt="formula"> </p></blockquote><blockquote><p>Multi-headed Attetion可以看作是多通道的Attention。以2heads为例，计算过程如下，增加了多个参数矩阵。<br><img src="/images/transformer/multi-head.png" alt="multi-head"><br>在最后，将bi1与bi2一起其他的相应多通道b在特征维度拼接起来，为保证dim不变，利用一个新的参数矩阵Wo将输出元素的维度变为和输入相同。<br><img src="/images/transformer/concat.png" alt="concat"><br>一个线性层就可看作一个参数矩阵，所以上述两步操作可看作如下的过程。<br><img src="/images/transformer/liner-representatoin.png" alt="liner-representatoin"> </p></blockquote><hr><h3 id="Cross-Attention-in-Transformer"><a href="#Cross-Attention-in-Transformer" class="headerlink" title="Cross Attention in Transformer"></a>Cross Attention in Transformer</h3><p>Cross Attention指的是Encoder与Decoder之间的Attention机制。其V和K来自于Encoder，而Q来自于masked Attention。由于是masked，向Q输入的部分其seq_len会与V和K的不同，又因为其作为Q输入，所以cross-attention输出的seq_len会与之相同，所以Decoder的输入输出seq_len是相同的。<br><img src="/images/transformer/cross-attention.png" alt="cross-attention">  </p><hr><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed-Forward"></a>Feed-Forward</h3><p>只有一个MLP分别去作用于seq中的每个词，<br>图中MLP权重是相同的，也不需要把Encoder的输出合并输入到大的MLP。因为这里只是想要把原始维度投影到想要的另一个维度，其信息融合已经在Encoder中做完了。<br><img src="/images/transformer/feed-forward.png" alt="feed-forward"></p><hr><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>编码器要有embedding，解码器要有embedding，softmax层之前有一个Liner层，这三个层共享权重。</p><hr><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>为了给Attention加上时序信息，给输入加上位置信息。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Attention-Is-All-Your-Need&quot;&gt;&lt;a href=&quot;#Attention-Is-All-Your-Need&quot; class=&quot;headerlink&quot; title=&quot;Attention Is All Your Need&quot;&gt;&lt;/a&gt;Attentio</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Seq2Seq-Model" scheme="http://example.com/tags/Seq2Seq-Model/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="http://example.com/2022/01/29/resnet/"/>
    <id>http://example.com/2022/01/29/resnet/</id>
    <published>2022-01-29T06:18:59.404Z</published>
    <updated>2022-01-29T06:18:59.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><p>深层的神经网络通常很难进行训练，本文使用了一个残差学习网络结构来训练比以往的神经网络要深得多的模型。残差网络容易训练，并且在深层神经网络中表现出来较好的准确率。</p><p><img src="/images/resnet/plain_error.png" alt="plain_error"><br>在未使用残差网络的模型中，当网络层数变多时，训练误差以及测试误差均会升高。</p><h2 id="Is-learning-better-networks-as-easy-as-stacking-more-layers"><a href="#Is-learning-better-networks-as-easy-as-stacking-more-layers" class="headerlink" title="Is learning better networks as easy as stacking more layers ?"></a><strong>Is learning better networks as easy as stacking more layers ?</strong></h2><ul><li>当网络变得特别深时，会出现梯度爆炸或梯度消失</li><li>传统的解决方法是：参数在初始化时要做的好一点，不要太大也不要太小；加入一些Batch Normalization Layers</li><li>传统的解决方法使得神经网络能够收敛，但是网络的精度却变得更差，而这并非是模型变得复杂后导致的过拟合问题，因为模型的训练误差也变高了</li><li>正常来说，如果在一个浅层的神经网络后直接加入更多的层，这些层只做identity mapping，那么这个深层神经网络的误差绝不会高于浅层的神经网络，但是传统的神经网络模型并未找到这样的解（或更好的解）</li><li>如果深层网络后面的层都是是恒等映射，那么模型就可以转化为一个浅层网络</li></ul><h2 id="Deep-residual-learning-framework"><a href="#Deep-residual-learning-framework" class="headerlink" title="Deep residual learning framework"></a><strong>Deep residual learning framework</strong></h2><ul><li>将残差块的输入与块内最后一个神经网络层的线性输出求和后在进行激活，得到残差块的输出<br><img src="/images/resnet/residual_block.png" alt="plain_error"></li><li>残差块只简单地做了shortcut connections，没有引入额外的训练参数，不会增加网络复杂度</li><li>已有的神经网络很难拟合潜在的恒等映射函数H(x) = x，但是ResNet将残差块设计为H(x) = F(x) + x，其直接把恒等映射作为网络的一部分，只要F(x) = 0，便得到了恒等映射。而此时F(x) = H(x) - x 称为残差函数，就是当前残差块的学习目标（学习出这样一个F(x)函数满足如图输出）</li><li>值得一提的是，一个残差块中应该至少有两层（中间要包含一个非线性激活），否则就会出现如下情况，这显然是没有用的工作<br><img src="/images/resnet/no_use.png" alt="plain_error"></li></ul><h2 id="Deeper-bottleneck-architecture"><a href="#Deeper-bottleneck-architecture" class="headerlink" title="Deeper bottleneck architecture"></a><strong>Deeper bottleneck architecture</strong></h2><ul><li>当神经网络层数进一步增多时，参数的增长会带来很大的计算开销。此时可以考虑使用1*1的卷积核暂时减少通道数来减少整个网络的数据规模<br><img src="/images/resnet/deeper.png" alt="plain_error"></li></ul><h2 id="Analysis-of-Deep-Residual-Networks"><a href="#Analysis-of-Deep-Residual-Networks" class="headerlink" title="Analysis of Deep Residual Networks"></a><strong>Analysis of Deep Residual Networks</strong></h2><ul><li>基本BP算法流程如下<br><img src="/images/resnet/bp.jpg" alt="plain_error"></li><li>残差块的反向传播过程较好地解释了残差网络避免梯度消失原因，具体推导过程如下</li><li>推导中忽略偏置项和激活函数<br><img src="/images/resnet/res_bp.png" alt="plain_error"></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Residual-Network&quot;&gt;&lt;a href=&quot;#Residual-Network&quot; class=&quot;headerlink&quot; title=&quot;Residual Network&quot;&gt;&lt;/a&gt;Residual Network&lt;/h1&gt;&lt;p&gt;深层的神经网络通常很难进行训</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Residual-learning" scheme="http://example.com/tags/Residual-learning/"/>
    
  </entry>
  
  <entry>
    <title>GAN #1</title>
    <link href="http://example.com/2022/01/29/Gan0/"/>
    <id>http://example.com/2022/01/29/Gan0/</id>
    <published>2022-01-29T06:18:55.732Z</published>
    <updated>2022-01-29T06:18:55.732Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Generative-Adversarial-Nets"><a href="#Generative-Adversarial-Nets" class="headerlink" title="Generative Adversarial Nets"></a>Generative Adversarial Nets</h1><p>Gan利用对抗的方法，提出了生成式模型的新框架。它需要同时训练两个模型（生成模型G和判别模型D），G用于捕获数据的分布，而D需要判别出一个样本是来自训练集还是生成模型。生成模型的目标是尽最大可能让判别模型犯错（无法成功判别数据的来源），G和D在本文中被定义为多层感知机，整个系统通过反向传播来进行训练。</p><h2 id="AN-analogy-to-GAN"><a href="#AN-analogy-to-GAN" class="headerlink" title="AN analogy to GAN"></a><strong>AN analogy to GAN</strong></h2><p>在生成对抗网络框架中，生成模型与判别互相对抗。可以把生成模型类比为造假币者，判别模型类比为警察。生成模型试图制造假币骗过判别模型，而判别模型努力区分假币。二者在这样的对抗中不断学习提升各自的水平，直到生成模型的造的假币和真的一模一样，判别器无法区分。另外，警察进步不能过大或过小。进步过大时，造假者直接被一锅端，无法继续造假钞；进步过小时，造假者不需进步也能骗过警察，则没有动力进步</p><h2 id="Adversarial-nets"><a href="#Adversarial-nets" class="headerlink" title="Adversarial nets"></a><strong>Adversarial nets</strong></h2><ul><li><p>对抗网络认为数据集代表着一个联合分布，每一个样本都可以由高维随机变量表示。假设数据集是许多张2*2大小的黑白图片，每张图片有四个像素点，于是将该分布看作四维随机变量的分布，每一维代表着一个像素的取值（黑白只取0或1）<br><img src="/images/gan/mrv.jpg"></p></li><li><p>在让生成模型学习数据集代表了分布之前，首先定义一个符合高斯分布的先验噪声Z，生成模型就是要学习把Z映射成为数据集代表的分布，MLP理论上可以拟合这样一个函数来完成目标。生成器以Z为输入，输出一个尽量符合数据集分布的样本  </p></li><li><p>判别器也是一个MLP，判别器以数据集或生成器生产的样本为输入，输出一个标量（0：生成器生成的样本，1：真实样本，判别器的输出介于[0,1]），判别样本的真伪。判别器要学习一个二分类任务  </p></li><li><p>D与G对于同一个目标函数采取相反的优化方式：<br><img src="/images/gan/obj_func.png"><br>在不同分布里采样计算后得到期望和，生成器和鉴别器分别要调整参数最大化和最小化这一期望和，体现了对抗的过程，D尽量区分生成的数据和真实数据，G尽量使得生成的数据和真实数据难以区分</p></li><li><p>下图展示了对抗网络训练过程中，各个成分的变化：<br><img src="/images/gan/graph_proc.png"><br>Z为高斯噪声，绿色线代表G生成数据的概率密度函数，黑色线代表真实数据的概率密度函数，蓝色代表D输出标量的函数<br>（a）G将Z随即映射为另一个分布，与真实分布存在一定差距，此时D未经学习，分类能力弱<br>（b）固定G，训练D，D分类能力明显上升<br>（c）固定D，训练G，G学习到把Z向真实数据分布映射<br>（d）反复多轮后，G映射的分布与真实数据相同，判别器无法区分</p></li><li><p>对抗网络算法流程如下：<br><img src="/images/gan/algorithm.png"><br>先更新D，再更新G，G只与目标函数中后半段有关。k是一个超参数，不能太大也不能大小。取太大判别器训练得太好，取太小判别器变化太小</p></li></ul><h2 id="Theoretical-results"><a href="#Theoretical-results" class="headerlink" title="Theoretical results"></a><strong>Theoretical results</strong></h2><ul><li>理论上，对抗网络存在全局最优解：生成器映射的分布等于真实数据分布</li><li>对抗网络算法可以求解目标函数</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Generative-Adversarial-Nets&quot;&gt;&lt;a href=&quot;#Generative-Adversarial-Nets&quot; class=&quot;headerlink&quot; title=&quot;Generative Adversarial Nets&quot;&gt;&lt;/a&gt;Gener</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Generative-Model" scheme="http://example.com/tags/Generative-Model/"/>
    
  </entry>
  
  <entry>
    <title>D2L: GoogLeNet</title>
    <link href="http://example.com/2022/01/29/d2l_15/"/>
    <id>http://example.com/2022/01/29/d2l_15/</id>
    <published>2022-01-29T06:18:33.434Z</published>
    <updated>2022-01-29T06:18:33.434Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>Inception块<ul><li>4个路径从不同层面抽取信息，然后再输出通道合并，最终输出高宽与输入相等，要把更多的通道数留给比较重要的通道<br><img src="/images/d2l/15/1.png"></li><li>要达到相同的输出通道数，Inception块与直接的3x3或5x5卷积相比，参数和计算复杂度更低</li></ul></li><li>GoogLeNet<ul><li>5个stage（高宽减半一次就是一个stage），9个Inception块<br><img src="/images/d2l/15/2.png"></li></ul></li><li>Inception后续具有多个变种<ul><li>Inception-BN(v2)：使用batch normalization</li><li>Inception-v3：修改了inception块</li><li>Inception-v4：使用了残差连接</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inception</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>        self.relu=nn.ReLU()<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        p1 = self.relu(self.p1_1(x))<br>        p2 = self.relu(self.p2_2(self.relu(self.p2_1(x))))<br>        p3 = self.relu(self.p3_2(self.relu(self.p3_1(x))))<br>        p4 = self.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 以通道维拼接张量</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br><br>b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>,<br>                                           padding=<span class="hljs-number">1</span>))<br><br>b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>), nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,resize=<span class="hljs-number">96</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"),</span><br><span class="hljs-comment">#     save_name="GoogLeNet"</span><br><span class="hljs-comment"># )</span><br><br>d2l.evaluate(<br>    net,test_iter,loss_f,<br>    <span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/GoogLeNet_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GoogLeNet&quot;&gt;&lt;a href=&quot;#GoogLeNet&quot; class=&quot;headerlink&quot; title=&quot;GoogLeNet&quot;&gt;&lt;/a&gt;GoogLeNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-know</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Convolution and Pooling</title>
    <link href="http://example.com/2022/01/29/d2l_10/"/>
    <id>http://example.com/2022/01/29/d2l_10/</id>
    <published>2022-01-29T06:18:27.246Z</published>
    <updated>2022-01-29T06:18:27.246Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Convolution-and-Pooling"><a href="#Convolution-and-Pooling" class="headerlink" title="Convolution and Pooling"></a>Convolution and Pooling</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>卷积</p><ul><li>平移不变性和局部性是在图片中寻找某种模式的原则。<ul><li>因为模式不会随着其在图片中位置改变而改变，所以一个识别器（卷积核）被设计为具有平移不变性（即参数只与输入的像素值有关，而与像素在图片的位置无关），去学习图片中的一种模式</li><li>模式与其相邻的局部相关，识别器每次仅去看图片的一部分</li><li>对全连接层使用平移不变性和局部性得到卷积层</li></ul></li></ul></li><li><p>卷积层<br><img src="/images/d2l/10/1.png"></p><ul><li>不同卷积核（值）会对图片带来不同的效果，当某种效果对任务有帮助时，网络很有可能就会学习出这种卷积核<br><img src="/images/d2l/10/2.png"></li></ul></li><li><p>填充与步幅</p><ul><li>在输入的四周加入额外的行和列以控制卷积后的输出图像大小，卷积核大小一般选奇数，能上下对称地填充图片来保证输入输出图片大小不变<br><img src="/images/d2l/10/3.png"></li><li>增大卷积步幅，快速缩小图片</li><li>总结<br><img src="/images/d2l/10/4.png"></li></ul></li><li><p>通道</p><ul><li>每个通道有自己的卷积核，输入通道不同通道的对应卷积后直接相加后再加偏置项，最后输出一个单通道</li><li>多输出通道就是多个上述操作输出的多个单通道</li><li>每个输出通道可以识别特定的模式，输入通道识别并组合（加权相加）输入中的模式</li></ul></li><li><p>池化</p><ul><li>池化层缓解卷积对位置的敏感性</li><li>池化层不学习任何参数，有最大池化、平均池化等</li><li>池化层也可以调整填充与步幅</li><li>经过池化层不会改变通道数</li></ul></li><li><p>tricks</p><ul><li>填充：一般填充就是为了使图小大小不变</li><li>步幅：一般设置为1，计算量太大时增大步幅</li><li>最终图像大小：一般为3x3、5x5、7x7</li><li>1x1卷积层：不识别空间模式，而是用来改变通道数</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Convolution-and-Pooling&quot;&gt;&lt;a href=&quot;#Convolution-and-Pooling&quot; class=&quot;headerlink&quot; title=&quot;Convolution and Pooling&quot;&gt;&lt;/a&gt;Convolution and P</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: LeNet</title>
    <link href="http://example.com/2022/01/29/d2l_11/"/>
    <id>http://example.com/2022/01/29/d2l_11/</id>
    <published>2022-01-29T06:18:26.253Z</published>
    <updated>2022-01-29T06:18:26.253Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>最早是用于手写数字识别，识别信件上的邮政编码</li><li>网络结构<br><img src="/images/d2l/11/1.png" alt="$cover"></li><li>提出了一个数据集：MNIST<ul><li>5w个训练数据</li><li>1w个测试数据</li><li>图像大小 28x28</li><li>10类</li></ul></li><li>总结<ul><li>LeNet是早期成功的神经网络</li><li>先使用卷积层学习图片空间信息</li><li>然后使用全连接层转换到类别空间</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-comment"># 定义Reshape层</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Reshape</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br><br>net=nn.Sequential(<br>    Reshape(),<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br><br>x=torch.rand((<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>),dtype=torch.float32)<br><br><span class="hljs-comment"># 观察每一层输出的tensor尺寸</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    x=layer(x)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">':\t'</span>,x.size())<br><br><span class="hljs-comment"># 在Fashion-MINST上的表现</span><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     25,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     param_name="LeNet",device=torch.device("cuda:0")</span><br><span class="hljs-comment">#     )</span><br>d2l.evaluate(net,test_iter,loss_f,<span class="hljs-string">".\params\LeNet_25"</span>)   <br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LeNet&quot;&gt;&lt;a href=&quot;#LeNet&quot; class=&quot;headerlink&quot; title=&quot;LeNet&quot;&gt;&lt;/a&gt;LeNet&lt;/h1&gt;&lt;h2 id=&quot;Basic-knowledge&quot;&gt;&lt;a href=&quot;#Basic-knowledge&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
</feed>
