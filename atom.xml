<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xavier&#39;s blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-02-10T13:47:18.173Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Xavier</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL Datatype</title>
    <link href="http://example.com/2023/02/10/mysql_2/"/>
    <id>http://example.com/2023/02/10/mysql_2/</id>
    <published>2023-02-10T13:47:18.173Z</published>
    <updated>2023-02-10T13:47:18.173Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MySQL-Datatype"><a href="#MySQL-Datatype" class="headerlink" title="MySQL Datatype"></a>MySQL Datatype</h1><p>mysql 数据类型反应在列（字段上），不同记录在同一字段上的分量数据类型相同。mysql支持多种数据类型，大致分为三类：数值、日期/时间、字符串（字符）。选取类型的准则是在满足数据需求的情况下尽量选择范围小的数据类型。</p><h2 id="数值类型"><a href="#数值类型" class="headerlink" title="数值类型"></a><strong>数值类型</strong></h2><p>mysql 支持所有标准 SQL 数值数据类型，这些类型包括严格数值数据类型(INTEGER、SMALLINT、DECIMAL 和 NUMERIC等)，以及近似数值数据类型(FLOAT、REAL 和 DOUBLE PRECISION等)。BIT数据类型保存位字段值。<br>所有数值类型详细信息见下图。<br><img src="/images/mysql/1.jpg"></p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ul><li>bit类型会以二进制形式显示数值。</li><li>decimal中的M和D分别代表记录数值的总位数和小数位数，一般需求非常高精度数据或有很大数时使用decimal类型</li></ul><h2 id="字符类型"><a href="#字符类型" class="headerlink" title="字符类型"></a><strong>字符类型</strong></h2><p>下图展示了 mysql 字符数据类型的详细信息。<br><img src="/images/mysql/2.jpg"></p><h3 id="注意-1"><a href="#注意-1" class="headerlink" title="注意"></a>注意</h3><ul><li>char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数</li><li>char 定长，记录分量存储时自动补齐空格至指定长度个字符，即为记录分配定长空间</li><li>varchr 不定长，不补齐空格，分配空间按实际计算，分配大小为实际数据字节大小+1-3个字节（用于存放实际实际长度）</li></ul><h2 id="日期-时间类型"><a href="#日期-时间类型" class="headerlink" title="日期/时间类型"></a><strong>日期/时间类型</strong></h2><p>下图展示了 mysql 日期/时间数据类型的详细信息。<br><img src="/images/mysql/2.jpg"></p><h3 id="注意-2"><a href="#注意-2" class="headerlink" title="注意"></a>注意</h3><ul><li>时间类型其实是格式化的字符串，如当插入一条date类型的记录分量时，使用这样的语句<code>INSERT INTO date_table(dt) VALUES('2023-02-07')</code></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MySQL-Datatype&quot;&gt;&lt;a href=&quot;#MySQL-Datatype&quot; class=&quot;headerlink&quot; title=&quot;MySQL Datatype&quot;&gt;&lt;/a&gt;MySQL Datatype&lt;/h1&gt;&lt;p&gt;mysql 数据类型反应在列（字段上），不同</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
  </entry>
  
  <entry>
    <title>Basic SQL</title>
    <link href="http://example.com/2023/02/10/mysql_1/"/>
    <id>http://example.com/2023/02/10/mysql_1/</id>
    <published>2023-02-10T13:13:58.221Z</published>
    <updated>2023-02-10T13:13:58.221Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-SQL"><a href="#Basic-SQL" class="headerlink" title="Basic SQL"></a>Basic SQL</h1><p>数据库语言可分为两个部分，DDL和DML。</p><ul><li>DDL（Data Definitoin Language）：描述数据库中要存储的现实世界实体（操作表），操作数据库的结构等</li><li>DML（Data Manipulation Language）：操作数据库存储的对象或记录</li></ul><h2 id="SQL-DDL-数据定义"><a href="#SQL-DDL-数据定义" class="headerlink" title="SQL DDL:数据定义"></a><strong>SQL DDL:数据定义</strong></h2><h3 id="数据库命令"><a href="#数据库命令" class="headerlink" title="数据库命令"></a>数据库命令</h3><ul><li>查询数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">--显示当前mysql服务器中所有数据库</span><br><span class="hljs-keyword">SHOW</span> DATABASES;<br><span class="hljs-comment">--显示创建某一数据库时的语句</span><br><span class="hljs-keyword">SHOW</span> <span class="hljs-keyword">CREATE</span> DATABASE 数据库名称;<br></code></pre></td></tr></tbody></table></figure></li><li>创建新数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">--字符集为utf-8(默认就是)，校验规则utf8_general_ci(不区分大小写，默认就是)</span><br><span class="hljs-comment">--创建数据库名称是使用反引号规避关键字</span><br><span class="hljs-comment">--数据库的字符集和校对规则会成为该数据库下表的默认规则</span><br><span class="hljs-comment">--utf8_bin区分大小写，大小写指的是数据库中记录的字符大小写 </span><br><span class="hljs-keyword">CREATE</span> DATABASE 数据库名称 <span class="hljs-type">CHARACTER</span> <span class="hljs-keyword">SET</span> 字符集 <span class="hljs-keyword">COLLATE</span> 校验规则;<br></code></pre></td></tr></tbody></table></figure></li><li>删除数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DROP</span> DATABASE 数据库名称;<br></code></pre></td></tr></tbody></table></figure></li><li>备份和恢复数据库  <ul><li>备份数据库即将数据库导出至其他文件，恢复数据库即从备份文件恢复至某一DBMS，备份与恢复操作不是sql指令需要在shell执行，mysql提供的程序在mysql目录中的bin文件夹下，备份其实就是备份数据库生命周期内从头到尾对应的sql语句。</li><li>备份：<code>./mysqldump -u 用户名 -p -B 数据库1 数据库2 ... &gt; 备份文件的路径</code></li><li>恢复：直接用dbms终端打开备份sql文件并执行或者打开mysql命令行执行：<code>source 备份文件路径</code></li></ul></li></ul><h3 id="表命令"><a href="#表命令" class="headerlink" title="表命令"></a>表命令</h3><ul><li>查询表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">--显示当前数据库中全部表</span><br><span class="hljs-keyword">SHOW</span> TABLES;<br><span class="hljs-comment">--显示某一张表的信息</span><br><span class="hljs-keyword">DESC</span> 表名<br></code></pre></td></tr></tbody></table></figure></li><li>创建新表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql">   <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> 表名<br>   (<br>字段名称<span class="hljs-number">1</span> 数据类型(容量) 约束，<br>字段名称<span class="hljs-number">2</span> 数据类型(容量) 约束，<br>字段名称<span class="hljs-number">3</span> 数据类型(容量) 约束,<br>...    <br>   )<span class="hljs-type">CHARACTER</span> <span class="hljs-keyword">SET</span> 字符集 <span class="hljs-keyword">COLLATE</span> 校验规则 ENGINE 引擎;<br></code></pre></td></tr></tbody></table></figure></li><li>删除表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">TABLE</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 添加列<br><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> 表名<br><span class="hljs-keyword">ADD</span> (<br>字段名 数据类型(容量) 约束,<br>字段名 数据类型(容量) 约束,<br>...<br>);<br><br>#修改列<br><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> 表名<br>MODIFY 字段名 数据类型(容量) 约束;<br><br>#修改列名<br><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> 表名<br>CHANGE 旧字段名 新字段名 数据类型(容量) 约束;<br><br>#删除列<br><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> 表名<br><span class="hljs-keyword">DROP</span> 字段名;<br><br># 修改表名<br>RENAME <span class="hljs-keyword">TABLE</span> 旧表名 <span class="hljs-keyword">TO</span> 新表名;<br></code></pre></td></tr></tbody></table></figure></li><li>备份及恢复表<ul><li>同样使用mysqldump,使用命令：<br><code>mysqldump -u 用户名 -p密码 数据库名 表1 表2... &gt; 备份文件路径</code></li><li>恢复表同恢复数据库的操作，要注意切换到目标数据库之后再执行备份语句</li></ul></li></ul><h2 id="SQL-DML-数据操作"><a href="#SQL-DML-数据操作" class="headerlink" title="SQL DML:数据操作"></a><strong>SQL DML:数据操作</strong></h2><ul><li>从表中获取数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 获取表中某一列数据<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br># 或获取表中所有数据<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 更新符合要求的行中某列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称 <span class="hljs-operator">=</span> 新值 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br># 更新符合要求的行中若干列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称<span class="hljs-number">1</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">2</span> <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br></code></pre></td></tr></tbody></table></figure></li><li>删除表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 删除表中的某行<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 值;<br># 删除所有行（在不删除表的情况下删除所有的行。这意味着表的结构、字段和索引都是完整的）<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称;<br><span class="hljs-keyword">delete</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>向表中插入数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称 <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br># 指定要插入的列<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称(列<span class="hljs-number">1</span>,列<span class="hljs-number">2</span>,...) <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br></code></pre></td></tr></tbody></table></figure></li><li>去重后从表中获取某列中值  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>根据条件获取表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">and</span> 列 运算符 值;<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">or</span> 列 运算符 值;<br></code></pre></td></tr></tbody></table></figure></li><li>根据指定的列对结果集进行排序  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br># 先按列名称<span class="hljs-number">1</span>排序，然后以列名称<span class="hljs-number">3</span>排序<br><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span>,列名称<span class="hljs-number">3</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)],列名称<span class="hljs-number">3</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Basic-SQL&quot;&gt;&lt;a href=&quot;#Basic-SQL&quot; class=&quot;headerlink&quot; title=&quot;Basic SQL&quot;&gt;&lt;/a&gt;Basic SQL&lt;/h1&gt;&lt;p&gt;数据库语言可分为两个部分，DDL和DML。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DDL（Dat</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
  </entry>
  
  <entry>
    <title>Inversion of Control</title>
    <link href="http://example.com/2022/09/15/IoC/"/>
    <id>http://example.com/2022/09/15/IoC/</id>
    <published>2022-09-15T11:31:53.922Z</published>
    <updated>2022-09-15T11:31:53.922Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Inversion-of-Control"><a href="#Inversion-of-Control" class="headerlink" title="Inversion of Control"></a>Inversion of Control</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><h3 id="DIP、IoC、DI、SL-是什么"><a href="#DIP、IoC、DI、SL-是什么" class="headerlink" title="DIP、IoC、DI、SL 是什么"></a>DIP、IoC、DI、SL 是什么</h3><ul><li><p><strong>DIP</strong>，即依赖倒置原则（Dependency inversion principle），不是一种技术，而是一种面向对象设计思想，其使得高层次的类不依赖于低层次的类的实现细节，依赖关系被倒置（高低层次的类都反而依赖接口），从而使得低层次类依赖于高层次类的需求抽象。</p></li><li><p><strong>IoC</strong>，即控制反转（Inversion of Control）。也是一种面向对象设计思想，是相对于DIP更进一步的解耦（连在所需类中自行创建具体的对象也不需要）。实现方式便是将用户或框架设计好的对象交给容器控制，而非传统编程过程中在其他对象内部控制，这种方式在现代的web开发、图像开发中很常用。</p></li><li><p><strong>SL</strong>，即服务定位器（Service Locator）。是IoC的一种具体实现方法，实现了按需返回服务实例，在该模式中，服务会被提前注册到服务定位器中，并通过 ID 唯一标识。应用需要某个服务时，通过 ID 或所需类型从服务定位器中得到这个服务的实例。服务定位器解耦了服务调用者和具体的服务实现。</p></li><li><p><strong>DI</strong>，即依赖注入（Dependency Injection）。是IoC的一种具体实现方法，组件之间的依赖关系由容器在 <em>运行期间</em> 自行决定（使用反射技术），由容器动态地将依赖项注入到组件当中。</p></li></ul><h3 id="IoC-DI的优点"><a href="#IoC-DI的优点" class="headerlink" title="IoC/DI的优点"></a>IoC/DI的优点</h3><ul><li><p>没有引入IOC之前，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，A直接使用new关键字创建B的实例，程序高度耦合，效率低下，无论是创建还是使用B对象，控制权都在自己手上。</p></li><li><p>传统的代码，每个对象负责管理与自己需要依赖的对象，导致如果需要切换依赖对象的实现类时，需要修改很多地方。同时，过度耦合也使得对象难以进行单元测试。</p></li><li><p>实现IoC之后，依赖关系高度解耦，在可见的代码中不需要手动new，这个过程被推迟到运行过程中由容器动态控制，使单元测试、程序修改更加方便，并且解放了程序员在大型程序复杂依赖的手动控制。</p></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>传统编程</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-comment">//需要手动创建依赖实例,更换某一组件时，组件和应用代码都要更改</span><br>DialModule dialModule = <span class="hljs-keyword">new</span> DialModule();<br>VideoPlayerModule videoplayerModule=<span class="hljs-keyword">new</span> VideoPlayerModule();<br>Phone phone=<span class="hljs-keyword">new</span> Phone(videoplayerModule,dialModule);<br><br>phone.Play(<span class="hljs-keyword">new</span> Video(<span class="hljs-string">"the Suit"</span>, <span class="hljs-number">120</span>));<br>phone.Dial(<span class="hljs-string">"2838896"</span>);<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title">DialModule</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"calling <span class="hljs-subst">{phone_number}</span>..."</span>);<br>    }<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">Video</span><br>{<br>    <span class="hljs-keyword">public</span> <span class="hljs-built_in">string</span> Name { <span class="hljs-keyword">get</span>; }<br>    <span class="hljs-keyword">public</span> <span class="hljs-built_in">int</span> Period { <span class="hljs-keyword">get</span>; }<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Video</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> name,<span class="hljs-built_in">int</span> period</span>)</span><br>    {<br>        Name=name;<br>        Period=period;<br>    }<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">VideoPlayerModule</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Play</span>(<span class="hljs-params">Video video</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"playing <span class="hljs-subst">{video.Name}</span>"</span>);<br>    }<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">Phone</span><br>{<br>    <span class="hljs-keyword">private</span> VideoPlayerModule _videoPlayerModule;<br>    <span class="hljs-keyword">private</span> DialModule _dialModule;<br><br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Phone</span>(<span class="hljs-params">VideoPlayerModule videoPlayerModule, DialModule dialModule</span>)</span><br>    {<br>        _videoPlayerModule = videoPlayerModule;<br>        _dialModule = dialModule;<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        _dialModule.Dial(phone_number);<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Play</span>(<span class="hljs-params">Video video</span>)</span><br>    {<br>        _videoPlayerModule.Play(video);<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>接口编程（DIP）</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-comment">//需要手动创建依赖实例,更换某一组件时，只需要在这里改</span><br>IDial dialModule = <span class="hljs-keyword">new</span> CommonDial();<br><br>Phone phone=<span class="hljs-keyword">new</span> Phone(dialModule);<br><br>phone.Dial(<span class="hljs-string">"xxxxxxx"</span>);<br><br><br><span class="hljs-keyword">interface</span> <span class="hljs-title">IDial</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span>;<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">CommonDial</span>:<span class="hljs-title">IDial</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"calling <span class="hljs-subst">{phone_number}</span>..."</span>);<br>    }<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">PrivateDial</span> : <span class="hljs-title">IDial</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"secretly calling <span class="hljs-subst">{phone_number}</span>..."</span>);<br>    }<br>}<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title">Phone</span><br>{<br>    <br>    <span class="hljs-keyword">private</span> IDial _dialModule;<br><br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Phone</span>(<span class="hljs-params">IDial dialModule</span>)</span><br>    {<br>        <br>        _dialModule = dialModule;<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        _dialModule.Dial(phone_number);<br>    }<br><br>    <br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>控制反转（IoC）</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-comment">//微软提供的依赖注入扩展</span><br><span class="hljs-keyword">using</span> Microsoft.Extensions.DependencyInjection;<br><br><br><span class="hljs-comment">//实例化一个容器</span><br>IServiceCollection services=<span class="hljs-keyword">new</span> ServiceCollection();<br><span class="hljs-comment">//注册服务</span><br>services.AddScoped&lt;IDial, CommonDial&gt;();<br>services.AddScoped&lt;IDial, PrivateDial&gt;();<br>services.AddScoped&lt;Phone&gt;();<br><br><span class="hljs-comment">//获取服务定位器</span><br><span class="hljs-keyword">var</span> provider=services.BuildServiceProvider();<br><span class="hljs-comment">//首个实例的创建需要使用服务定位器的方式获得</span><br><span class="hljs-comment">//之后的依赖，通过依赖注入的方式在运行中由容器分配</span><br>Phone? phone=provider.GetService&lt;Phone&gt;();<br><br><span class="hljs-keyword">if</span>(phone!=<span class="hljs-literal">null</span>)<br>{<br>    phone.Dial(<span class="hljs-string">"xxxxxxxxxxxx"</span>);<br>}<br><br><span class="hljs-keyword">interface</span> <span class="hljs-title">IDial</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span>;<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">CommonDial</span>:<span class="hljs-title">IDial</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"calling <span class="hljs-subst">{phone_number}</span>..."</span>);<br>    }<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">PrivateDial</span> : <span class="hljs-title">IDial</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"secretly calling <span class="hljs-subst">{phone_number}</span>..."</span>);<br>    }<br>}<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title">Phone</span><br>{<br>    <br>    <span class="hljs-keyword">private</span> IDial _dialModule;<br><br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Phone</span>(<span class="hljs-params">IDial dialModule</span>)</span><br>    {<br>        <br>        _dialModule = dialModule;<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Dial</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> phone_number</span>)</span><br>    {<br>        _dialModule.Dial(phone_number);<br>    }<br><br>    <br>}<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Inversion-of-Control&quot;&gt;&lt;a href=&quot;#Inversion-of-Control&quot; class=&quot;headerlink&quot; title=&quot;Inversion of Control&quot;&gt;&lt;/a&gt;Inversion of Control&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Computer Literacy" scheme="http://example.com/categories/Computer-Literacy/"/>
    
    
    <category term="IoC" scheme="http://example.com/tags/IoC/"/>
    
    <category term="DI" scheme="http://example.com/tags/DI/"/>
    
    <category term="DIP" scheme="http://example.com/tags/DIP/"/>
    
    <category term="Design Patterns" scheme="http://example.com/tags/Design-Patterns/"/>
    
  </entry>
  
  <entry>
    <title>Asynchronous programming</title>
    <link href="http://example.com/2022/09/15/AsyncAwait/"/>
    <id>http://example.com/2022/09/15/AsyncAwait/</id>
    <published>2022-09-15T01:46:23.596Z</published>
    <updated>2022-09-15T01:46:23.596Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Asynchronous-programming"><a href="#Asynchronous-programming" class="headerlink" title="Asynchronous programming"></a>Asynchronous programming</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a>Concept and Principle</h2><ul><li><p>异步编程与多线程编程</p><ul><li>异步编程一般在单线程（或线程池）上实现并发执行，不涉及线程切换，减小了维护多线程的开销。而多线程编程在多核处理器上做并行执行，需要考虑线程间同步以及线程切换等问题</li><li>异步编程适合于I/O密集型操作，而多线程编程适合于计算密集型工作。这是因为在I/O过程中线程会被阻塞但依然要维护其占用的内存资源，并且还有进行线程切换开销；而异步编程则避免了这些问题，将I/O操作封装为异步函数，cpu在执行到I/O操作时向DMA发送指令后直接执行其他代码，当I/O操作结束后执行回调</li></ul></li><li><p>async/await 结构</p><ul><li>async与await被许多语言都设置为了异步编程的语法糖，有多种实现（如python中的coroutine、c#中的Task、js中的promise）。</li><li>async/await 结构可分成三部分：<ol><li>调用方法：该方法调用异步方法，在异步方法执行其任务的时候继续执行该方法下其他代码</li><li>异步方法：在执行完成前立即返回调用方法，在调用方法继续执行的过程中完成任务</li><li>await 表达式：用于异步方法内部，指出需要异步执行的任务(在await之前的代码还都是同步执行)。一个异步方法可以包含多个 await 表达式，当异步方法中不包含await表达式时，将会同步执行（顺序执行）异步方法</li></ol></li></ul></li><li><p>async/await执行过程</p><ol><li>调用方法执行到由async修饰的异步方法，进入该方法先同步执行</li><li>顺序执行到await修饰的语句，调用该语句后立即返回原调用方法</li><li>在原调用方法中继续执行后面的代码，同时异步方法也在执行await的语句</li></ol></li><li><p>注意</p><ul><li>调用方法和异步方法可能是并行的，也可能是并发的，这方面不需要程序员考虑，只需要知道异步方法的执行不影响调用方法的执行</li><li>异步方法正常的返回值并不是方法内指明的返回值（python中返回coroutine、c#中返回Task、js中返回promise），但用await修饰会直接返回异步方法内指明的返回值</li><li>如果异步方法还未执行完，而在调用方法中就要使用异步方法的result，则会死等到异步方法执行完毕</li><li>异步方法一般回调联合使用</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><ul><li><p>c#中实现的异步编程基于Task，底层是线程池</p>  <figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><br>App app = <span class="hljs-keyword">new</span> App();<br>app.Run();<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">NeuralNetwork</span><br>{<br>    <span class="hljs-keyword">public</span> <span class="hljs-built_in">double</span> Acc { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }<br>    <span class="hljs-keyword">public</span> <span class="hljs-built_in">double</span> Loss { <span class="hljs-keyword">get</span>; <span class="hljs-keyword">set</span>; }<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-built_in">string</span> _name;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">NeuralNetwork</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> name</span>)</span><br>    {<br>        _name = name;<br>    }<br><br>    <span class="hljs-comment">//训练结束后调用回调函数</span><br>    <span class="hljs-comment">//由于是异步方法，其不会阻塞界面线程</span><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> <span class="hljs-keyword">void</span> <span class="hljs-title">TrainAsync</span>(<span class="hljs-params">Action&lt;<span class="hljs-built_in">double</span>, <span class="hljs-built_in">double</span>&gt; action</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">"我之前还是同步执行"</span>);<br>        <span class="hljs-keyword">await</span> Task.Delay(<span class="hljs-number">3000</span>);<br>        Acc = <span class="hljs-number">0.9f</span>;<br>        Loss = <span class="hljs-number">0.00466f</span>;<br>        action(Acc, Loss);<br>    }<br><br>    <span class="hljs-comment">//虽然也是异步方法，但方法完成后不会通知调用方法，在此场景下还是低效</span><br>    <span class="hljs-comment">//在不需要回调的情境下可以使用</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">async</span> Task&lt;Tuple&lt;<span class="hljs-built_in">double</span>, <span class="hljs-built_in">double</span>&gt;&gt; TrainAsync()<br>    {<br>        Console.WriteLine(<span class="hljs-string">"我之前还是同步执行"</span>);<br><br>        <span class="hljs-keyword">await</span> Task.Delay(<span class="hljs-number">3000</span>);<br>        Acc = <span class="hljs-number">0.9f</span>;<br>        Loss = <span class="hljs-number">0.00466f</span>;<br><br>        <span class="hljs-keyword">return</span> Tuple.Create(Acc, Loss);<br>    }<br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">App</span><br>{<br>    <span class="hljs-keyword">private</span> NeuralNetwork _network = <span class="hljs-keyword">new</span> NeuralNetwork(<span class="hljs-string">"Cnn"</span>);<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">RunWithCallBack</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">30</span>; i++)<br>        {<br>            Thread.Sleep(<span class="hljs-number">500</span>);<br>            Console.WriteLine(<span class="hljs-string">"running..."</span>);<br><br>            <span class="hljs-comment">//模拟训练模型，需要长时间操作</span><br>            <span class="hljs-keyword">if</span> (i == <span class="hljs-number">3</span>)<br>                <span class="hljs-comment">//进入到函数中，执行到调用await后立即返回到RunWithCallBack继续执行</span><br>                _network.TrainAsync(<span class="hljs-keyword">this</span>.Show);<br>        }<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Run</span>(<span class="hljs-params"></span>)</span><br>    {<br>        Task&lt;Tuple&lt;<span class="hljs-built_in">double</span>, <span class="hljs-built_in">double</span>&gt;&gt; t = <span class="hljs-literal">null</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">30</span>; i++)<br>        {<br><br>            Thread.Sleep(<span class="hljs-number">500</span>);<br>            Console.WriteLine(<span class="hljs-string">"running..."</span>);<br>            <br><br>            <span class="hljs-comment">//模拟训练模型，需要长时间操作</span><br>            <span class="hljs-keyword">if</span> (i == <span class="hljs-number">3</span>)<br>            {<br>                <span class="hljs-comment">//如果在这里直接输出，则依然会产生阻塞的效果</span><br>                <span class="hljs-comment">//Console.WriteLine(_network.TrainAsync().Result);</span><br><br>                <span class="hljs-comment">//使用await，await返回的直接就是_network.TrainAsync().Result</span><br>                t = _network.TrainAsync();<br><br>            }<br><br>            <span class="hljs-comment">//在第十次刷新时检查是否训练完成，没完成继续等</span><br>            <span class="hljs-keyword">if</span> (i == <span class="hljs-number">10</span>)<br>                Console.WriteLine(t.Result);<br><br>        }<br><br><br>    }<br><br>    <span class="hljs-comment">//界面展示精度与损失</span><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Show</span>(<span class="hljs-params"><span class="hljs-built_in">double</span> acc, <span class="hljs-built_in">double</span> loss</span>)</span><br>    {<br>        Console.WriteLine(<span class="hljs-string">$"acc:<span class="hljs-subst">{acc}</span>,loss:<span class="hljs-subst">{loss}</span>"</span>);<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure></li><li><p>python实现的异步编程使用了协程概念，与JS一样底层都是单线程</p>  <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#asyncio包帮助实现异步编程</span><br><span class="hljs-comment">#将所有异步和同步函数加入一个事件循环，在这个循环内按异步定义为每个函数分配时间片</span><br><span class="hljs-keyword">import</span> asyncio<br><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">RunNN</span>(<span class="hljs-params">f</span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"===="</span>)<br>    <span class="hljs-keyword">await</span> asyncio.sleep(<span class="hljs-number">2</span>)<br>    <br>    loss=<span class="hljs-number">0.0045</span><br>    acc=<span class="hljs-number">0.97</span><br>    f(loss,acc)<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">app</span>():</span><br>    <span class="hljs-keyword">while</span>(<span class="hljs-literal">True</span>):<br>        <span class="hljs-keyword">await</span> asyncio.sleep(<span class="hljs-number">0.5</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">"running..."</span>)<br>        <br><br>asyncio.gather(app(), RunNN(<span class="hljs-keyword">lambda</span> x, y: <span class="hljs-built_in">print</span>(<span class="hljs-string">f"loss:<span class="hljs-subst">{x}</span>,acc<span class="hljs-subst">{y}</span>"</span>)))<br><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    <span class="hljs-keyword">await</span> asyncio.gather(app(), RunNN(<span class="hljs-keyword">lambda</span> x, y: <span class="hljs-built_in">print</span>(<span class="hljs-string">f"loss:<span class="hljs-subst">{x}</span>,acc<span class="hljs-subst">{y}</span>"</span>)))<br><br>asyncio.run(main())<br><br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Asynchronous-programming&quot;&gt;&lt;a href=&quot;#Asynchronous-programming&quot; class=&quot;headerlink&quot; title=&quot;Asynchronous programming&quot;&gt;&lt;/a&gt;Asynchronous p</summary>
      
    
    
    
    <category term="Computer Literacy" scheme="http://example.com/categories/Computer-Literacy/"/>
    
    
    <category term="Asynchronous-programming" scheme="http://example.com/tags/Asynchronous-programming/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Object Detection</title>
    <link href="http://example.com/2022/09/15/d2l_20/"/>
    <id>http://example.com/2022/09/15/d2l_20/</id>
    <published>2022-09-15T01:45:20.166Z</published>
    <updated>2022-09-15T01:45:20.166Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><p>目标检测任务与图像分类不同，目标检测要在图片中识别出多个不同的目标并表明每个目标的位置</p><ul><li><p>边缘框</p><ul><li>边缘框表示了目标的位置，有两种坐标表示法<ul><li>(左上x，左上y，右下x，右下y)</li><li>(左上x，左上y，边框宽，边框高)</li></ul></li></ul></li><li><p>锚框</p><ul><li>锚框在目标检测算法中很常用，许多目标检测算法都用到了这项技术</li><li>锚框是对边缘框的一个猜测：<ul><li>提出多个被称为锚框的区域</li><li>预测每个锚框中是否含有目标物体</li><li>如果有，则继续预测从这个锚框到真实边缘框的偏移</li></ul></li><li>使用交并比（IoU）来计算两个框的相似度(预测框和标签框)<br><img src="/images/d2l/20/1.png"></li><li>赋予锚框标号<ul><li>在训练时，每个锚框都是一个训练样本，每次读取一张图片都要进行一次赋予锚框标号的操作</li><li>将每个锚框要么标注为背景，要么标注为与真实边缘框相关</li><li>可能生成大量锚框导致负样本过多</li><li>锚框可固定生成，或根据图片生成，甚至随机</li><li>假设一个图片有4个标签边缘框，生成了9个锚框，一种赋予标号的算法如下图<br><img src="/images/d2l/20/2.png"></li></ul></li><li>在预测时，使用非极大值抑制（NMS）输出<br><img src="/images/d2l/20/3.png"></li></ul></li><li><p>目标检测常用算法</p><ol><li><p>Faster R-CNN<br>图片进入一个CNN后分成两条路线，一条路线进入RPN（负责生成锚框），另一条路线经过RoI pooling（将不同大小的锚框提取为同一大小）后连接至全连接层，最后做出分类以及边缘框预测。Faster R-CNN相对其他算法来说还是很慢，但精度很高，适合刷榜。<br><img src="/images/d2l/20/4.png"></p></li><li><p>SSD（单发多框检测）<br>SSD由一个基础的网络来抽取特征，然后多个卷积层块来减半高宽。每段都会生成锚框，底部段拟合小物体，顶部段拟合大物体。每个锚框都会预测类别和边缘框<br><img src="/images/d2l/20/5.png"></p></li><li><p>YOLO<br>SSD中锚框有大量重叠，YOLO将图片均匀分成SxS个锚框，每个锚框预测B个边缘框。</p></li></ol></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li><p>边缘框实现</p>  <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>img=Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">"./1.jpg"</span>)<br>plt.imshow(img)<br>plt.show()<br>fig=plt.imshow(img)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">box_corner_to_center</span>(<span class="hljs-params">boxes</span>):</span><br>    <span class="hljs-string">"""(左上，右下)转换到(中间，宽度，高度)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        boxs : 一组边缘框 </span><br><span class="hljs-string">    """</span><br>    x1, y1, x2, y2 = boxes[:, <span class="hljs-number">0</span>], boxes[:, <span class="hljs-number">1</span>], boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">3</span>]<br>    cx = (x1 + x2) / <span class="hljs-number">2</span><br>    cy = (y1 + y2) / <span class="hljs-number">2</span><br>    w = x2 - x1<br>    h = y2 - y1<br>    <span class="hljs-comment"># 将新坐标堆叠起来变为二维Tensor，注意与cat不同</span><br>    boxes = torch.stack((cx, cy, w, h))<br>    <span class="hljs-comment"># 返回转置</span><br>    <span class="hljs-keyword">return</span> boxes.T<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">box_center_to_corner</span>(<span class="hljs-params">boxes</span>):</span><br>    <span class="hljs-string">"""(中间，宽度，高度)转换到(左上，右下)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        boxs : 一组边缘框 </span><br><span class="hljs-string">    """</span><br>    cx, cy, w, h = boxes[:, <span class="hljs-number">0</span>], boxes[:, <span class="hljs-number">1</span>], boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">3</span>]<br>    x1 = cx - <span class="hljs-number">0.5</span> * w<br>    y1 = cy - <span class="hljs-number">0.5</span> * h<br>    x2 = cx + <span class="hljs-number">0.5</span> * w<br>    y2 = cy + <span class="hljs-number">0.5</span> * h<br>    boxes = torch.stack((x1, y1, x2, y2))<br>    <span class="hljs-keyword">return</span> boxes.T<br><br>x=torch.tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]])<br>x=box_corner_to_center(x)<br><span class="hljs-built_in">print</span>(x)<br>x=box_center_to_corner(x)<br><span class="hljs-built_in">print</span>(x)<br><br><br>box1=torch.tensor([<span class="hljs-number">360</span>,<span class="hljs-number">170</span>,<span class="hljs-number">650</span>,<span class="hljs-number">650</span>])<br>box2=torch.tensor([<span class="hljs-number">120</span>,<span class="hljs-number">210</span>,<span class="hljs-number">350</span>,<span class="hljs-number">290</span>])<br><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_bboxes</span>(<span class="hljs-params">fig,bboxes,color</span>):</span><br>    <span class="hljs-string">"""显示带边缘框的图片</span><br><span class="hljs-string"></span><br><span class="hljs-string">    """</span> <br>    <span class="hljs-keyword">for</span> b,c <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(bboxes,color):<br>        fig.axes.add_patch(_bbox_to_rect(b, c))<br>    plt.show()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_bbox_to_rect</span>(<span class="hljs-params">bbox, color</span>):</span><br>        <span class="hljs-keyword">return</span> plt.Rectangle(xy=(bbox[<span class="hljs-number">0</span>], bbox[<span class="hljs-number">1</span>]), width=bbox[<span class="hljs-number">2</span>] - bbox[<span class="hljs-number">0</span>],<br>                                height=bbox[<span class="hljs-number">3</span>] - bbox[<span class="hljs-number">1</span>], fill=<span class="hljs-literal">False</span>,<br>                                edgecolor=color, linewidth=<span class="hljs-number">2</span>)<br><br><br><br>show_bboxes(fig,[box1,box2],[<span class="hljs-string">'red'</span>,<span class="hljs-string">'green'</span>])<br><br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Object-Detection&quot;&gt;&lt;a href=&quot;#Object-Detection&quot; class=&quot;headerlink&quot; title=&quot;Object Detection&quot;&gt;&lt;/a&gt;Object Detection&lt;/h1&gt;&lt;h2 id=&quot;Concept-a</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Fine Tune</title>
    <link href="http://example.com/2022/09/15/d2l_19/"/>
    <id>http://example.com/2022/09/15/d2l_19/</id>
    <published>2022-09-15T01:45:16.954Z</published>
    <updated>2022-09-15T01:45:16.954Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine Tune"></a>Fine Tune</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li><p>一般神经网络的架构都分为两块：</p><ul><li>特征抽取部分</li><li>线性分类部分<br><img src="/images/d2l/19/1.png"></li></ul></li><li><p>微调  </p><ul><li><p>一个模型特征提取部分可以作为上游模型训练其他任务<br><img src="/images/d2l/19/2.png"></p></li><li><p>pre-train的过程一般是一个大数据集上的正常训练任务，而在fine-tune的过程中使用更强的正则化、更小的学习率、更少的epoch</p></li><li><p>原数据集和目标数据集要相似，且原数据集比目标数据集要大</p></li></ul></li><li><p>微调的一些技巧</p><ul><li>重用分类器权重：原数据集中也可能有下游任务数据的部分标号，可以用预训练模型分类器中对应标号的对应向量来初始化下游任务分类部分</li><li>固定前面的层：神经网络中靠近输入的层更加通用，可以固定底部的一些层的参数不参与更新</li><li>对于个人或小企业来说，通常不会从头开始训练模型，而是进行微调</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models,transforms<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> d2l<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">64</span>)<br><br>train_aug=transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    transforms.RandomHorizontalFlip(),<br>    <span class="hljs-comment"># transforms.ToTensor(), #在load时已经将PIL转化为了Tensor</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>,<span class="hljs-number">0.456</span>,<span class="hljs-number">0.406</span>],[<span class="hljs-number">0.229</span>,<span class="hljs-number">0.224</span>,<span class="hljs-number">0.225</span>]),<br>    <br>])<br><br>test_aug=transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    <span class="hljs-comment"># transforms.ToTensor(),</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>,<span class="hljs-number">0.456</span>,<span class="hljs-number">0.406</span>],[<span class="hljs-number">0.229</span>,<span class="hljs-number">0.224</span>,<span class="hljs-number">0.225</span>])<br>])<br><br><br><span class="hljs-comment"># 指定预训练模型</span><br>finetune_res=models.resnet18(pretrained=<span class="hljs-literal">True</span>,progress=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 将分类器部分重新设计，in_features属性记录了原模型的本层的输入特征数</span><br>finetune_res.fc=nn.Linear(finetune_res.fc.in_features,<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 初始化新分类器参数</span><br>nn.init.xavier_uniform_(finetune_res.fc.weight)<br><br>loss_f=nn.CrossEntropyLoss()<br><span class="hljs-comment"># 增加正则化并且学习率应该设得很小</span><br>opt=optim.Adam(finetune_res.parameters(),weight_decay=<span class="hljs-number">0.1</span>,lr=<span class="hljs-number">5e-5</span>)<br><br><span class="hljs-comment"># 不需要迭代很多轮</span><br>d2l.train(<br>    <span class="hljs-number">5</span>,loss_f,opt,finetune_res,train_iter,<br>    save_name=<span class="hljs-string">"res18_pretrained"</span>,device=torch.device(<span class="hljs-string">"cuda:0"</span>),<br>    aug=train_aug<br>    )<br><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Fine-Tune&quot;&gt;&lt;a href=&quot;#Fine-Tune&quot; class=&quot;headerlink&quot; title=&quot;Fine Tune&quot;&gt;&lt;/a&gt;Fine Tune&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#Conc</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Data Augmentation</title>
    <link href="http://example.com/2022/09/15/d2l_18/"/>
    <id>http://example.com/2022/09/15/d2l_18/</id>
    <published>2022-09-15T01:45:13.995Z</published>
    <updated>2022-09-15T01:45:13.995Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><p>我们收集的训练数据通常很难覆盖到未来可能部署的全部场景（比如人脸识别的应用可能会部署到不同摄像头状况、天气、时间等的场景）。数据增强则在一个已有的数据上做数据变换，起到增大数据集的作用，使其有更好的多样性。</p><ul><li><p>数据增强</p><ul><li>数据增强只在训练时进行</li><li>一般采用在线生成的方式</li><li>数据增强假设测试环境中会出现增强后的数据，如果测试环境和训练集高度一致则没有必要做数据增强</li></ul></li><li><p>数据增强方法</p><ul><li>翻转（上下、左右翻转）</li><li>切割（随机高宽比、大小、位置切割一块，然后再变为固定大小）</li><li>颜色（色调、饱和度、亮度）</li><li>其他（高斯模糊、锐化、遮挡等）</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pytorch在提供transforms模块中提供了很多数据增广函数</span><br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_images</span>(<span class="hljs-params">imgs, num_rows, num_cols, titles=<span class="hljs-literal">None</span>, scale=<span class="hljs-number">1.5</span></span>):</span>  <br>    <br>    figsize = (num_cols * scale, num_rows * scale)<br>    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)<br>    axes = axes.flatten()<br>    <span class="hljs-keyword">for</span> i, (ax, img) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(axes, imgs)):<br>        <span class="hljs-keyword">if</span> torch.is_tensor(img):<br>            ax.imshow(transforms.ToPILImage()(img))<br>        <span class="hljs-keyword">else</span>:<br>            ax.imshow(img)<br>        ax.axes.get_xaxis().set_visible(<span class="hljs-literal">False</span>)<br>        ax.axes.get_yaxis().set_visible(<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">if</span> titles:<br>            ax.set_title(titles[i])<br>    plt.show()<br>    <span class="hljs-keyword">return</span> axes<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">4</span>,(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br>X,y=<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(test_iter))<br><br><span class="hljs-comment"># 显示tensor形式的图片</span><br><span class="hljs-comment"># X=X.view(4,224,224)</span><br><br>show_images(X,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,scale=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 数据增广</span><br><span class="hljs-comment"># trans=transforms.RandomCrop((20,20))</span><br><span class="hljs-comment"># trans=transforms.RandomHorizontalFlip()</span><br><span class="hljs-comment"># trans=transforms.GaussianBlur(5)</span><br>trans=transforms.RandomErasing(<span class="hljs-number">1</span>)<br>show_images(trans(X),<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,scale=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 显示使用PIL读进内存的图片</span><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br>test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        download=<span class="hljs-literal">True</span>,transform=transforms.ToTensor()<br>    )<br><br><span class="hljs-built_in">print</span>(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape)<br><span class="hljs-comment"># 将Tensor形式转换为PIL形式</span><br>image=transforms.ToPILImage()(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>image.show()<br><span class="hljs-keyword">import</span> torchvision<br>test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        download=<span class="hljs-literal">True</span><br>    )<br><span class="hljs-comment"># 第i张图片test_data[i][0]，test_data[i][1]是第一张图片的标签</span><br>d2l.show_images([test_data[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>)], <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, scale=<span class="hljs-number">0.8</span>)<br><br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">r"./1.jpg"</span>)<br><span class="hljs-built_in">print</span>(image)<br><span class="hljs-comment"># 将PIL形式转换为Tensor形式</span><br><span class="hljs-built_in">print</span>(transforms.ToTensor()(image))<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Data-Augmentation&quot;&gt;&lt;a href=&quot;#Data-Augmentation&quot; class=&quot;headerlink&quot; title=&quot;Data Augmentation&quot;&gt;&lt;/a&gt;Data Augmentation&lt;/h1&gt;&lt;h2 id=&quot;Conce</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: ResNet</title>
    <link href="http://example.com/2022/09/15/d2l_17/"/>
    <id>http://example.com/2022/09/15/d2l_17/</id>
    <published>2022-09-15T01:45:10.964Z</published>
    <updated>2022-09-15T01:45:10.964Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li><p>加更多的层不一定总是改进精度</p><ul><li>新的层可能是使模型收敛范围偏差到一个不符合预期的区域</li><li>ResNet使各层更容易学会恒等变换，从而更容易使模型收敛范围达到Nested function classes<br><img src="/images/d2l/17/1.png"></li></ul></li><li><p>残差块</p><ul><li>基本的ResBlock结构如下，f(x)+x保证了包含原收敛范围<br><br><img src="/images/d2l/17/2.png"></li><li>具体使用时，ResBlock的设计细节<br><br><img src="/images/d2l/17/4.png"></li></ul></li><li><p>ResNet架构<br>一般来说现在的主流设计架构就是接入一个Stage（7x7Conv-3x3MP），之后再连接具体想要的网络架构，ResNet架构如下也是这种设计思想，具体架构如下<br>  <img src="/images/d2l/17/5.png"></p></li></ul><ul><li>Tricks<ul><li>实际应用中，Res34用的最多，达不到要求可以继续用Res50</li><li>Res152、Res101一般用来刷榜</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Residual</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,in_channels,out_channels,</span></span><br><span class="hljs-params"><span class="hljs-function">        use_1x1conv=<span class="hljs-literal">False</span>,stride=<span class="hljs-number">1</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1=nn.Conv2d(in_channels,out_channels,<span class="hljs-number">3</span>,stride,<span class="hljs-number">1</span>)<br>        self.conv2=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.bn1=nn.BatchNorm2d(out_channels)<br>        self.bn2=nn.BatchNorm2d(out_channels)<br>        <span class="hljs-comment"># inplace更省内存(显存)</span><br>        self.relu=nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.conv3=<span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span>(use_1x1conv):<br>            self.conv3=nn.Conv2d(in_channels,out_channels,<span class="hljs-number">1</span>,stride)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        Y=self.relu(self.bn1(self.conv1(X)))<br>        Y=self.bn2(self.conv2(Y))<br><br>        <span class="hljs-keyword">if</span>(self.conv3):<br>            X=self.conv3(X)<br>        <span class="hljs-keyword">return</span> self.relu(Y+X)<br><br>s1=nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),nn.BatchNorm2d(<span class="hljs-number">64</span>),nn.ReLU(),nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet_block</span>(<span class="hljs-params">input_channels, num_channels, num_residuals,</span></span><br><span class="hljs-params"><span class="hljs-function">                 first_block=<span class="hljs-literal">False</span></span>):</span><br>    blk = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>            blk.append(<br>                Residual(input_channels, num_channels, use_1x1conv=<span class="hljs-literal">True</span>,<br>                         stride=<span class="hljs-number">2</span>))<br>        <span class="hljs-keyword">else</span>:<br>            blk.append(Residual(num_channels, num_channels))<br>    <span class="hljs-keyword">return</span> blk<br><br>s2=nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">2</span>,<span class="hljs-literal">True</span>))<br>s3=nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>,<span class="hljs-number">2</span>))<br>s4=nn.Sequential(*resnet_block(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">2</span>))<br>s5=nn.Sequential(*resnet_block(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">2</span>))<br><br>device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>res_net=nn.Sequential(<br>    s1,s2,s3,s4,s5,<br>    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>)<br><br>x=torch.rand((<span class="hljs-number">20</span>,<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><span class="hljs-built_in">print</span>(res_net(x).shape)<br><br>opt=optim.Adam(res_net.parameters())<br>train_iter,val_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,nn.CrossEntropyLoss(),opt,</span><br><span class="hljs-comment">#     res_net,train_iter,save_name="res_net"</span><br><span class="hljs-comment">#     )</span><br><br>d2l.evaluate(res_net,val_iter,nn.CrossEntropyLoss(),<span class="hljs-string">"./params/res_net_2"</span>,device=torch.device(<span class="hljs-string">"cuda:0"</span>))<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;ResNet&quot;&gt;&lt;a href=&quot;#ResNet&quot; class=&quot;headerlink&quot; title=&quot;ResNet&quot;&gt;&lt;/a&gt;ResNet&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#Concept-and-Prin</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Batch Normalization</title>
    <link href="http://example.com/2022/09/15/d2l_16/"/>
    <id>http://example.com/2022/09/15/d2l_16/</id>
    <published>2022-09-15T01:45:08.398Z</published>
    <updated>2022-09-15T01:45:08.398Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>问题<ul><li>损失出现在最后，由BP算法和梯度消失，后面的层训练的会更快</li><li>数据在最前面，前面的层训练的慢且前面的层变化后面的层也要跟着变（抽取的底层信息变化让后面的层要重新学），所以后面的层要重新学习很多次，导致收敛变慢</li><li>考虑在学习底部层时避免变化顶部层</li></ul></li><li>批量归一化<ul><li>固定小批量里面的均值和方差，然后再做额外的调整（可学习的参数gama和beta）<br><img src="/images/d2l/16/1.png"></li><li>是线性变换</li><li>作用在<ul><li>全连接层和卷积层输出后，激活函数前</li><li>全连接层和卷积层输入前</li></ul></li><li>对于全连接层作用于特征维</li><li>对于卷积层作用于通道维（将每一个像素都当作一个样本，通道数就是一个样本的特征数）</li></ul></li><li>批量归一化在做什么？<ul><li>最初的论文是想用它来减少内部协变量转移（使每一层的输出分布变化不那么剧烈）</li><li>后续有论文指出，批量归一化可能只是在小批量中加入噪声控制模型复杂度</li></ul></li><li>总结<ul><li>批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放</li><li>批量归一化可以加速收敛（可以设置更大的学习率），一般不改变模型精度</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch<br><br>net=nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),nn.BatchNorm2d(<span class="hljs-number">6</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),nn.BatchNorm2d(<span class="hljs-number">16</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.BatchNorm1d(<span class="hljs-number">120</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.BatchNorm1d(<span class="hljs-number">84</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters(),lr=<span class="hljs-number">1.0</span>)<br><br><span class="hljs-keyword">import</span> d2l<br><br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,nn.CrossEntropyLoss(),</span><br><span class="hljs-comment">#     optim.Adam(net.parameters()),</span><br><span class="hljs-comment">#     net,train_iter,save_name="LeNet_bn",</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"))</span><br>d2l.evaluate(<br>    net,test_iter,nn.CrossEntropyLoss(),<br>    param_path=<span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/LeNet_bn_10"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Batch-Normalization&quot;&gt;&lt;a href=&quot;#Batch-Normalization&quot; class=&quot;headerlink&quot; title=&quot;Batch Normalization&quot;&gt;&lt;/a&gt;Batch Normalization&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: GoogLeNet</title>
    <link href="http://example.com/2022/09/15/d2l_15/"/>
    <id>http://example.com/2022/09/15/d2l_15/</id>
    <published>2022-09-15T01:45:05.900Z</published>
    <updated>2022-09-15T01:45:05.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>Inception块<ul><li>4个路径从不同层面抽取信息，然后再输出通道合并，最终输出高宽与输入相等，要把更多的通道数留给比较重要的通道<br><img src="/images/d2l/15/1.png"></li><li>要达到相同的输出通道数，Inception块与直接的3x3或5x5卷积相比，参数和计算复杂度更低</li></ul></li><li>GoogLeNet<ul><li>5个stage（高宽减半一次就是一个stage），9个Inception块<br><img src="/images/d2l/15/2.png"></li></ul></li><li>Inception后续具有多个变种<ul><li>Inception-BN(v2)：使用batch normalization</li><li>Inception-v3：修改了inception块</li><li>Inception-v4：使用了残差连接</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inception</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>        self.relu=nn.ReLU()<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        p1 = self.relu(self.p1_1(x))<br>        p2 = self.relu(self.p2_2(self.relu(self.p2_1(x))))<br>        p3 = self.relu(self.p3_2(self.relu(self.p3_1(x))))<br>        p4 = self.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 以通道维拼接张量</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br><br>b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>,<br>                                           padding=<span class="hljs-number">1</span>))<br><br>b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>), nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,resize=<span class="hljs-number">96</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"),</span><br><span class="hljs-comment">#     save_name="GoogLeNet"</span><br><span class="hljs-comment"># )</span><br><br>d2l.evaluate(<br>    net,test_iter,loss_f,<br>    <span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/GoogLeNet_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GoogLeNet&quot;&gt;&lt;a href=&quot;#GoogLeNet&quot; class=&quot;headerlink&quot; title=&quot;GoogLeNet&quot;&gt;&lt;/a&gt;GoogLeNet&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#Conc</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: NiN</title>
    <link href="http://example.com/2022/09/15/d2l_14/"/>
    <id>http://example.com/2022/09/15/d2l_14/</id>
    <published>2022-09-15T01:45:03.519Z</published>
    <updated>2022-09-15T01:45:03.519Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>全连接层的问题：<ul><li>全连接层参数比卷积层的参数多很多，导致很多的内存（显存）及计算带宽占用</li><li>全连接层容易带来过拟合</li></ul></li><li>NiN思想：完全不要全连接层</li><li>NiN块：<ul><li>一个卷积层后跟两个起到全连接层的作用的卷积层</li><li>起到全连接层的作用的卷积层为1x1步幅为1无填充的卷积层</li></ul></li><li>NiN架构<ul><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层</li><li>最后使用全局平均池化层得到输出（通道数是类别数）<br><img src="/images/d2l/14/1.png"></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NiNBlock</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,in_channels,out_channels,</span></span><br><span class="hljs-params"><span class="hljs-function">        kernel_size,stride,padding</span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv=nn.Conv2d(<br>            in_channels,out_channels,kernel_size,<br>            stride=stride,padding=padding<br>        )<br>        self.f1=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">1</span>)<br>        self.f2=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">1</span>)<br>        self.relu=nn.ReLU()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        x=self.relu(self.conv(x))<br>        x=self.relu(self.f1(x))<br>        x=self.relu(self.f2(x))<br>        <span class="hljs-keyword">return</span> x<br><br>nin_net=nn.Sequential(<br>    NiNBlock(<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,<span class="hljs-number">11</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),<br>    NiNBlock(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),<br>    NiNBlock(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),nn.Dropout(),<br>    NiNBlock(<span class="hljs-number">384</span>,<span class="hljs-number">10</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>    <span class="hljs-comment"># 目标输出size为1x1，也就是全局池化</span><br>    nn.AdaptiveAvgPool2d(<span class="hljs-number">1</span>),<br>    nn.Flatten()<br>)<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(nin_net.parameters())<br><br><span class="hljs-keyword">import</span> d2l<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,resize=<span class="hljs-number">224</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,loss_f,opt,nin_net,train_iter,</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"),</span><br><span class="hljs-comment">#     save_name="NIN"</span><br><span class="hljs-comment"># )</span><br>d2l.evaluate(<br>    nin_net,test_iter,loss_f,<br>    <span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/NIN_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NiN&quot;&gt;&lt;a href=&quot;#NiN&quot; class=&quot;headerlink&quot; title=&quot;NiN&quot;&gt;&lt;/a&gt;NiN&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#Concept-and-Principle&quot; class</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: VGGNet</title>
    <link href="http://example.com/2022/09/15/d2l_13/"/>
    <id>http://example.com/2022/09/15/d2l_13/</id>
    <published>2022-09-15T01:45:00.927Z</published>
    <updated>2022-09-15T01:45:00.927Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>AlexNet的设计很随意，如何变大变深无规律性，VGG探讨了如何对CNN进行扩展</li><li>如何更深更大？<ul><li>更多全连接层（太贵）</li><li>更多的卷积层</li><li>将卷积层组合成块（VGG）</li></ul></li><li>VGG块<ul><li>使用小卷积核深网络比大小卷积核浅网络效果好</li><li>3x3卷积层（n层、m通道）</li><li>2x2最大池化层<br><img src="/images/d2l/13/2.png"></li></ul></li><li>VGG架构<ul><li>多个VGG块后接全连接层</li><li>不同次数的重复块得到不同架构（VGG-16、VGG-19等）<br><img src="/images/d2l/13/1.png"></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> dropout, nn,optim<br><span class="hljs-keyword">import</span> d2l<br><span class="hljs-comment"># 返回VGG块</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_block</span>(<span class="hljs-params">num_convs,in_channels,out_channels</span>):</span><br>    layers=[]<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(<br>            nn.Conv2d(in_channels,out_channels,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>            )<br>        layers.append(nn.ReLU())<br>        in_channels=out_channels<br><br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>    <br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_architecture</span>(<span class="hljs-params">num_blocks,in_channels</span>):</span><br>    blocks=[]<br>    out_channels=<span class="hljs-number">16</span><br>    blocks.append(vgg_block(<span class="hljs-number">1</span>,in_channels,out_channels))<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks-<span class="hljs-number">1</span>):<br>        in_channels=out_channels<br>        out_channels*=<span class="hljs-number">2</span><br>        blocks.append(vgg_block(<span class="hljs-number">1</span>,in_channels,out_channels))<br>    <br>    <span class="hljs-keyword">return</span> nn.Sequential(*blocks)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_5</span>(<span class="hljs-params">in_channels</span>):</span><br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        vgg_architecture(<span class="hljs-number">5</span>,in_channels),<br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">1024</span>),<br>        nn.Dropout(),<br>        nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">512</span>),<br>        nn.Dropout(),<br>        nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>    )<br><br>vgg=vgg_5(<span class="hljs-number">1</span>).to(torch.device(<span class="hljs-string">"cuda:0"</span>))<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">64</span>,<span class="hljs-number">224</span>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(vgg.parameters())<br><br>d2l.train(<span class="hljs-number">10</span>,loss_f,opt,vgg,train_iter,save_name=<span class="hljs-string">"vgg_5"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;VGGNet&quot;&gt;&lt;a href=&quot;#VGGNet&quot; class=&quot;headerlink&quot; title=&quot;VGGNet&quot;&gt;&lt;/a&gt;VGGNet&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#Concept-and-Prin</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: AlexNet</title>
    <link href="http://example.com/2022/09/15/d2l_12/"/>
    <id>http://example.com/2022/09/15/d2l_12/</id>
    <published>2022-09-15T01:44:57.424Z</published>
    <updated>2022-09-15T01:44:57.424Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>在深度学习之前：<ul><li>核方法<ul><li>特征提取</li><li>选择核函数</li><li>凸优化问题</li><li>漂亮的定理</li></ul></li><li>几何学<ul><li>抽取特征</li><li>将计算机视觉问题描述为几何问题（如多相机）</li><li>凸优化</li><li>漂亮的定理</li><li>建立假设模型，若假设满足，效果会很好</li></ul></li><li>特征工程<ul><li>特征工程（人工特征提取）是关键，不太关心机器学习模型</li><li>特征描述子（SIFT、SURF）</li><li>视觉词袋（聚类）</li><li>最后一般用SVM</li></ul></li></ul></li><li>AlexNet赢得了2012年ImageNet竞赛的冠军，引起了深度学习的热潮，其本质上是一个更深更大的LeNet<ul><li>主要改进：<ul><li>丢弃法</li><li>ReLU</li><li>MaxPooling</li><li>数据增强（截取、调亮度、调色温等）</li><li>更深更大</li></ul></li></ul></li><li>网络结构与复杂度<br><img src="/images/d2l/12/1.png"></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AlexNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1=nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,kernel_size=<span class="hljs-number">11</span>,stride=<span class="hljs-number">4</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv2=nn.Conv2d(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>)<br>        self.conv3=nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv4=nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">384</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv5=nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">256</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.liner1=nn.Linear(<span class="hljs-number">6400</span>,<span class="hljs-number">4096</span>)<br>        self.liner2=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">4096</span>)<br>        self.liner3=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)<br><br>        self.pool=nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>)<br>        self.flatten=nn.Flatten()<br>        self.relu=nn.ReLU()<br>        self.dropout=nn.Dropout(p=<span class="hljs-number">0.5</span>)<br>    <br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        <br>        x=self.relu(self.pool(self.conv1(x)))<br>        <br>        x=self.relu(self.pool(self.conv2(x)))<br>        x=self.relu(self.conv3(x))<br>        x=self.relu(self.conv4(x))<br>        x=self.relu(self.conv5(x))<br>        x=self.pool(x)<br>        x=self.flatten(x)<br>        x=self.dropout((self.liner1(x)))<br>        x=self.dropout((self.liner2(x)))<br>        x=self.liner3(x)<br><br>        <span class="hljs-keyword">return</span> x<br><br>net=AlexNet().to(torch.device(<span class="hljs-string">"cuda:0"</span>))<br><br><span class="hljs-comment"># x=torch.rand((1,1,224,224)).cuda()</span><br><span class="hljs-comment"># print(net(x).size())</span><br><br><span class="hljs-comment"># 读取Fashion-MNIST，将图片直接拉伸为224x224</span><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>,<span class="hljs-number">224</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     25,nn.CrossEntropyLoss(),</span><br><span class="hljs-comment">#     optim.Adam(net.parameters()),</span><br><span class="hljs-comment">#     net,train_iter,save_name="AlexNet",</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"))</span><br>d2l.evaluate(<br>    net,test_iter,nn.CrossEntropyLoss(),<br>    param_path=<span class="hljs-string">"D:\code\machine_learning\limu_d2l\params\AlexNet_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AlexNet&quot;&gt;&lt;a href=&quot;#AlexNet&quot; class=&quot;headerlink&quot; title=&quot;AlexNet&quot;&gt;&lt;/a&gt;AlexNet&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#Concept-and-</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: LeNet</title>
    <link href="http://example.com/2022/09/15/d2l_11/"/>
    <id>http://example.com/2022/09/15/d2l_11/</id>
    <published>2022-09-15T01:44:54.708Z</published>
    <updated>2022-09-15T01:44:54.708Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>最早是用于手写数字识别，识别信件上的邮政编码</li><li>网络结构<br><img src="/images/d2l/11/1.png" alt="$cover"></li><li>提出了一个数据集：MNIST<ul><li>5w个训练数据</li><li>1w个测试数据</li><li>图像大小 28x28</li><li>10类</li></ul></li><li>总结<ul><li>LeNet是早期成功的神经网络</li><li>先使用卷积层学习图片空间信息</li><li>然后使用全连接层转换到类别空间</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-comment"># 定义Reshape层</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Reshape</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br><br>net=nn.Sequential(<br>    Reshape(),<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br><br>x=torch.rand((<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>),dtype=torch.float32)<br><br><span class="hljs-comment"># 观察每一层输出的tensor尺寸</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    x=layer(x)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">':\t'</span>,x.size())<br><br><span class="hljs-comment"># 在Fashion-MINST上的表现</span><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     25,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     param_name="LeNet",device=torch.device("cuda:0")</span><br><span class="hljs-comment">#     )</span><br>d2l.evaluate(net,test_iter,loss_f,<span class="hljs-string">".\params\LeNet_25"</span>)   <br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LeNet&quot;&gt;&lt;a href=&quot;#LeNet&quot; class=&quot;headerlink&quot; title=&quot;LeNet&quot;&gt;&lt;/a&gt;LeNet&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#Concept-and-Principl</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Convolution and Pooling</title>
    <link href="http://example.com/2022/09/15/d2l_10/"/>
    <id>http://example.com/2022/09/15/d2l_10/</id>
    <published>2022-09-15T01:44:51.987Z</published>
    <updated>2022-09-15T01:44:51.987Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Convolution-and-Pooling"><a href="#Convolution-and-Pooling" class="headerlink" title="Convolution and Pooling"></a>Convolution and Pooling</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li><p>卷积</p><ul><li>平移不变性和局部性是在图片中寻找某种模式的原则。<ul><li>因为模式不会随着其在图片中位置改变而改变，所以一个识别器（卷积核）被设计为具有平移不变性（即参数只与输入的像素值有关，而与像素在图片的位置无关），去学习图片中的一种模式</li><li>模式与其相邻的局部相关，识别器每次仅去看图片的一部分</li><li>对全连接层使用平移不变性和局部性得到卷积层</li></ul></li></ul></li><li><p>卷积层<br><img src="/images/d2l/10/1.png"></p><ul><li>不同卷积核（值）会对图片带来不同的效果，当某种效果对任务有帮助时，网络很有可能就会学习出这种卷积核<br><img src="/images/d2l/10/2.png"></li></ul></li><li><p>填充与步幅</p><ul><li>在输入的四周加入额外的行和列以控制卷积后的输出图像大小，卷积核大小一般选奇数，能上下对称地填充图片来保证输入输出图片大小不变<br><img src="/images/d2l/10/3.png"></li><li>增大卷积步幅，快速缩小图片</li><li>总结<br><img src="/images/d2l/10/4.png"></li></ul></li><li><p>通道</p><ul><li>每个通道有自己的卷积核，输入通道不同通道的对应卷积后直接相加后再加偏置项，最后输出一个单通道</li><li>多输出通道就是多个上述操作输出的多个单通道</li><li>每个输出通道可以识别特定的模式，输入通道识别并组合（加权相加）输入中的模式</li></ul></li><li><p>池化</p><ul><li>池化层缓解卷积对位置的敏感性</li><li>池化层不学习任何参数，有最大池化、平均池化等</li><li>池化层也可以调整填充与步幅</li><li>经过池化层不会改变通道数</li></ul></li><li><p>tricks</p><ul><li>填充：一般填充就是为了使图小大小不变</li><li>步幅：一般设置为1，计算量太大时增大步幅</li><li>最终图像大小：一般为3x3、5x5、7x7</li><li>1x1卷积层：不识别空间模式，而是用来改变通道数</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Convolution-and-Pooling&quot;&gt;&lt;a href=&quot;#Convolution-and-Pooling&quot; class=&quot;headerlink&quot; title=&quot;Convolution and Pooling&quot;&gt;&lt;/a&gt;Convolution and P</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Sequential Model</title>
    <link href="http://example.com/2022/09/15/d2l_9/"/>
    <id>http://example.com/2022/09/15/d2l_9/</id>
    <published>2022-09-15T01:44:49.025Z</published>
    <updated>2022-09-15T01:44:49.025Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sequential-Model"><a href="#Sequential-Model" class="headerlink" title="Sequential Model"></a>Sequential Model</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><p>序列模型是考虑时间信息的模型</p><ul><li><p>序列数据</p><ul><li>数据带有时序结构，如电影的评价随时间变化<ul><li>电影拿奖后评分上升</li><li>导演、演员负面报道后评分下降</li></ul></li></ul></li><li><p>统计工具  </p><ul><li>将序列中每个元素看作随机变量，显然他们不是独立的<br><img src="/images/d2l/9/1.png"></li><li>在实际操作中，时序序列一般只能正向建模去预测<br><img src="/images/d2l/9/2.png"></li><li>要使用序列模型预测T时刻x的概率，核心是求T时刻的条件概率（似然），这里的f可看作神经网络，神经网络将训练集建模。自回归指的是用数据对见过的数据建模（因为最后预测也是在预测相同的数据），与非序列模型用数据对独立于数据的标签建模不同。<br><img src="/images/d2l/9/3.png"></li><li>具体如何建模？<ul><li>马尔科夫假设<br>当前预测的数据只跟过去的tau个数据相关，tau是一个固定常数。假设x是标量数据，此时只需要将其看作回归问题，使用MLP把tau个x当作特征训练得到t时刻标量x。MLP进行梯度优化的过程便是最大化似然概率的过程<br><img src="/images/d2l/9/4.png"></li><li>潜变量模型<br>引入一个可不断更新的潜变量用于概括历史信息，使得建模更加简单（RNN）<br><img src="/images/d2l/9/5.png"></li></ul></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>马尔可夫假设+MLP<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 使用正弦函数加上噪声生成序列数据</span><br>T=<span class="hljs-number">1000</span><br><br>time=torch.arange(<span class="hljs-number">1</span>,T+<span class="hljs-number">1</span>,dtype=torch.float32)<br>data=torch.sin(<span class="hljs-number">0.01</span>*time)+torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.1</span>,time.shape)<br><br><span class="hljs-comment"># 将数据映射为数据对</span><br>tau=<span class="hljs-number">4</span><br>labels=data[tau:].view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>features=torch.zeros(T-tau,tau)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau):<br>    features[:,i]=data[i:T-tau+i]<br><br>train_dataset=TensorDataset(features[:<span class="hljs-number">600</span>],labels[:<span class="hljs-number">600</span>])<br>test_dataset=TensorDataset(features[<span class="hljs-number">600</span>:],labels[<span class="hljs-number">600</span>:])<br><br>train_iter=DataLoader(train_dataset,batch_size=<span class="hljs-number">16</span>)<br>test_iter=DataLoader(test_dataset,batch_size=<span class="hljs-number">16</span>)<br><br>net=nn.Sequential(<br>    nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>),<br>    nn.Dropout(<span class="hljs-number">0.1</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">16</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">16</span>,<span class="hljs-number">1</span>)<br>    )<br>loss_f=nn.MSELoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-keyword">try</span>:<br>    net.load_state_dict(torch.load(<span class="hljs-string">"9.params"</span>))<br><br><span class="hljs-keyword">except</span>:<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>        train_loss=[]<br>        test_loss=[]<br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            out=net(X)<br>            l=loss_f(out,y)<br><br>            l.backward()<br>            train_loss.append(l.item())<br>            opt.step()<br>            opt.zero_grad()<br><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> test_iter:<br>                out=net(X)<br>                l=loss_f(out,y)<br>                test_loss.append(l.item())<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>,<span class="hljs-subst">{<span class="hljs-built_in">sum</span>(train_loss)}</span>  <span class="hljs-subst">{<span class="hljs-built_in">sum</span>(test_loss)}</span>"</span>)<br>    torch.save(net.state_dict(),<span class="hljs-string">"9.params"</span>)<br><br><span class="hljs-comment"># 使用测试集预测</span><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>t=<span class="hljs-number">600</span><br>steps=<span class="hljs-number">396</span><br><br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],net(features[<span class="hljs-number">600</span>:]).view(-<span class="hljs-number">1</span>).detach().numpy())<br><br><span class="hljs-comment"># 使用预测值进行多步预测</span><br><br>win=[math.sin(i*<span class="hljs-number">0.01</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t-<span class="hljs-number">4</span>,t)]<br>true=[]<br>pred=[]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):<br>    X=torch.tensor(win,dtype=torch.float32)<br>    out=net(X)<br>    truth=math.sin((t+i)*<span class="hljs-number">0.01</span>)<br>    true.append(truth);pred.append(out.item())<br>    <span class="hljs-comment"># print(out.item(),t)</span><br>    win.pop(<span class="hljs-number">0</span>)<br>    win.append(out)<br><br><br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],pred)<br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],true)<br>plt.show()<br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Sequential-Model&quot;&gt;&lt;a href=&quot;#Sequential-Model&quot; class=&quot;headerlink&quot; title=&quot;Sequential Model&quot;&gt;&lt;/a&gt;Sequential Model&lt;/h1&gt;&lt;h2 id=&quot;Concept-a</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Numerical Stability &amp; Initialization</title>
    <link href="http://example.com/2022/09/15/d2l_7/"/>
    <id>http://example.com/2022/09/15/d2l_7/</id>
    <published>2022-09-15T01:44:40.080Z</published>
    <updated>2022-09-15T01:44:40.080Z</updated>
    
    <content type="html"><![CDATA[<h1 id="D2L-Numerical-Stability-amp-Initialization"><a href="#D2L-Numerical-Stability-amp-Initialization" class="headerlink" title="D2L: Numerical Stability &amp; Initialization"></a>D2L: Numerical Stability &amp; Initialization</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>数值的稳定性<ul><li>神经网络的梯度<br>求某一层的参数的梯度，直接就对损失函数关于该层参数求导,然后通过链式法则，化成d-t次的矩阵乘法<br><img src="/images/d2l/7/chain_d.png"></li><li>梯度爆炸与梯度消失<br>上述连续的乘法运算会带来两个问题：梯度爆炸与梯度消失<br><img src="/images/d2l/7/e_v.png"><br>梯度爆炸带来的问题：梯度值超过计算机可表示大小、对学习率敏感<br>梯度消失带来的问题：梯度值变为0（超出计算可表示精度的小浮点数）、无论如何选择学习率训练都没有进展、神经网络无法做到更深</li></ul></li><li>模型初始化<ul><li>如何让训练更加稳定？（不产生梯度消失和梯度爆炸）<br>要让梯度值保持在合理的范围内，一般有如下方法：<ul><li>将乘法变为加法（ResNet、LSTM）</li><li>归一化（梯度归一化、梯度裁剪）</li><li>选定合适的激活函数</li><li>合理的初始参数</li></ul></li><li>让每层的方差是一个常数<br><img src="/images/d2l/7/bn.png"></li><li>合理的权重初始化<ul><li>需要在一个合理值区间里随机初始参数<ul><li>远离最优解的地方损失函数很复杂（梯度很大）</li><li>最优解附近比较平缓 </li></ul></li></ul></li><li>Xavier初始化<br>使得输入空间的方差和输出空间的方差尽量相等</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;D2L-Numerical-Stability-amp-Initialization&quot;&gt;&lt;a href=&quot;#D2L-Numerical-Stability-amp-Initialization&quot; class=&quot;headerlink&quot; title=&quot;D2L: Num</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Weight Decay &amp; Dropout</title>
    <link href="http://example.com/2022/09/15/d2l_6/"/>
    <id>http://example.com/2022/09/15/d2l_6/</id>
    <published>2022-09-15T01:44:35.846Z</published>
    <updated>2022-09-15T01:44:35.846Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Weight-Decay-amp-Dropout"><a href="#Weight-Decay-amp-Dropout" class="headerlink" title="Weight Decay &amp; Dropout"></a>Weight Decay &amp; Dropout</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>权重衰退<br>权重衰退可以控制模型复杂度，使其复杂度不会太大，从而一定程度上避免过拟合<ul><li>使用均方范数作为硬性限制<br>通过限制参数的取值范围来控制模型容量，具体有如下例子<br>在限制参数向量范数的情况下优化损失函数，一般不会使用这种正则方式<br><img src="/images/d2l/6/hard_res.png"></li><li>使用均方范数作为柔性限制<br>上述硬性限制有一个等价方案，具体如下，这就是一般的正则化方法，其作用同样也是使得参数被限制在一个较小的范围<br><img src="/images/d2l/6/soft_res.png"><br>下面是正则项对最优解影响的一个演示<br>坐标轴分别是w的各个分量，圆线是等高线。<br>正则项给了另外一个梯度，把原始的损失函数算出的最优解往原点拉，必然会导致W的取值范围变小从而使模型复杂度降低，也就减小了过拟合。另外一种理解，正则项加入后优化目标就不再全局最优点了，所以肯定会减小训练集拟合程度，也就减小了过拟合<br>考虑为什么λ控制了正则项的重要程度，因为求偏导时λ会变成梯度前的常数项<br><img src="/images/d2l/6/effect.png"></li><li>参数更新<br>带正则项后参数更新过程如下，这也说明了为什么这种方法叫权重衰退：更新前先把权重减小，然后继续更新梯度<br><img src="/images/d2l/6/update.png"></li><li>注意<br>权重衰减也只在训练过程中使用，用来限制训练过程中的参数，在最终的验证过程中，指标还是原先的损失函数</li></ul></li><li>丢弃法<ul><li>动机<br>一个好的模型需要对输入数据加入扰动鲁棒，使用有噪音的数据等价于Tikhonov正则。丢弃法就是在层之间加入噪音，丢弃法也可以看作一个正则</li><li>无偏差地加入噪音<br>丢弃法就是加入了无偏噪音<br><img src="/images/d2l/6/noise.png"></li><li>使用丢弃法  <ul><li>对其发作用域隐藏层地输出上，即随机将某些神经元的输出置零且其它输出按上述公式增大</li><li>丢弃法只在训练过程中使用，测试和验证过程中不使用，这样保证了确定的输出</li><li>丢弃法常用于全连接层</li></ul></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>权重衰减从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 训练数据设置比较小，容易过拟合</span><br>num_train=<span class="hljs-number">20</span><br>num_test=<span class="hljs-number">100</span><br>num_inputs=<span class="hljs-number">200</span><br>batch_size=<span class="hljs-number">5</span><br><br>true_w,true_b=torch.ones(num_inputs)*<span class="hljs-number">0.01</span>,<span class="hljs-number">0.05</span><br><br>train_data=synthetic_data(true_w,true_b,num_train)<br>train_iter=data_loader(train_data,batch_size)<br>test_data=synthetic_data(true_w,true_b,num_test)<br>test_iter=data_loader(test_data,batch_size)<br><br><span class="hljs-comment"># for X,y in train_iter:</span><br><span class="hljs-comment">#     print(X,y)</span><br>w=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=true_w.shape,requires_grad=<span class="hljs-literal">True</span>)<br>b=torch.zeros(<span class="hljs-number">1</span>,requires_grad=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># L2正则</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L2_penalty</span>(<span class="hljs-params">lambada,w</span>):</span><br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(w**<span class="hljs-number">2</span>)/<span class="hljs-number">2</span>*lambada<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">liner_reg</span>(<span class="hljs-params">w,b,X</span>):</span><br>    <span class="hljs-keyword">return</span> torch.matmul(X,w)+b<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net</span>(<span class="hljs-params">X</span>):</span><br>    <span class="hljs-keyword">return</span> liner_reg(w,b,X)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">squared_loss_L2</span>(<span class="hljs-params">y_hat,y</span>):</span><br>    loss=(y_hat.view(y.shape)-y)**<span class="hljs-number">2</span>/<span class="hljs-number">2</span>/<span class="hljs-built_in">len</span>(y)<br>    <span class="hljs-keyword">return</span> (loss+L2_penalty(<span class="hljs-number">0.5</span>,w)).<span class="hljs-built_in">sum</span>()<br><br>opt=optim.SGD([w,b],lr=<span class="hljs-number">0.01</span>)<br><br><br><br>train(<span class="hljs-number">100</span>,squared_loss_L2,opt,net,train_iter,test_iter)<br></code></pre></td></tr></tbody></table></figure><ul><li>权重衰减简洁实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">net=nn.Sequential(nn.Linear(num_inputs,<span class="hljs-number">1</span>))<br>loss_f=nn.MSELoss()<br><span class="hljs-comment"># weight_decay代表了L2范数前面的λ系数</span><br><span class="hljs-comment"># 权重衰减系数很小时，在当前数据集下过拟合非常明显</span><br><span class="hljs-comment"># 权重衰减系数很大时，欠拟合则会非常明显</span><br>opt=optim.SGD(net.parameters(),lr=<span class="hljs-number">0.01</span>,weight_decay=<span class="hljs-number">1.2</span>)<br><br>train(<span class="hljs-number">100</span>,loss_f,opt,net,train_iter,test_iter)<br><span class="hljs-comment"># 其它范数的正则pytorch没有直接的实现</span><br><span class="hljs-comment"># 手动实现也很简单</span><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Weight-Decay-amp-Dropout&quot;&gt;&lt;a href=&quot;#Weight-Decay-amp-Dropout&quot; class=&quot;headerlink&quot; title=&quot;Weight Decay &amp;amp; Dropout&quot;&gt;&lt;/a&gt;Weight Decay</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Modle Selection</title>
    <link href="http://example.com/2022/09/15/d2l_5/"/>
    <id>http://example.com/2022/09/15/d2l_5/</id>
    <published>2022-09-15T01:44:33.421Z</published>
    <updated>2022-09-15T01:44:33.421Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Modle-Selection"><a href="#Modle-Selection" class="headerlink" title="Modle Selection"></a>Modle Selection</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>训练误差和泛化误差  <ul><li>训练误差：模型在训练数据上的误差（不太关心）</li><li>泛化误差：模型在新数据上的误差（很关心）</li></ul></li><li>验证数据集和测试数据集<ul><li>验证数据集<br>用于在训练过程中评估模型好坏的数据集，一般从训练集中划分出一部分，验证数据集不能作为训练集让模型训练，用来动态调整模型超参数</li><li>测试数据集<br>模型最终训练完毕后，使用测试集测试模型泛化能力，<strong>不能使用测试集来调整模型超参数</strong>，大多数情况下不会被打上标签</li></ul></li><li>K-折交叉验证<br>通常情况下，我们都没有足够富裕的数据去从训练集中划分验证集，这是使用K-折交叉验证能较简单的解决问题<ul><li>思想：一般情况将K折交叉验证用于模型调优，找到使得模型泛化性能最优的超参值。找到后，在全部训练集上重新训练模型，并使用独立验证集对模型性能做出最终评价。</li><li>算法：K折就将训练集分为K块，训练代价为原来的K倍<ol><li>将原始数据集划分为相等的K部分（“折”）</li><li>将第i部分作为验证集，其余作为训练集</li><li>训练模型，计算模型在验证集上的准确率</li><li>每次用不同的部分i作为验证集，重复步骤2和3 K次</li><li>将平均准确率作为使用当前超参时的模型准确率</li><li>找到一个较好的超参数后，再用全部训练集训练模型，并在一个全新的验证集上验证，不用调超参数，达到一个较好的验证准确率时，直接去测试</li></ol></li></ul></li><li>过拟合和欠拟合<br><img src="/images/d2l/5/fitting.png"><ul><li>模型容量的影响<br>数据集复杂程度应该与模型复杂程度正相关，否则就会出现过拟合与欠拟合。举例来说，当模型很复杂而数据很简单时，模型可以直接就把所有数据记住而丧失泛化能力；而模型过于简单时，如感知机模型，无法正确划分异或的数据<br>模型足够复杂时，有其他手段减少过拟合；模型太简单没前途<br><img src="/images/d2l/5/capacity.png"></li></ul></li><li>估计模型容量<br>模型种类确定时（如神经网络），模型容量由两个因素估计：参数个数、参数取值范围 </li><li>数据复杂度<br>有多个重要因素：<ul><li>样本个数</li><li>特征个数</li><li>时间、空间结构</li><li>多样性</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Modle-Selection&quot;&gt;&lt;a href=&quot;#Modle-Selection&quot; class=&quot;headerlink&quot; title=&quot;Modle Selection&quot;&gt;&lt;/a&gt;Modle Selection&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-P</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Perceptron</title>
    <link href="http://example.com/2022/09/15/d2l_4/"/>
    <id>http://example.com/2022/09/15/d2l_4/</id>
    <published>2022-09-15T01:44:30.588Z</published>
    <updated>2022-09-15T01:44:30.588Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h1><h2 id="Concept-and-Principle"><a href="#Concept-and-Principle" class="headerlink" title="Concept and Principle"></a><strong>Concept and Principle</strong></h2><ul><li>感知机  <ul><li>模型<br>感知机只比线性分类多了一个激活函数，激活函数为单层感知机带来了分类能力，为多层感知机带来了非线性因素<br><img src="/images/d2l/4/perceptron.png"></li><li>训练<br>训练感知机等价于批量大小为1的梯度下降，按顺序逐个取样本，与随机梯度下降不同<br><img src="/images/d2l/4/train_p.png"></li><li>单层感知机无法解决异或问题，他只能产生线性分割面，这导致了第一次AI寒冬</li></ul></li><li>多层感知机<ul><li>多层感知机由多个感知机组成，分为输入层、隐藏层、输出层，层内不连接，层间全连接<br><img src="/images/d2l/4/mlp.png"></li><li>每个感知机输出后要经过一个非线性的激活函数，否则多层感知机等价于单层感知机</li><li>常用激活函数：Sigmoiod、Tanh、ReLU，性能都没太大区别，ReLU计算更容易，如果没有特别的想法，用ReLU就行</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><br>train_iter,_=LoadData(<span class="hljs-number">256</span>)<br>num_in,num_out,num_h=<span class="hljs-number">784</span>,<span class="hljs-number">10</span>,<span class="hljs-number">256</span><br><span class="hljs-comment"># 隐层参数</span><br>w1=torch.randn(num_in,num_h,requires_grad=<span class="hljs-literal">True</span>)<br>b1=torch.zeros(num_h,requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 输出层参数</span><br>w2=torch.randn(num_h,num_out,requires_grad=<span class="hljs-literal">True</span>)<br>b2=torch.zeros(num_out,requires_grad=<span class="hljs-literal">True</span>)<br>params=[w1,b2,w1,b2]<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.SGD(params,lr=<span class="hljs-number">0.001</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReLU</span>(<span class="hljs-params">X</span>):</span><br>    a=torch.zeros_like(X)<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">max</span>(X,a)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Net</span>(<span class="hljs-params">X:Tensor</span>):</span><br>    X=X.view(-<span class="hljs-number">1</span>,num_in)<br>    <span class="hljs-comment"># @被重载为了矩阵乘法</span><br>    X=ReLU(X@w1+b1)<br>    <span class="hljs-keyword">return</span> X@w2+b2<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Train</span>():</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>        loss=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            X=X.view(-<span class="hljs-number">1</span>,<span class="hljs-number">784</span>)<br>            out=Net(X)<br>            l=loss_f(out,y)<br><br>            l.backward()<br>            opt.step()<br>            opt.zero_grad()<br>            loss=l.item()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch}</span>,<span class="hljs-subst">{loss}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Perceptron&quot;&gt;&lt;a href=&quot;#Perceptron&quot; class=&quot;headerlink&quot; title=&quot;Perceptron&quot;&gt;&lt;/a&gt;Perceptron&lt;/h1&gt;&lt;h2 id=&quot;Concept-and-Principle&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
  </entry>
  
</feed>
