<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xavier&#39;s blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-11-24T09:15:12.949Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Xavier</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>D2L: Modle Selection</title>
    <link href="http://example.com/2021/11/24/d2l_5/"/>
    <id>http://example.com/2021/11/24/d2l_5/</id>
    <published>2021-11-24T09:15:12.949Z</published>
    <updated>2021-11-24T09:15:12.949Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/d2l/1/cover.png" alt="$cover"></p><h1 id="Modle-Selection"><a href="#Modle-Selection" class="headerlink" title="Modle Selection"></a>Modle Selection</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>训练误差和泛化误差  <ul><li>训练误差：模型在训练数据上的误差（不太关心）</li><li>泛化误差：模型在新数据上的误差（很关心）</li></ul></li><li>验证数据集和测试数据集<ul><li>验证数据集<br>用于在训练过程中评估模型好坏的数据集，一般从训练集中划分出一部分，验证数据集不能作为训练集让模型训练，用来动态调整模型超参数</li><li>测试数据集<br>模型最终训练完毕后，使用测试集测试模型泛化能力，<strong>不能使用测试集来调整模型超参数</strong>，大多数情况下不会被打上标签</li></ul></li><li>K-折交叉验证<br>通常情况下，我们都没有足够富裕的数据去从训练集中划分验证集，这是使用K-折交叉验证能较简单的解决问题<ul><li>思想：一般情况将K折交叉验证用于模型调优，找到使得模型泛化性能最优的超参值。找到后，在全部训练集上重新训练模型，并使用独立验证集对模型性能做出最终评价。</li><li>算法：K折就将训练集分为K块，训练代价为原来的K倍<ol><li>将原始数据集划分为相等的K部分（“折”）</li><li>将第i部分作为验证集，其余作为训练集</li><li>训练模型，计算模型在验证集上的准确率</li><li>每次用不同的部分i作为验证集，重复步骤2和3 K次</li><li>将平均准确率作为使用当前超参时的模型准确率</li><li>找到一个较好的超参数后，再用全部训练集训练模型，并在一个全新的验证集上验证，不用调超参数，达到一个较好的验证准确率时，直接去测试</li></ol></li></ul></li><li>过拟合和欠拟合<br><img src="/images/d2l/5/fitting.png"><ul><li>模型容量的影响<br>数据集复杂程度应该与模型复杂程度正相关，否则就会出现过拟合与欠拟合。举例来说，当模型很复杂而数据很简单时，模型可以直接就把所有数据记住而丧失泛化能力；而模型过于简单时，如感知机模型，无法正确划分异或的数据<br>模型足够复杂时，有其他手段减少过拟合；模型太简单没前途<br><img src="/images/d2l/5/capacity.png"></li></ul></li><li>估计模型容量<br>模型种类确定时（如神经网络），模型容量由两个因素估计：参数个数、参数取值范围 </li><li>数据复杂度<br>有多个重要因素：<ul><li>样本个数</li><li>特征个数</li><li>时间、空间结构</li><li>多样性</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/d2l/1/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Modle-Selection&quot;&gt;&lt;a href=&quot;#Modle-Selection&quot; class=&quot;headerlink&quot; title=&quot;Modle </summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Perceptron</title>
    <link href="http://example.com/2021/11/24/d2l_4/"/>
    <id>http://example.com/2021/11/24/d2l_4/</id>
    <published>2021-11-24T08:09:16.313Z</published>
    <updated>2021-11-24T08:09:16.313Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/d2l/1/cover.png" alt="$cover"></p><h1 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>感知机  <ul><li>模型<br>感知机只比线性分类多了一个激活函数，激活函数为单层感知机带来了分类能力，为多层感知机带来了非线性因素<br><img src="/images/d2l/4/perceptron.png"></li><li>训练<br>训练感知机等价于批量大小为1的梯度下降，按顺序逐个取样本，与随机梯度下降不同<br><img src="/images/d2l/4/train_p.png"></li><li>单层感知机无法解决异或问题，他只能产生线性分割面，这导致了第一次AI寒冬</li></ul></li><li>多层感知机<ul><li>多层感知机由多个感知机组成，分为输入层、隐藏层、输出层，层内不连接，层间全连接<br><img src="/images/d2l/4/mlp.png"></li><li>每个感知机输出后要经过一个非线性的激活函数，否则多层感知机等价于单层感知机</li><li>常用激活函数：Sigmoiod、Tanh、ReLU，性能都没太大区别，ReLU计算更容易，如果没有特别的想法，用ReLU就行</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>从零实现</li></ul><pre><code class="python">from torch import nn,optimtrain_iter,_=LoadData(256)num_in,num_out,num_h=784,10,256# 隐层参数w1=torch.randn(num_in,num_h,requires_grad=True)b1=torch.zeros(num_h,requires_grad=True)# 输出层参数w2=torch.randn(num_h,num_out,requires_grad=True)b2=torch.zeros(num_out,requires_grad=True)params=[w1,b2,w1,b2]loss_f=nn.CrossEntropyLoss()opt=optim.SGD(params,lr=0.001)def ReLU(X):    a=torch.zeros_like(X)    return torch.max(X,a)def Net(X:Tensor):    X=X.view(-1,num_in)    # @被重载为了矩阵乘法    X=ReLU(X@w1+b1)    return X@w2+b2def Train():    for epoch in range(50):        loss=0        for X,y in train_iter:            X=X.view(-1,784)            out=Net(X)            l=loss_f(out,y)            l.backward()            opt.step()            opt.zero_grad()            loss=l.item()        print(f"{epoch},{loss}")</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/d2l/1/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Perceptron&quot;&gt;&lt;a href=&quot;#Perceptron&quot; class=&quot;headerlink&quot; title=&quot;Perceptron&quot;&gt;&lt;/a&gt;</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Softmax Regression</title>
    <link href="http://example.com/2021/11/23/d2l_3/"/>
    <id>http://example.com/2021/11/23/d2l_3/</id>
    <published>2021-11-23T14:29:44.147Z</published>
    <updated>2021-11-23T14:29:44.147Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/d2l/1/cover.png" alt="$cover"></p><h1 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>softmax操作子<br>将输出变为一个概率分布（保证非负性与归一性）<br><img src="/images/d2l/3/softmax.png"></li><li>交叉熵损失<br>用于衡量两个概率分布的区别,将softmax输出的分布与one-hot形式的标签作为两个分布<br><img src="/images/d2l/3/cross_entropy.png"></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>数据集</li></ul><pre><code class="python">import torchimport torchvisionfrom torch.utils import datafrom torchvision import transformsdef LoadData(batch_size,resize=None):    # 用于把图片转为Tensor，会自动归一化    trans=[transforms.ToTensor()]    if resize:        trans.insert(0,transforms.Resize(resize))    trans=transforms.Compose(trans)        train_data=torchvision.datasets.FashionMNIST(        root="./dataset",train=True,        transform=trans,download=True    )    test_data=torchvision.datasets.FashionMNIST(        root="./dataset",train=False,        transform=trans,download=True    )    # print(test_data[0])        return (data.DataLoader(train_data,batch_size=batch_size,shuffle=True),        data.DataLoader(test_data,batch_size=batch_size,shuffle=False))    train_iter,test_iter=LoadData(256) </code></pre><ul><li>从零实现</li></ul><pre><code class="python"># 将图片展平num_inputs=1*28*28# 共有10类num_outputs=10w=torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad=True)b=torch.zeros(num_outputs,requires_grad=True)def Softmax(X):    # 成batch分子组成的向量    X_exp=torch.exp(X)    # 成batch个分母    partition=X_exp.sum(1,keepdim=True)    # 应用广播机制    return X_exp/partitiondef CrossEntropy(y_hat,y):    # 只有y为下标那一项起作用    # 按正常one_hot编码为0的项直接就没算了    # 一个batch的y_hat都取对应y中的值为下标那个    # 最终得到了整个batch中所有样本的损失    return -torch.log(y_hat[range(len(y_hat)),y]).sum()def Net(w,b,X):    return Softmax(torch.matmul(X,w)+b)def Sgd(params,lr):    with torch.no_grad():        for i in params:            i-=lr*i.grad            i.grad.zero_()def Train():    for epoch in range(50):        loss=0        for X,y in train_iter:            X=X.view(-1,784)            out=Net(w,b,X)            l=CrossEntropy(out,y)            l.backward()            # 学习率设置这么小是因为在损失函数或优化器中            # 没有除以batch_size,导致求的梯度会很大            Sgd([w,b],0.0001)            loss=l.item()        print(f"{epoch},{loss}")</code></pre><ul><li>简洁实现</li></ul><pre><code class="python">train_iter,test_iter=LoadData(256)# 将图片展平num_inputs=1*28*28# 共有10类num_outputs=10from torch import nn,optim# Flatten() 保留第零维度，其他全部展平net=nn.Sequential(nn.Flatten(),nn.Linear(num_inputs,num_outputs))loss_f=nn.CrossEntropyLoss()opt=optim.Adam(net.parameters())for epoch in range(50):    loss=0    for X,y in train_iter:        out=net(X)        l=loss_f(out,y)        l.backward()        opt.step()        opt.zero_grad()        loss=l.item()    print(f"{epoch},{loss}")</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/d2l/1/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Softmax-Regression&quot;&gt;&lt;a href=&quot;#Softmax-Regression&quot; class=&quot;headerlink&quot; title=&quot;</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Linear Regression</title>
    <link href="http://example.com/2021/11/23/d2l_2/"/>
    <id>http://example.com/2021/11/23/d2l_2/</id>
    <published>2021-11-23T07:59:48.186Z</published>
    <updated>2021-11-23T07:59:48.186Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/d2l/1/cover.png" alt="$cover"></p><h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>线性模型  <ul><li>基本的线性模型可抽象为如下表示,其可以看作一个单层单神经元的神经网络<br><img src="/images/d2l/2/l_model.png"></li><li>线性模型有显示解<br><img src="/images/d2l/2/ex_solution.png"></li></ul></li><li>优化<ul><li>梯度下降<br>需要注意的是梯度是t-1时刻得来的，而且梯度是t-1时刻样本点、标签值以及W参数值对应的梯度，因为他们都是损失函数中的因变量<br><img src="/images/d2l/2/grad_desc.png"></li><li>成批计算梯度<br>batch_size不能太大也不能太小。太小：并行计算难以发挥效果；太大：内存消耗增加、易陷入局部最优。batch_size是另一个重要的超参数。</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>从零实现</li></ul><pre><code class="python">import randomimport torchfrom torch.utils.data.dataloader import DataLoadertrue_w=torch.tensor([2.2,3])true_b=torch.tensor([-1.9])num_examples=2500batch_size=50epoch=100learning_rate=0.01# 初始化参数w=torch.normal(0,0.01,true_w.shape,requires_grad=True)b=torch.zeros(true_b.shape,requires_grad=True)def SyntheticData(w,b,num_examples):    X=torch.normal(0,1,size=(num_examples,len(w)))    # 支持多维度、不同维度的矩阵乘法    # 直接加b用了广播机制    y=torch.matmul(X,w) + b    # 增加噪声，使得不能完全拟合    y+=torch.normal(0,0.01,y.shape)    return X,y.view(-1,1)# 生成小批量def DataIter(batch_size,features,labels):    num_examples=len(features)        indices=list(range(num_examples))    # 打乱顺序    random.shuffle(indices)    for i in range(0,num_examples,batch_size):        batch_indices=torch.tensor(            indices[i:min(i+batch_size,num_examples)]            )                # Tensor可以接受一个列表的下标来取值        yield features[batch_indices],labels[batch_indices]# 定义模型def LinReg(X,w,b):        return torch.matmul(X,w)+b# 定义损失函数def SquaredLoss(y_hat,y):    return (y_hat-y.view(y_hat.shape))**2/2/batch_size# 优化算法def sgd(params,lr,batch_size):    with torch.no_grad():        for param in params:            param-=lr*param.grad            param.grad.zero_()    features,labels=SyntheticData(true_w,true_b,num_examples)# 训练def Train():    for i in range(epoch):        e_loss=[]        for X,y in DataIter(batch_size,features,labels):            out=LinReg(X,w,b)            # 将batch_size个损失求和            l=SquaredLoss(y,out).sum()            l.backward()            sgd([w,b],learning_rate,batch_size)            e_loss.append(l.item())        print(f"{i+1},loss:{sum(e_loss)/num_examples}")Train()# 简单验证x=torch.tensor([108,2.666])print(true_w,w)print(true_b,b)print(LinReg(x,true_w,true_b).item())print(LinReg(x,w,b).item())</code></pre><ul><li>简洁实现</li></ul><pre><code class="python">import numpy as npimport torchfrom torch.utils import datafrom torch import nnfrom torch import optimtrue_w=torch.tensor([2.2,3])true_b=torch.tensor([-1.9])num_examples=5000def SyntheticData(w,b,num_examples):    X=torch.normal(0,1,size=(num_examples,len(w)))    y=torch.matmul(X,w)+b    y+=torch.normal(0,0.01,y.shape)    return X,y.view(-1,1)features,labels=SyntheticData(true_w,true_b,num_examples)# 使第一维成为两个Tensor的共同索引，所以两个Tensor的第一维size要相同# 用来组成dataloader可处理的形式dataset=data.TensorDataset(features,labels)data_iter=data.dataloader.DataLoader(dataset,shuffle=True,batch_size=150)net=nn.Sequential(nn.Linear(len(true_w),1,bias=True))loss_f=nn.MSELoss()opt=optim.SGD(net.parameters(),lr=0.01)for epoch in range(100):    e_loss=[]    for X,y in data_iter:        out=net(X)        l=loss_f(out,y)        e_loss.append(l.item())        l.backward()        opt.step()        opt.zero_grad()        print(f"{epoch+1} {sum(e_loss)}")x=torch.tensor([108,2.666])print(true_w,true_b)for i in net.parameters():    print(i)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/d2l/1/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Linear-Regression&quot;&gt;&lt;a href=&quot;#Linear-Regression&quot; class=&quot;headerlink&quot; title=&quot;Li</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://example.com/2021/11/22/Transformer/"/>
    <id>http://example.com/2021/11/22/Transformer/</id>
    <published>2021-11-22T08:36:52.958Z</published>
    <updated>2021-11-22T08:36:52.958Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/transformer/transformer.jpg" alt="$cover"></p><h1 id="Attention-Is-All-Your-Need"><a href="#Attention-Is-All-Your-Need" class="headerlink" title="Attention Is All Your Need"></a>Attention Is All Your Need</h1><p>以往主流的序列转录(seq2seq)模型中常常基于包含Encoder与Decoder的复杂RNN或CNN，这些模型也会在Encoder与Decoder中使用Attention机制。<br>Transformer仅仅使用了Attention机制，完全没用到循环和卷积，将循环层换为了Multi-headed Attetion。Transformer训练速度更快，预测能力更好。  </p><hr><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>RNN难以并行计算计算效率低，Transformer可并行。</li><li>RNN带有时序信息，但在序列较长时，早期的信息可能在后期丢失。Attention可通过在输入序列中加入index增加时序，并且不会存在信息丢失问题。</li><li>用CNN可以替换掉RNN实现并行计算，但由于感受野的限制其依然存在难以对长序列进行建模的问题。Transformer中的Attention机制一次性看到所有的序列，消除了这一问题。</li><li>CNN可利用多个输出通道识别不一样的模式，在Transformer中使用Multi-headed Attetion，也实现了这样的特性。</li></ul><hr><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Transformer整体分为Encoder与Decoder两大部分。  </p><ul><li>Input Embedding/Output Embedding将词映射到向量。</li><li>Postoinal Encoding</li><li>Nx指有N个该块叠在一起。</li><li>Add表示残差连接，Norm表示正则处理。</li><li>在训练时，解码器的输入（outputs）是真实值(Ground Truth)；在测试时，输入（outputs）是前一时刻的输出。<br><img src="/images/transformer/architecture.png" alt="architecture"></li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>N=6，out_dim=input_dim=512。由两个子层组成，两个子层分别为Multi-headed Attetion和简单的MLP。每个子层使用残差连接和layer normalization。</p><blockquote><p>layer normalization and batch normalization</p><blockquote><p>batch:对每一个feature，将这个feature在一个batch中的所有数据的均值变为0方差变为1<br><img src="/images/transformer/batch_norm.png" alt="batch_norm"><br>layer:对每一个样本，将这个样本所有的特征均值变为0方差变为1<br><img src="/images/transformer/layer_norm.png" alt="layer_norm"><br>成sequence的normalization:<br>蓝为batch，黄为layer，取所有数据去做norm。阴影部分为样本实际长度（即该样本序列的seq_len），在实际长度外取全0。<br><img src="/images/transformer/seq_norm.png" alt="seq_norm"><br>使用layernormalization相对稳定一些。每个样本自己做均值方差再去norm。</p></blockquote></blockquote><hr><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>N=6,由三个子层组成，其中后两个子层Encoder一致，而第一个子层有所不同，其使用了Masked Multi-headed Attetion。在Decoder中还使用了自回归。当前的outputs输入是上一次的输出。在训练过程中，Decoder的输入outputs为ground truth，但在t时刻的输入不应包含t时刻之后的输入，所以第一个子层引入了masked机制。</p><hr><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><blockquote><p>Transformer使用了Scaled Dot-Product Attention。其将输入的一个seq进行融合输出一个等长的seq。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>在融合时，每一个输出都考虑了其对应位置输入元素和seq中其他元素的相关度。Wq与Wk是两个参数矩阵，原始输入做矩阵运算后得到两个向量q、k，最后用内积运算即得到了原始输入的相关性（内积运算代表了余弦相似度）。<br><img src="/images/transformer/dot-product.png" alt="dot-product"><br>同理可计算出a1与自身及序列中所有元素的相关性，全部计算完成后输入到softmax层进行归一化。<br><img src="/images/transformer/to-softmax.png" alt="to-softmax"><br>利用一个新的参数矩阵Wv获得v1-v4。v与相关性系数的带权和得到b1。<br><img src="/images/transformer/b1.png" alt="b1"><br>同理，可以得到b2、b3、b4。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>上述步骤可表示为矩阵运算。<br><img src="/images/transformer/use_matrix.png" alt="use_matrix"><br>最终的计算公式可如下表示。这里多了一个除以根号dk(input_dim)，这是因为向量(Transformer中dk=512)比较长时，内积绝对值可能会出现比较大的情况，这对梯度下降是不利的。<br><img src="/images/transformer/formula.png" alt="formula"> </p></blockquote><blockquote><p>Multi-headed Attetion可以看作是多通道的Attention。以2heads为例，计算过程如下，增加了多个参数矩阵。<br><img src="/images/transformer/multi-head.png" alt="multi-head"><br>在最后，将bi1与bi2一起其他的相应多通道b在特征维度拼接起来，为保证dim不变，利用一个新的参数矩阵Wo将输出元素的维度变为和输入相同。<br><img src="/images/transformer/concat.png" alt="concat"><br>一个线性层就可看作一个参数矩阵，所以上述两步操作可看作如下的过程。<br><img src="/images/transformer/liner-representatoin.png" alt="liner-representatoin"> </p></blockquote><hr><h3 id="Cross-Attention-in-Transformer"><a href="#Cross-Attention-in-Transformer" class="headerlink" title="Cross Attention in Transformer"></a>Cross Attention in Transformer</h3><p>Cross Attention指的是Encoder与Decoder之间的Attention机制。其V和K来自于Encoder，而Q来自于masked Attention。由于是masked，向Q输入的部分其seq_len会与V和K的不同，又因为其作为Q输入，所以cross-attention输出的seq_len会与之相同，所以Decoder的输入输出seq_len是相同的。<br><img src="/images/transformer/cross-attention.png" alt="cross-attention">  </p><hr><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed-Forward"></a>Feed-Forward</h3><p>只有一个MLP分别去作用于seq中的每个词，<br>图中MLP权重是相同的，也不需要把Encoder的输出合并输入到大的MLP。因为这里只是想要把原始维度投影到想要的另一个维度，其信息融合已经在Encoder中做完了。<br><img src="/images/transformer/feed-forward.png" alt="feed-forward"></p><hr><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>编码器要有embedding，解码器要有embedding，softmax层之前有一个Liner层，这三个层共享权重。</p><hr><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>为了给Attention加上时序信息，给输入加上位置信息。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/transformer/transformer.jpg&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Attention-Is-All-Your-Need&quot;&gt;&lt;a href=&quot;#Attention-Is-All-Your-Need</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Seq2Seq-Model" scheme="http://example.com/tags/Seq2Seq-Model/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="http://example.com/2021/11/22/resnet/"/>
    <id>http://example.com/2021/11/22/resnet/</id>
    <published>2021-11-22T08:36:44.959Z</published>
    <updated>2021-11-22T08:36:44.959Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/resnet/cover.jpg" alt="$cover"></p><h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><p>深层的神经网络通常很难进行训练，本文使用了一个残差学习网络结构来训练比以往的神经网络要深得多的模型。残差网络容易训练，并且在深层神经网络中表现出来较好的准确率。</p><p><img src="/images/resnet/plain_error.png" alt="plain_error"><br>在未使用残差网络的模型中，当网络层数变多时，训练误差以及测试误差均会升高。</p><h2 id="Is-learning-better-networks-as-easy-as-stacking-more-layers"><a href="#Is-learning-better-networks-as-easy-as-stacking-more-layers" class="headerlink" title="Is learning better networks as easy as stacking more layers ?"></a><strong>Is learning better networks as easy as stacking more layers ?</strong></h2><ul><li>当网络变得特别深时，会出现梯度爆炸或梯度消失</li><li>传统的解决方法是：参数在初始化时要做的好一点，不要太大也不要太小；加入一些Batch Normalization Layers</li><li>传统的解决方法使得神经网络能够收敛，但是网络的精度却变得更差，而这并非是模型变得复杂后导致的过拟合问题，因为模型的训练误差也变高了</li><li>正常来说，如果在一个浅层的神经网络后直接加入更多的层，这些层只做identity mapping，那么这个深层神经网络的误差绝不会高于浅层的神经网络，但是传统的神经网络模型并未找到这样的解（或更好的解）</li><li>如果深层网络后面的层都是是恒等映射，那么模型就可以转化为一个浅层网络</li></ul><h2 id="Deep-residual-learning-framework"><a href="#Deep-residual-learning-framework" class="headerlink" title="Deep residual learning framework"></a><strong>Deep residual learning framework</strong></h2><ul><li>将残差块的输入与块内最后一个神经网络层的线性输出求和后在进行激活，得到残差块的输出<br><img src="/images/resnet/residual_block.png" alt="plain_error"></li><li>残差块只简单地做了shortcut connections，没有引入额外的训练参数，不会增加网络复杂度</li><li>已有的神经网络很难拟合潜在的恒等映射函数H(x) = x，但是ResNet将残差块设计为H(x) = F(x) + x，其直接把恒等映射作为网络的一部分，只要F(x) = 0，便得到了恒等映射。而此时F(x) = H(x) - x 称为残差函数，就是当前残差块的学习目标（学习出这样一个F(x)函数满足如图输出）</li><li>值得一提的是，一个残差块中应该至少有两层（中间要包含一个非线性激活），否则就会出现如下情况，这显然是没有用的工作<br><img src="/images/resnet/no_use.png" alt="plain_error"></li></ul><h2 id="Deeper-bottleneck-architecture"><a href="#Deeper-bottleneck-architecture" class="headerlink" title="Deeper bottleneck architecture"></a><strong>Deeper bottleneck architecture</strong></h2><ul><li>当神经网络层数进一步增多时，参数的增长会带来很大的计算开销。此时可以考虑使用1*1的卷积核暂时减少通道数来减少整个网络的数据规模<br><img src="/images/resnet/deeper.png" alt="plain_error"></li></ul><h2 id="Analysis-of-Deep-Residual-Networks"><a href="#Analysis-of-Deep-Residual-Networks" class="headerlink" title="Analysis of Deep Residual Networks"></a><strong>Analysis of Deep Residual Networks</strong></h2><ul><li>残差块的反向传播过程较好地解释了残差网络避免梯度消失原因，具体推导过程如下</li><li>推导中忽略偏置项和激活函数<br><img src="/images/resnet/res_bp.png" alt="plain_error"></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/resnet/cover.jpg&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Residual-Network&quot;&gt;&lt;a href=&quot;#Residual-Network&quot; class=&quot;headerlink&quot; title=&quot;Res</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Residual-learning" scheme="http://example.com/tags/Residual-learning/"/>
    
  </entry>
  
  <entry>
    <title>GAN #1</title>
    <link href="http://example.com/2021/11/22/Gan0/"/>
    <id>http://example.com/2021/11/22/Gan0/</id>
    <published>2021-11-22T08:36:36.426Z</published>
    <updated>2021-11-22T08:36:36.426Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/gan/cover.png" alt="$cover"></p><h1 id="Generative-Adversarial-Nets"><a href="#Generative-Adversarial-Nets" class="headerlink" title="Generative Adversarial Nets"></a>Generative Adversarial Nets</h1><p>Gan利用对抗的方法，提出了生成模型的新框架。它需要同时训练两个模型（生成模型G和判别模型D），G用于捕获数据的分布，而D需要判别出一个样本是来自训练集还是生成模型。生成模型的目标是尽最大可能让判别模型犯错（无法成功判别数据的来源），G和D在本文中被定义为多层感知机，整个系统通过反向传播来进行训练。</p><h2 id="AN-analogy-to-GAN"><a href="#AN-analogy-to-GAN" class="headerlink" title="AN analogy to GAN"></a><strong>AN analogy to GAN</strong></h2><p>在生成对抗网络框架中，生成模型与判别互相对抗。可以把生成模型类比为造假币者，判别模型类比为警察。生成模型试图制造假币骗过判别模型，而判别模型努力区分假币。二者在这样的对抗中不断学习提升各自的水平，直到生成模型的造的假币和真的一模一样，判别器无法区分。另外，警察进步不能过大或过小。进步过大时，造假者直接被一锅端，无法继续造假钞；进步过小时，造假者不需进步也能骗过警察，则没有动力进步</p><h2 id="Adversarial-nets"><a href="#Adversarial-nets" class="headerlink" title="Adversarial nets"></a><strong>Adversarial nets</strong></h2><ul><li><p>对抗网络认为数据集代表着一个分布，每一个样本都可以由高维随机变量表示。假设数据集是许多张2*2大小的黑白图片，每张图片有四个像素点，于是将该分布看作四维随机变量的分布，每一维代表着一个像素的取值（黑白只取0或1）<br><img src="/images/gan/mrv.jpg"></p></li><li><p>在让生成模型学习数据集代表了分布之前，首先定义一个符合高斯分布的先验噪声Z，生成模型就是要学习把Z映射成为数据集代表的分布，MLP理论上可以拟合这样一个函数来完成目标。生成器以Z为输入，输出一个尽量符合数据集分布的样本  </p></li><li><p>判别器也是一个MLP，判别器以数据集或生成器生产的样本为输入，输出一个标量（0：生成器生成的样本，1：真实样本，判别器的输出介于[0,1]），判别样本的真伪。判别器要学习一个二分类任务  </p></li><li><p>D与G对于同一个目标函数采取相反的优化方式：<br><img src="/images/gan/obj_func.png"><br>在不同分布里采样计算后得到期望和，生成器和鉴别器分别要调整参数最大化和最小化这一期望和，体现了对抗的过程，D尽量区分生成的数据和真实数据，G尽量使得生成的数据和真实数据难以区分</p></li><li><p>下图展示了对抗网络训练过程中，各个成分的变化：<br><img src="/images/gan/graph_proc.png"><br>Z为高斯噪声，绿色线代表G生成数据的概率密度函数，黑色线代表真实数据的概率密度函数，蓝色代表D输出标量的函数<br>（a）G将Z随即映射为另一个分布，与真实分布存在一定差距，此时D未经学习，分类能力弱<br>（b）固定G，训练D，D分类能力明显上升<br>（c）固定D，训练G，G学习到把Z向真实数据分布映射<br>（d）反复多轮后，G映射的分布与真实数据相同，判别器无法区分</p></li><li><p>对抗网络算法流程如下：<br><img src="/images/gan/algorithm.png"><br>先更新D，再更新G，G只与目标函数中后半段有关。k是一个超参数，不能太大也不能大小。取太大判别器训练得太好，取太小判别器变化太小</p></li></ul><h2 id="Theoretical-results"><a href="#Theoretical-results" class="headerlink" title="Theoretical results"></a><strong>Theoretical results</strong></h2><ul><li>理论上，对抗网络存在全局最优解：生成器映射的分布等于真实数据分布</li><li>对抗网络算法可以求解目标函数</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/gan/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Generative-Adversarial-Nets&quot;&gt;&lt;a href=&quot;#Generative-Adversarial-Nets&quot; class=&quot;hea</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="papers" scheme="http://example.com/tags/papers/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
    <category term="Generative-Model" scheme="http://example.com/tags/Generative-Model/"/>
    
  </entry>
  
  <entry>
    <title>C# Asynchronous programming #3</title>
    <link href="http://example.com/2021/11/22/CSAsync3/"/>
    <id>http://example.com/2021/11/22/CSAsync3/</id>
    <published>2021-11-22T08:36:21.830Z</published>
    <updated>2021-11-22T08:36:21.830Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/CSAsynchronousProgramming/cover.png" alt="$cover"></p><h1 id="C-Asynchronous-programming-3"><a href="#C-Asynchronous-programming-3" class="headerlink" title="C# Asynchronous programming #3"></a>C# Asynchronous programming #3</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><h3 id="Use-Thread-in-Applications"><a href="#Use-Thread-in-Applications" class="headerlink" title="Use Thread in Applications"></a><strong>Use Thread in Applications</strong></h3><ul><li>在带界面的WPF、UWP、WinForm等程序种，若主线程执行耗时的操作，就会导致整个程序无响应。因为主线程同时还要处理消息循环，而渲染和鼠标键盘事件处理等工作都是在消息循环中执行的</li><li>针对这种耗时的操作，一种流行的做法使启用一个worker线程，执行完操作后再更新到UI</li><li>富客户端应用的线程模型通常是：<ul><li>UI控件只能从创建他们的线程来进行访问（通常是主线程）</li><li>当想从worker线程更新UI时，应该把请求交给UI线程</li></ul></li></ul><h3 id="Synchronization-Contexts"><a href="#Synchronization-Contexts" class="headerlink" title="Synchronization Contexts"></a><strong>Synchronization Contexts</strong></h3><ul><li>在System.ComponentModel下有一个抽象类：SynchronizationContext，它使得Thread Marshaling得到泛化<ul><li>Thread Marshaling：把一些数据的所有权从一个线程交给了另一个线程</li></ul></li></ul><h3 id="Thread-Pool"><a href="#Thread-Pool" class="headerlink" title="Thread Pool"></a><strong>Thread Pool</strong></h3><ul><li><p>当开始一个线程时，将花费数百微秒来组织一些内容（如一个新的局部变量栈），产生了开销</p></li><li><p>线程池可以节省这些开销：</p><ul><li>预先创建一个可循环使用的线程的池来减少创建新线程的开销</li></ul></li><li><p>线程池对于搞笑的并行编程和细粒度的并发是必不可少的</p></li><li><p>关于c#中的线程池需要注意以下几点：</p><ul><li>不可以设置池线程的<code>Name</code></li><li>池线程都是后台线程</li><li>阻塞池线程可能使性能降低</li><li>池线程优先级可以被自由的更改，当它被释放回池的时候优先级将被还原为正常状态</li><li>可以通过<code>IsThreadPoolThread</code>属性来判断是否为池线程</li><li><code>Task</code>也使用线程池</li></ul></li><li><p>线程池中的整洁</p><ul><li>线程池提供了另一个功能，即确保不会产生CPU超额订阅（活跃的线程数超过CPU核数），超额订阅对性能影响很大</li><li>CLR通过对任务排队并对其启动进行节流限制来避免线程池中的超额订阅</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/CSAsynchronousProgramming/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;C-Asynchronous-programming-3&quot;&gt;&lt;a href=&quot;#C-Asynchronous-p</summary>
      
    
    
    
    <category term="C#" scheme="http://example.com/categories/C/"/>
    
    
    <category term="Asynchronous-programming" scheme="http://example.com/tags/Asynchronous-programming/"/>
    
    <category term="CS-Threading" scheme="http://example.com/tags/CS-Threading/"/>
    
  </entry>
  
  <entry>
    <title>C# Asynchronous programming #2</title>
    <link href="http://example.com/2021/11/22/CSAsync2/"/>
    <id>http://example.com/2021/11/22/CSAsync2/</id>
    <published>2021-11-22T08:36:10.315Z</published>
    <updated>2021-11-22T08:36:10.315Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/CSAsynchronousProgramming/cover.png" alt="$cover"></p><h1 id="C-Asynchronous-programming-2"><a href="#C-Asynchronous-programming-2" class="headerlink" title="C# Asynchronous programming #2"></a>C# Asynchronous programming #2</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><h3 id="Thread-Safety"><a href="#Thread-Safety" class="headerlink" title="Thread Safety"></a><strong>Thread Safety</strong></h3><ul><li>本地状态与共享状态<ul><li>Local 本地独立：CLR为每个线程分配自己的内存栈，以便使本地变量保持独立</li><li>Shared 共享：如果多个线程引用到了同一个对象实例，那么他们就共享了数据；被Lambad表达式或匿名委托捕获的本地变量，会被编译器转化为字段（field），所以也会被共享；静态字段也会在线程间共享</li></ul></li></ul><pre><code class="c#">class Program{        static void Main(string[] args)    {        /*本地变量不共享*/        //打印六次hello...        Thread thread = new Thread(GO);        thread.Start();        GO();        /*同一实例共享*/        //打印一次Done！        ThreadTest test = new ThreadTest();        new Thread(test.Test).Start();        Thread.Sleep(1000);        test.Test();        /*匿名委托共享*/        //打印一次Done        bool done = false;        ThreadStart action = () =&gt;            {                if (!done)                {                    //此处还是会有输出多次的风险                    done = true;                    System.Console.WriteLine("Done");                }            };        new Thread(action).Start();        Thread.Sleep(new TimeSpan(0, 0, 1));        action.Invoke();        /*静态字段共享*/        //打印一次Done！        ThreadTest test1 = new ThreadTest();        ThreadTest test2 = new ThreadTest();        new Thread(test1.AnotherTest).Start();        Thread.Sleep(10);        test2.AnotherTest();    }    static void GO()    {           //cycle是本地变量，属于本地状态        //在每个线程的内存栈上，都会创建cycle的 独立副本        for (int cycle = 0;  cycle&lt; 3; cycle++)        {            System.Console.WriteLine("hello...");        }    }}class ThreadTest{    private bool _done;    private static bool done;    public void AnotherTest()    {        //此处还是会有输出多次的风险        if(!done)        {            System.Console.WriteLine("DONE!");            done = true;        }    }    public void Test()    {        //此处还是会有输出多次的风险        if(!_done)        {            System.Console.WriteLine("Done!");            _done = true;        }    }}</code></pre><ul><li>线程安全<ul><li>上述后涉及线程共享数据的代码是缺乏线程安全的，其实际输出无法确定，理论上Done有可能会打印两次，因为一个线程可能正在评估if，而另一个语句没来得及调整<code>done</code>为<code>true</code></li><li>保证线程安全：消除代码执行过程中的不确定性<ol><li>尽可能的避免使用共享状态以保证线程安全</li><li>使用<code>lock</code>语句加锁，在读取和写入共享数据的时候，通过使用互斥锁，就可以修复前面代码中的问题，当两个线程同时竞争一个锁的时候（锁可以基于任何引用类型对象），一个线程会等待或阻塞，直到锁重新变成可用状态</li></ol></li><li>然而，lock也并非线程安全的银弹，lock也会引起一些其他问题（如死锁）</li></ul></li></ul><pre><code class="c#">    class Program    {        static readonly object locker=new object();        static bool done;        static void Main(string[] args)        {            new Thread(GO).Start();            GO();        }        static void GO()        {            // 锁要基于引用类型的变量            // 线程安全            lock(locker)            {                if (!done)                {                    System.Console.WriteLine("Done");                    Thread.Sleep(1000);                    done = true;                }            }        }    }</code></pre><h3 id="Transfer-Parameter"><a href="#Transfer-Parameter" class="headerlink" title="Transfer Parameter"></a><strong>Transfer Parameter</strong></h3><ul><li>如果想往线程启动方法里传递参数，最简单的方式就是使用lambda表达式，在里面使用参数调用方法</li></ul><pre><code class="c#">class Program    {                static void Main(string[] args)        {            new Thread(() =&gt; { Print("hello world"); }).Start();        }        static void Print(string s)        {            System.Console.WriteLine(s);        }    }</code></pre><ul><li>还可以使用<code>Thread.Start</code>方法来传递参数，类似委托中<code>Invoke</code>时的传参<ul><li>Thread的重载构造函数可以接受下列两个委托之一作为参数：<ol><li><code>public delegate void ThreadStart();</code></li><li><code>public delegate void ParameterizedThreadStart(object obj);</code></li></ol></li><li>第二种委托接收带参数的方法名</li></ul></li></ul><pre><code class="c#">class Program{        static void Main(string[] args)    {        Thread thread = new Thread(Print);        thread.Start("hello world");    }    static void Print(object s)    {        s = s as string;        System.Console.WriteLine(s);    }}</code></pre><ul><li>需要注意的是lambda表达式（匿名委托）传的参数会当作字段，即就算传的是值类型的变量也会得到其地址并被线程共享；而第二种传参强制要求了引用变量，也会被线程共享</li></ul><pre><code class="c#">class Program{      static void Main(string[] args)    {        // Thread1();        Thread2();    }    // 线程共享了同一个局部变量i    // 会出现重复的数    static void Thread1()    {        for (int i = 0; i &lt; 5; i++)        {            new Thread(() =&gt; { System.Console.WriteLine(i); }).Start();        }    }    // 每个线程获得不同局部变量的地址    // 不会出现重复的数    static void Thread2()    {        for (int i = 0; i &lt; 5; i++)        {            int temp = i;            new Thread(() =&gt; { System.Console.WriteLine(temp); }).Start();        }    }    }</code></pre><h3 id="Exception"><a href="#Exception" class="headerlink" title="Exception"></a><strong>Exception</strong></h3><ul><li>异常处理块种的线程抛出异常时，不会被捕获，解决方案是传入线程的方法中处理异常</li></ul><pre><code class="c#">class Program{      static void Main(string[] args)    {        // 正常捕获        new Thread(Go2).Start();        // 无法捕获        try        {            new Thread(Go1).Start();        }        catch        {            System.Console.WriteLine("Exceptoin!");        }    }       static void Go1()    {        throw new Exception();    }    static void Go2()    {        try        {            throw new Exception();        }        catch        {            System.Console.WriteLine("catch sucessfully");        }    }    }</code></pre><h3 id="Foreground-Threads-and-Background-Threads"><a href="#Foreground-Threads-and-Background-Threads" class="headerlink" title="Foreground Threads and Background Threads"></a><strong>Foreground Threads and Background Threads</strong></h3><ul><li>默认情况下，手动创建的线程就是前台线程</li><li>只要有前台线程在运行，那么应用程序就会一直处于活动状态<ul><li>后台线程运行不会保持应用程序的活动状态</li><li>一旦所有前台线程停止，应用程序随即停止，后台线程也会立即终止</li></ul></li><li>线程的前台、后台与它的优先级无关</li><li>通过<code>IsBackground</code>属性判断线程是否是后台线程</li></ul><pre><code class="c#">class Program{      static void Main(string[] args)    {        Thread thread = new Thread(() =&gt; { Console.WriteLine(Console.ReadLine());});                // 如果将thread设置为后台线程，当主线程（前台线程）执行完之后程序立即结束        if(args.Length&gt;0)            thread.IsBackground = true;        thread.Start();    }       }</code></pre><ul><li>如果在退出前想要等待后台线程执行完毕，可以考虑使用<code>Join</code></li><li>应用程序无法正常退出的一个常见原因就是还有活跃的前台线程</li></ul><h3 id="Priority-of-Threading"><a href="#Priority-of-Threading" class="headerlink" title="Priority of Threading"></a><strong>Priority of Threading</strong></h3><ul><li>线程的优先级（<code>Thread</code>中<code>Priority</code>属性）决定了相对于操作系统中其他活跃线程所占的执行时间</li><li>优先级划分：<ul><li><code>enum ThreadPriority{Lowest,BelowNormal,Normal,AboveNormal,Highest}</code></li></ul></li><li>提升线程优先级：<ul><li>提升线程优先级时要特别注意，因为他可能饿死其他线程</li><li>如果想让某线程的优先级比其他进程中的线程优先级高，那么就必须提升进程优先级</li></ul></li><li>手动提升进程/线程优先级适用于只做少量工作且需要较低延迟的非UI进程</li><li>对于需要大量计算的应用程序（尤其是带UI的），手动提升进程/线程优先级可能会使其他进程/线程饿死，从而降低整个计算机的速度</li></ul><h3 id="Signaling"><a href="#Signaling" class="headerlink" title="Signaling"></a><strong>Signaling</strong></h3><ul><li>某个线程在收到其他线程发来通知之前一直处于等待状态，发送通知的过程就称为Signaling，不同于信号量机制</li><li>最简单的信号结构就是<code>ManualResetEvent</code><ul><li>调用其上<code>WaitOne</code>方法会阻塞当前线程，直到另一个线程通过调用<code>Set</code>方法开启信号</li></ul></li></ul><pre><code class="c#">class Program{     static void Main(string[] args)    {        var signal = new ManualResetEvent(false);        new Thread(() =&gt;        {            System.Console.WriteLine("Waiting for signal...");            // 因为在实例化时设置了false主线程打开信号之前处于阻塞状态            // 成功接收到信号时不会将signal状态重置为nonusignaled            signal.WaitOne();            // 获得信号后直接关闭该信号量            System.Console.WriteLine("Got signal!");        }).Start();        for (int i = 0; i &lt; 5; i++)        {            System.Console.WriteLine(i);        }        Thread.Sleep(1000);        signal.Set();    }   }</code></pre><ul><li>调用完<code>Set</code>后信号会处于signaled状态，通过调用<code>Reset</code>将信号重新变为nonsignaled状态</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/CSAsynchronousProgramming/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;C-Asynchronous-programming-2&quot;&gt;&lt;a href=&quot;#C-Asynchronous-p</summary>
      
    
    
    
    <category term="C#" scheme="http://example.com/categories/C/"/>
    
    
    <category term="Asynchronous-programming" scheme="http://example.com/tags/Asynchronous-programming/"/>
    
    <category term="CS-Threading" scheme="http://example.com/tags/CS-Threading/"/>
    
  </entry>
  
  <entry>
    <title>C# Asynchronous programming #1</title>
    <link href="http://example.com/2021/11/22/CSAsync1/"/>
    <id>http://example.com/2021/11/22/CSAsync1/</id>
    <published>2021-11-22T08:35:34.837Z</published>
    <updated>2021-11-22T08:35:34.837Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/CSAsynchronousProgramming/cover.png" alt="$cover"></p><h1 id="C-Asynchronous-programming-1"><a href="#C-Asynchronous-programming-1" class="headerlink" title="C# Asynchronous programming #1"></a>C# Asynchronous programming #1</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><hr><h3 id="What-is-Thread"><a href="#What-is-Thread" class="headerlink" title="What is Thread ?"></a><strong>What is Thread ?</strong></h3><ul><li>线程是一个可执行路径，每一个线程可以独立于其他线程执行</li><li>每个线程在均进程(Process)内执行，在操作系统中，进行提供了程序运行的独立环境</li><li>单线程应用，在进程的独立环境中只跑一个线程，该线程具有独占权</li><li>多线程应用，单个进程中跑多个线程，多个线程共享当前的执行环境（尤其是内存）<ul><li>共享：多个线程共同占有某种资源，如一个线程在后台读取数据，另一个线程在数据到达后进行展示。  </li></ul></li></ul><p>下面是c#中最简单的异步编程实例：</p><pre><code class="c#">class Program{    static void Main(string[] args)    {        Thread.CurrentThread.Name = "Main Thread...";        //开辟一个新线程                    Thread thread = new Thread(WriteY);        thread.Name = "Y Thread...";        thread.Start();        Thread.Sleep(1);        //同时在主线程上做一些工作        System.Console.WriteLine(Thread.CurrentThread.Name);        for (int i = 0; i &lt; 1000; i++)        {            System.Console.Write("x");        }            }    static void WriteY()    {        System.Console.WriteLine(Thread.CurrentThread.Name);        for (int i = 0; i &lt; 1000; i++)        {            System.Console.Write("y");        }            }}</code></pre><ul><li>在单核计算机上，操作系统必须为每个线程分配<strong>时间片</strong>（Windows下通常为20ms）来模拟并发，从而在本例中，会输出重复的x块与y块</li><li>在多核或多处理器的计算机上，使用c#创建的多线程可以真正意义上并行执行。然而，在本例中由于控制台程序处理并发请求机制的微妙性，仍然会得到重复的x块与y块<br><img src="/images/CSAsynchronousProgramming/first_example.png" alt="first_example"></li><li>c#中线程的一些属性：<ul><li>线程一旦开始执行，属性<code>IsAlive</code>就变为<code>true</code>，线程结束就变为<code>false</code></li><li>线程结束的条件：线程构造器中传入的委托结束了执行</li><li>线程一旦结束，便无法重启</li><li>每个线程都有一个<code>Name</code>属性，通常用于调试，<code>Name</code>只能设置一次，多次更改会抛出异常</li><li>静态属性<code>Thread.CurrentThread</code>，指向当前执行的线程</li></ul></li></ul><h3 id="Join-and-Sleep"><a href="#Join-and-Sleep" class="headerlink" title="Join and Sleep"></a><strong>Join and Sleep</strong></h3><ul><li>在线程A中调用另一个线程B实例的<code>Join</code>方法，线程A便会等待线程B执行结束后继续执行。</li><li>调用<code>Jion</code>方法时，可在参数中设置一个超时(毫秒/<code>TimeSpan</code>)<ul><li>使用有超时的重载方法时返回值<code>bool</code>类型，<code>true</code>：线程结束；<code>false</code>：超时</li></ul></li><li><code>Thread.Sleep</code>方法会暂停当前的线程，并等待一段时间<ul><li><code>Thread.Sleep(0)</code>会导致县城立即放弃本身当前时间片，自动将cpu转交给其他线程</li><li><code>Thread.Yield</code>与<code>Thread.Sleep</code>做同样的事情，不同的是<code>Thread.Yield</code>只会把执行交给同一处理器的其他线程</li><li>当等待<code>Sleep</code>和<code>Join</code>时，线程处于阻塞状态</li></ul></li></ul><pre><code class="c#">class Program    {        static Thread thread1;        static Thread thread2;        static void Main(string[] args)        {            thread1 = new Thread(ThreadProc);            thread1.Name = "t1";            thread1.Start();            thread2 = new Thread(ThreadProc);            thread2.Name = "t2";            thread2.Start();        }        private static void ThreadProc()        {            System.Console.WriteLine($"\nCurrent thread:{Thread.CurrentThread.Name}");            if(Thread.CurrentThread.Name=="t1"&amp;&amp;            ((thread2.ThreadState&amp;ThreadState.Unstarted)==0))                if(thread2.Join(2000))                    System.Console.WriteLine("t2 has been terminated");                else                    System.Console.WriteLine("overtime");            //TimeSpan(0,0,4) 0h，0m，4s            Thread.Sleep(new TimeSpan(0,0,4));            System.Console.WriteLine($"\nCurrent thread:{Thread.CurrentThread.Name}");            System.Console.WriteLine($"Thread1:{thread1.ThreadState}");            System.Console.WriteLine($"Thread2:{thread2.ThreadState}");        }    }</code></pre><p>在一次执行中，输出结果如下：<br><img src="/images/CSAsynchronousProgramming/join_sleep.png" alt="join_sleep"><br>在此次运行过程中，线程<code>t2</code>首先进入<code>ThreadProc</code>，之后开始Sleep，时间片交给线程<code>t1</code>，if判断成立，<code>t1</code>进入阻塞状态等待<code>t2</code>运行结束。<code>t2</code>Sleep结束后开始运行，运行结束后，<code>t1</code>继续运行直到结束</p><h3 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a><strong>Blocking</strong></h3><ul><li>如果线程的执行由于某种原因导致暂停，那么就认为该线程被阻塞了<ul><li>如<code>Sleep</code>、<code>Join</code></li></ul></li><li>被阻塞的线程会立即将其时间片交给其他线程，从此不再消耗处理器时间，知道满足阻塞结束条件为止</li><li>可以通过<code>ThreadState</code>属性来判断线程是否处于阻塞状态</li><li><code>ThreadState</code>是一个<em>flags enum</em>，通过按位与/或来合并数据项<br><img src="/images/CSAsynchronousProgramming/thread_state.png" alt="thread_state"><br><code>ThreadState</code>变化图如下：<br><img src="/images/CSAsynchronousProgramming/transform.png" alt="transform"><br>常用的状态只有四个：<code>Unstarted</code>、<code>Running</code>、<code>WaitSleepJoin</code>和<code>Stopped</code></li></ul><pre><code class="c#">  public static ThreadState Foo(ThreadState ts)          {              return ts &amp; (                  ThreadState.Stopped |                  ThreadState.Unstarted |                  ThreadState.WaitSleepJoin              );          }</code></pre><ul><li><p>当遇到下列四种情况解除阻塞：</p><ul><li>阻塞条件被满足</li><li>操作超时(如果设置了超时)</li><li>通过<code>Thread.Interrupt()</code>进行打断</li><li>通过<code>Thread.Abort()</code>进行中止</li></ul></li><li><p>上下文切换</p><ul><li>当线程阻塞或解除阻塞时，操作系统执行上下文切换。这会产生少量开销，通常为1或2微秒</li></ul></li><li><p>I/O密集型与CPU密集型</p><ul><li>花费大部分时间等待某事发生的操作称为I/O密集型，通常此事指输入/输出，但不是硬性要求，如<code>Thread.Sleep()</code>被视为I/O密集型</li><li>相反，一个花费大部分时间执行CPU密集型的操作成为CPU密集型</li></ul></li><li><p>阻塞与忙等待</p><ul><li>阻塞是在当前线程上同步的等待，<code>Console.ReadLine()</code>、<code>Thread.Sleep()</code>、<code>Thread.Join()</code>都是阻塞操作</li><li>忙等待以周期性的在一个循环里打转，也是同步的<br><code>while(DataTime.Now&lt;nextStartTime)</code></li><li>还有一种异步的操作，在操作完成后触发回调</li><li>如果条件很快得到满足（在几微秒之内）,短暂的忙等待更为适合，因为他避免了上下文切换的开销</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/CSAsynchronousProgramming/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;C-Asynchronous-programming-1&quot;&gt;&lt;a href=&quot;#C-Asynchronous-p</summary>
      
    
    
    
    <category term="C#" scheme="http://example.com/categories/C/"/>
    
    
    <category term="Asynchronous-programming" scheme="http://example.com/tags/Asynchronous-programming/"/>
    
    <category term="CS-Threading" scheme="http://example.com/tags/CS-Threading/"/>
    
  </entry>
  
  <entry>
    <title>D2L: Matrix</title>
    <link href="http://example.com/2021/11/22/d2l_1/"/>
    <id>http://example.com/2021/11/22/d2l_1/</id>
    <published>2021-11-22T08:34:36.985Z</published>
    <updated>2021-11-22T08:34:36.985Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/d2l/1/cover.png" alt="$cover"></p><h1 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>标量</p><pre><code class="python">x=torch.tensor([1.0])y=torch.tensor([2.0])print(x+y,x*y,x/y,x**y)</code></pre></li><li><p>向量</p><pre><code class="python">x=torch.arange(4)print(x,x[3],len(x),x.shape,x.size())</code></pre></li><li><p>矩阵</p><pre><code class="python">x=torch.eye(20)x=torch.arange(20).view(5,4)print(x,x.T,x.t())print(torch.arange(24).reshape(2,3,4))</code></pre></li><li><p>运算</p><pre><code class="python">A=torch.arange(20,dtype=torch.float32).view(5,4)B=A.clone() #创建A的副本给B，A与B指向不同地址C=B  #B与C指向相同地址# B，C都会改变C[0,:] = 0print(A,B,C)# 矩阵加 点乘 每个元素求sinprint(A+B,A*B,torch.sin(A))A=torch.arange(2*20).view(2,5,4).float()print(A)# 求和可指定维度,不指定则全部维度求和print(A.sum(axis=0),A.sum([0,1]),A.sum())# 求均值可指定维度,不指定则全部维度求均值# keepdim=True代表不把求均值的那个维度丢掉，一般用来保证广播机制的正常运行print(A.mean(axis=0,keepdim=True),A.mean([0,1]),A.mean())# 求个数不可以指定维度print(A.numel())# 点积X=torch.arange(4).float()Y=torch.ones(4)print(torch.dot(X,Y))# 2范数,即向量长度，如果输入超过一维，会展成一维然后当成向量计算print(torch.norm(X))# 矩阵乘法X=X.view(2,2)Y=Y.view(2,2)print(torch.mm(X,Y))</code></pre></li></ul><h2 id="Derivative"><a href="#Derivative" class="headerlink" title="Derivative"></a><strong>Derivative</strong></h2><ul><li><p>向量上的导数<br>与向量相关的导数有以下四种形式<br><img src="/images/d2l/1/dv.png"></p><ul><li>y标量，x向量<br>y是由x中各分量计算得到的标量，最终得到y分别每个分量求导得出的向量，这也是梯度的计算过程<br><img src="/images/d2l/1/dv_1.png"></li><li>y向量，x标量<br>向量y的每个分量都是x的函数，每个分量分别对x求导最终得出一个向量<br><img src="/images/d2l/1/dv_2.png"></li><li>y向量，x向量<br>向量y的每个分量都是由x中各分量计算得到的标量，y的每个分量都分别对x的每个分量求导得出一个向量，最后得到一个矩阵<br><img src="/images/d2l/1/dv_3.png"></li></ul></li><li><p>矩阵上的导数<br>导数同样也可以被扩展到矩阵<br><img src="/images/d2l/1/dm.png"></p></li></ul><h2 id="Automatic-derivative"><a href="#Automatic-derivative" class="headerlink" title="Automatic derivative"></a><strong>Automatic derivative</strong></h2><ul><li><p>链式求法则<br>在神经网络中需要关注向量的链式求导，关键还是要把形状搞对<br><img src="/images/d2l/1/chain.png"> </p></li><li><p>自动求导<br>其含义是计算一个函数在指定值上的导数，它不同于符号求导和数值求导</p><ul><li><p>计算图<br>计算图本质上就等价于链式求导法则的求导过程，它将计算表示成一个无环图，下面是具体的例子<br><img src="/images/d2l/1/calc_graph.png"></p></li><li><p>反向传播<br>反向传播解决了自动求导的问题，它利用在前向传播时计算图中存储的计算中间结果，一步一步反向算出链式求导中各步导数<br><img src="/images/d2l/1/bp.png"></p></li><li><p>自动求导实现</p><pre><code class="python"># pytorch中做Tensor的计算时默认直接构造计算图# 计算图中入度为0的节点还应该存储关于自身的梯度，需要手动设置requires_grad=Truex=torch.arange(4.0,requires_grad=True)y=2*torch.dot(x,x)print(y)y.backward()# 计算得出的梯度存在grad中print(x.grad)# 要注意pytorch默认会把同一个变量上的梯度累加# 在另一轮计算梯度时需要先清除之前的值x.grad.zero_()y=x.sum()y.backward()print(x.grad)# 在成batch地计算损失的过程中# 最后将一个向量对向量求导的过程，转化为了标量对向量求导x.grad.zero_()# y是一个向量，把它对应为一个batch的损失y=x*x# 如果直接求导，会得到一个矩阵，这不是我们想要的# 将batch中每一个损失加起来得到一个batch的损失后再求导# 这也是pytorch中所做的y.sum().backward()print(x.grad)# 将某些计算移动到计算图之外# 这在想要固定网络中某些参数时是有用的x.grad.zero_()y=x*x# u不被计入为计算图的一个单元，而是一个常数u=y.detach()z=u*xz.sum().backward()print(x.grad==u)# 控制流也可以正确构成计算图并计算梯度</code></pre></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/d2l/1/cover.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Matrix&quot;&gt;&lt;a href=&quot;#Matrix&quot; class=&quot;headerlink&quot; title=&quot;Matrix&quot;&gt;&lt;/a&gt;Matrix&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://example.com/categories/Machine-Learning/"/>
    
    
    <category term="Dive-Into-Deep-Learning" scheme="http://example.com/tags/Dive-Into-Deep-Learning/"/>
    
    <category term="Neural-Network" scheme="http://example.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Typography and tags</title>
    <link href="http://example.com/2021/10/15/Typography%20and%20tags/"/>
    <id>http://example.com/2021/10/15/Typography%20and%20tags/</id>
    <published>2021-10-15T07:34:05.000Z</published>
    <updated>2021-10-31T14:40:08.168Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/lake.png" alt="$cover"></p><h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><hr><p>This post uses <code>hexo-renderer-markdown-it</code> plugin as markdown processor, so please install it to achieve the effect.</p><pre><code class="bash">npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --savenpm i markdown-it-emoji --savenpm i markdown-it-mark --savenpm i markdown-it-deflist --savenpm i markdown-it-container --save</code></pre><h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><p>Add following to <code>_config.yml</code> of your site.</p><pre><code class="yml">markdown:  render:    html: true    xhtmlOut: false    breaks: false    linkify: true    typographer: true    quotes: '“”‘’'  plugins:    - markdown-it-abbr    - markdown-it-footnote    - markdown-it-ins    - markdown-it-sub    - markdown-it-sup    - markdown-it-deflist  anchors:    level: 2    collisionSuffix: 'v'    permalink: false    permalinkClass: header-anchor    permalinkSymbol: " "    permalinkBefore: false</code></pre><h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><hr><h2 id="Headings"><a href="#Headings" class="headerlink" title="Headings"></a>Headings</h2><pre><code class="markdown"># h1 Heading 8-)## h2 Heading### h3 Heading#### h4 Heading##### h5 Heading###### h6 Heading</code></pre><h1 id="h1-Heading-8"><a href="#h1-Heading-8" class="headerlink" title="h1 Heading 8-)"></a>h1 Heading 8-)</h1><h2 id="h2-Heading"><a href="#h2-Heading" class="headerlink" title="h2 Heading"></a>h2 Heading</h2><h3 id="h3-Heading"><a href="#h3-Heading" class="headerlink" title="h3 Heading"></a>h3 Heading</h3><h4 id="h4-Heading"><a href="#h4-Heading" class="headerlink" title="h4 Heading"></a>h4 Heading</h4><h5 id="h5-Heading"><a href="#h5-Heading" class="headerlink" title="h5 Heading"></a>h5 Heading</h5><h6 id="h6-Heading"><a href="#h6-Heading" class="headerlink" title="h6 Heading"></a>h6 Heading</h6><h2 id="Horizontal-Rules"><a href="#Horizontal-Rules" class="headerlink" title="Horizontal Rules"></a>Horizontal Rules</h2><pre><code class="markdown">___---***</code></pre><hr><hr><hr><h2 id="Typographic-replacements"><a href="#Typographic-replacements" class="headerlink" title="Typographic replacements"></a>Typographic replacements</h2><pre><code class="markdown">(c) (C) (r) (R) (tm) (TM) (p) (P) +-test.. test... test..... test?..... test!....!!!!!! ???? ,,  -- ---"Smartypants, double quotes" and 'single quotes'</code></pre><p>(c) (C) (r) (R) (tm) (TM) (p) (P) +-</p><p>test.. test… test….. test?….. test!….</p><p>!!!!!! ???? ,,  – —</p><p>“Smartypants, double quotes” and ‘single quotes’</p><h2 id="Emphasis"><a href="#Emphasis" class="headerlink" title="Emphasis"></a>Emphasis</h2><pre><code class="markdown">**This is bold text**__This is bold text__*This is italic text*_This is italic text_~~Strikethrough~~</code></pre><p><strong>This is bold text</strong></p><p><strong>This is bold text</strong></p><p><em>This is italic text</em></p><p><em>This is italic text</em></p><p><del>Strikethrough</del></p><h2 id="Blockquotes"><a href="#Blockquotes" class="headerlink" title="Blockquotes"></a>Blockquotes</h2><pre><code class="markdown">&gt; Blockquotes can also be nested...&gt;&gt; ...by using additional greater-than signs right next to each other...&gt; &gt; &gt; ...or with spaces between arrows.</code></pre><blockquote><p>Blockquotes can also be nested…</p><blockquote><p>…by using additional greater-than signs right next to each other…</p><blockquote><p>…or with spaces between arrows.</p></blockquote></blockquote></blockquote><h2 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h2><h3 id="Unordered"><a href="#Unordered" class="headerlink" title="Unordered"></a>Unordered</h3><pre><code class="markdown">+ Create a list by starting a line with `+`, `-`, or `*`+ Sub-lists are made by indenting 2 spaces:  - Marker character change forces new list start:    * Ac tristique libero volutpat at    + Facilisis in pretium nisl aliquet    - Nulla volutpat aliquam velit+ Very easy!</code></pre><ul><li>Create a list by starting a line with <code>+</code>, <code>-</code>, or <code>*</code></li><li>Sub-lists are made by indenting 2 spaces:<ul><li>Marker character change forces new list start:<ul><li>Ac tristique libero volutpat at</li></ul><ul><li>Facilisis in pretium nisl aliquet</li></ul><ul><li>Nulla volutpat aliquam velit</li></ul></li></ul></li><li>Very easy!</li></ul><h3 id="Ordered"><a href="#Ordered" class="headerlink" title="Ordered"></a>Ordered</h3><pre><code class="markdown">1. Lorem ipsum dolor sit amet  1. Indented list    1. Another level  2. Indent2. Consectetur adipiscing elit3. Integer molestie lorem at massa</code></pre><ol><li>Lorem ipsum dolor sit amet</li><li>Indented list    1. Another level</li><li>Indent</li><li>Consectetur adipiscing elit</li><li>Integer molestie lorem at massa</li></ol><pre><code class="markdown">1. You can use sequential numbers...1. ...or keep all the numbers as `1.`</code></pre><ol><li>You can use sequential numbers…</li><li>…or keep all the numbers as <code>1.</code></li></ol><h4 id="Start-numbering-with-offset"><a href="#Start-numbering-with-offset" class="headerlink" title="Start numbering with offset:"></a>Start numbering with offset:</h4><pre><code class="markdown">57. foo1. bar</code></pre><ol start="57"><li>foo</li><li>bar</li></ol><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><pre><code class="markdown">Inline `code`</code></pre><p>Inline <code>code</code></p><h3 id="Indented-code"><a href="#Indented-code" class="headerlink" title="Indented code"></a>Indented code</h3><pre><code class="markdown">// Some comments    line 1 of code    line 2 of code    line 3 of code</code></pre><pre><code>// Some commentsline 1 of codeline 2 of codeline 3 of code</code></pre><h3 id="Block-code-“fences”"><a href="#Block-code-“fences”" class="headerlink" title="Block code “fences”"></a>Block code “fences”</h3><pre><code class="markdown">```Sample text here...```</code></pre><pre><code>Sample text here...</code></pre><p>Syntax highlighting</p><pre><code class="markdown">``` js sample.jsvar foo = function (bar) {  return bar++;};console.log(foo(5));```</code></pre><pre><code class="js">var foo = function (bar) {  return bar++;};console.log(foo(5));</code></pre><h2 id="Tables"><a href="#Tables" class="headerlink" title="Tables"></a>Tables</h2><pre><code class="markdown">| Option | Description |Description | Description | Description | Description || ------ | ----------- |----------- | ----------- | ----------- | ----------- || data   | path to data files to supply the data that will be passed into templates. || engine | engine to be used for processing templates. Handlebars is the default. || ext    | extension to be used for dest files. |</code></pre><table><thead><tr><th>Option</th><th>Description</th><th>Description</th><th>Description</th><th>Description</th><th>Description</th></tr></thead><tbody><tr><td>data</td><td>path to data files to supply the data that will be passed into templates.</td><td></td><td></td><td></td><td></td></tr><tr><td>engine</td><td>engine to be used for processing templates. Handlebars is the default.</td><td></td><td></td><td></td><td></td></tr><tr><td>ext</td><td>extension to be used for dest files.</td><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="Right-aligned-columns"><a href="#Right-aligned-columns" class="headerlink" title="Right aligned columns"></a>Right aligned columns</h3><pre><code class="markdown">| Option | Description || ------:| -----------:|| data   | path to data files to supply the data that will be passed into templates. || engine | engine to be used for processing templates. Handlebars is the default. || ext    | extension to be used for dest files. |</code></pre><table><thead><tr><th align="right">Option</th><th align="right">Description</th></tr></thead><tbody><tr><td align="right">data</td><td align="right">path to data files to supply the data that will be passed into templates.</td></tr><tr><td align="right">engine</td><td align="right">engine to be used for processing templates. Handlebars is the default.</td></tr><tr><td align="right">ext</td><td align="right">extension to be used for dest files.</td></tr></tbody></table><h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><pre><code class="markdown">[link text](http://dev.nodeca.com)</code></pre><p><a href="http://dev.nodeca.com/">link text</a></p><pre><code class="markdown">[link with title](http://nodeca.github.io/pica/demo/ "title text!")</code></pre><p><a href="http://nodeca.github.io/pica/demo/" title="title text!">link with title</a></p><pre><code class="markdown">Autoconverted link https://github.com/nodeca/pica (enabled linkify)</code></pre><p>Autoconverted link <a href="https://github.com/nodeca/pica">https://github.com/nodeca/pica</a> (enabled linkify)</p><h2 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h2><pre><code class="markdown">![Minion](https://octodex.github.com/images/minion.png)![Stormtroopocat](https://octodex.github.com/images/stormtroopocat.jpg "The Stormtroopocat")</code></pre><p><img src="https://octodex.github.com/images/minion.png" alt="Minion"><br><img src="https://octodex.github.com/images/stormtroopocat.jpg" alt="Stormtroopocat" title="The Stormtroopocat"></p><p>Like links, Images also have a footnote style syntax</p><pre><code class="markdown">![Alt text][id]With a reference later in the document defining the URL location:[id]: https://octodex.github.com/images/dojocat.jpg  "The Dojocat"</code></pre><p><img src="https://octodex.github.com/images/dojocat.jpg" alt="Alt text" title="The Dojocat"></p><p>With a reference later in the document defining the URL location:</p><h2 id="Plugins"><a href="#Plugins" class="headerlink" title="Plugins"></a>Plugins</h2><p>The killer feature of <code>markdown-it</code> is very effective support of<br><a href="https://www.npmjs.org/browse/keyword/markdown-it-plugin">syntax plugins</a>. The sample <a href="#configuration">configuration snippet</a></p><h3 id="Emojies"><a href="#Emojies" class="headerlink" title="Emojies"></a><a href="https://github.com/markdown-it/markdown-it-emoji">Emojies</a></h3><pre><code class="markdown">Classic markup: :wink: :crush: :cry: :tear: :laughing: :yum:Shortcuts (emoticons): :-) :-( 8-) ;)</code></pre><p>Classic markup: <span class="github-emoji"><span>😉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> :crush: <span class="github-emoji"><span>😢</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> :tear: <span class="github-emoji"><span>😆</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f60b.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><p>Shortcuts (emoticons): :-) :-( 8-) ;)</p><h3 id="Subscript-Superscript"><a href="#Subscript-Superscript" class="headerlink" title="Subscript / Superscript"></a><a href="https://github.com/markdown-it/markdown-it-sub">Subscript</a> / <a href="https://github.com/markdown-it/markdown-it-sup">Superscript</a></h3><pre><code class="markdown">Superscript: 19^th^Subscript: H~2~O</code></pre><p>Superscript: 19^th^</p><p>Subscript: H<del>2</del>O</p><h3 id="lt-ins-gt"><a href="#lt-ins-gt" class="headerlink" title="<ins>"></a><a href="https://github.com/markdown-it/markdown-it-ins">&lt;ins&gt;</a></h3><pre><code class="markdown">++Inserted text++</code></pre><p>++Inserted text++</p><h3 id="lt-mark-gt"><a href="#lt-mark-gt" class="headerlink" title="<mark>"></a><a href="https://github.com/markdown-it/markdown-it-mark">&lt;mark&gt;</a></h3><pre><code class="markdown">==Marked text==</code></pre><p>==Marked text==</p><h3 id="Footnotes"><a href="#Footnotes" class="headerlink" title="Footnotes"></a><a href="https://github.com/markdown-it/markdown-it-footnote">Footnotes</a></h3><pre><code class="markdown">Footnote 1 link[^first].Footnote 2 link[^second].Inline footnote^[Text of inline footnote] definition.Duplicated footnote reference[^second].[^first]: Footnote **can have markup**    and multiple paragraphs.[^second]: Footnote text.</code></pre><p>Footnote 1 link[^first].</p><p>Footnote 2 link[^second].</p><p>Inline footnote^[Text of inline footnote] definition.</p><p>Duplicated footnote reference[^second].</p><p>[^first]: Footnote <strong>can have markup</strong></p><pre><code>and multiple paragraphs.</code></pre><p>[^second]: Footnote text.</p><h3 id="Definition-lists"><a href="#Definition-lists" class="headerlink" title="Definition lists"></a><a href="https://github.com/markdown-it/markdown-it-deflist">Definition lists</a></h3><pre><code class="markdown">Term 1:   Definition 1with lazy continuation.</code></pre><dl><dt>Term 1</dt><dd>Definition 1<br>with lazy continuation.</dd></dl><pre><code class="markdown">Term 2 with *inline markup*:   Definition 2        { some code, part of Definition 2 }    Third paragraph of definition 2.</code></pre><dl><dt>Term 2 with <em>inline markup</em></dt><dd>Definition 2</dd></dl><pre><code>    { some code, part of Definition 2 }Third paragraph of definition 2.</code></pre><p><em>Compact style:</em></p><pre><code class="markdown">Term 1  ~ Definition 1Term 2  ~ Definition 2a  ~ Definition 2b</code></pre><p>Term 1<br>  ~ Definition 1</p><p>Term 2<br>  ~ Definition 2a<br>  ~ Definition 2b</p><h3 id="Abbreviations"><a href="#Abbreviations" class="headerlink" title="Abbreviations"></a><a href="https://github.com/markdown-it/markdown-it-abbr">Abbreviations</a></h3><pre><code class="markdown">This is HTML abbreviation example.It converts "HTML", but keep intact partial entries like "xxxHTMLyyy" and so on.*[HTML]: Hyper Text Markup Language</code></pre><p>This is HTML abbreviation example.</p><p>It converts “HTML”, but keep intact partial entries like “xxxHTMLyyy” and so on.</p><p>*[HTML]: Hyper Text Markup Language</p><h3 id="Custom-containers"><a href="#Custom-containers" class="headerlink" title="Custom containers"></a><a href="https://github.com/markdown-it/markdown-it-container">Custom containers</a></h3><p>::: warning<br><em>here be dragons</em><br>:::</p><h2 id="Hexo-Built-in-Tags"><a href="#Hexo-Built-in-Tags" class="headerlink" title="Hexo Built-in Tags"></a>Hexo Built-in Tags</h2><h3 id="Blockquote-with-author"><a href="#Blockquote-with-author" class="headerlink" title="Blockquote with author"></a>Blockquote with author</h3><pre><code class="swig">{% blockquote David Levithan, Wide Awake %}Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy.{% endblockquote %}</code></pre><blockquote><p>Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy.</p><footer><strong>David Levithan</strong><cite>Wide Awake</cite></footer></blockquote><h3 id="Blockquote-for-twitter"><a href="#Blockquote-for-twitter" class="headerlink" title="Blockquote for twitter"></a>Blockquote for twitter</h3><pre><code class="swig">{% blockquote @DevDocs https://twitter.com/devdocs/status/356095192085962752 %}NEW: DevDocs now comes with syntax highlighting. http://devdocs.io{% endblockquote %}</code></pre><blockquote><p>NEW: DevDocs now comes with syntax highlighting. <a href="http://devdocs.io/">http://devdocs.io</a></p><footer><strong>@DevDocs</strong><cite><a href="https://twitter.com/devdocs/status/356095192085962752">twitter.com/devdocs/status/356095192085962752</a></cite></footer></blockquote><h3 id="Blockquote-for-weblink"><a href="#Blockquote-for-weblink" class="headerlink" title="Blockquote for weblink"></a>Blockquote for weblink</h3><pre><code class="swig">{% blockquote Seth Godin http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html Welcome to Island Marketing %}Every interaction is both precious and an opportunity to delight.{% endblockquote %}</code></pre><blockquote><p>Every interaction is both precious and an opportunity to delight.</p><footer><strong>Seth Godin</strong><cite><a href="http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html">Welcome to Island Marketing</a></cite></footer></blockquote><h3 id="Pull-Quotes"><a href="#Pull-Quotes" class="headerlink" title="Pull Quotes"></a>Pull Quotes</h3><pre><code class="swig">{% pullquote %}content{% endpullquote %}</code></pre><blockquote class="pullquote"><p>content</p></blockquote><h3 id="jsFiddle"><a href="#jsFiddle" class="headerlink" title="jsFiddle"></a>jsFiddle</h3><pre><code class="swig">{% jsfiddle o2gxgz9r default light %}</code></pre><iframe scrolling="no" width="100%" height="300" src="https://jsfiddle.net/o2gxgz9r/embedded/js,resources,html,css,result/light" frameborder="0" loading="lazy" allowfullscreen=""></iframe><h3 id="Gist"><a href="#Gist" class="headerlink" title="Gist"></a>Gist</h3><pre><code class="swig">{% gist b6365e79be6052e7531e7ba6ea8caf23 'Sample gist' %}</code></pre><script src="//gist.github.com/b6365e79be6052e7531e7ba6ea8caf23.js?file=Sample gist"></script><h3 id="iFrame"><a href="#iFrame" class="headerlink" title="iFrame"></a>iFrame</h3><pre><code class="swig">{% iframe https://www.bing.com %}</code></pre><iframe src="https://www.bing.com/" width="100%" height="300" frameborder="0" loading="lazy" allowfullscreen=""></iframe><h3 id="Link-to-open-in-new-tab"><a href="#Link-to-open-in-new-tab" class="headerlink" title="Link to open in new tab"></a>Link to open in new tab</h3><pre><code class="swig">{% link Google https://www.google.com default Google %}</code></pre><a href="https://www.google.com/" title="default Google" target="">Google</a><h3 id="Youtube"><a href="#Youtube" class="headerlink" title="Youtube"></a>Youtube</h3><pre><code class="swig">{% youtube l_lblj8Cq0o %}</code></pre><div class="video-container"><iframe src="https://www.youtube.com/embed/l_lblj8Cq0o" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/lake.png&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Installation&quot;&gt;&lt;a href=&quot;#Installation&quot; class=&quot;headerlink&quot; title=&quot;Installation&quot;&gt;&lt;/a&gt;I</summary>
      
    
    
    
    <category term="notes" scheme="http://example.com/categories/notes/"/>
    
    
    <category term="typography" scheme="http://example.com/tags/typography/"/>
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2021/10/14/first-post/"/>
    <id>http://example.com/2021/10/14/first-post/</id>
    <published>2021-10-14T07:34:05.000Z</published>
    <updated>2021-10-31T14:37:58.745Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/helloworld.jpg" alt="$cover"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/helloworld.jpg&quot; alt=&quot;$cover&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Genesis" scheme="http://example.com/categories/Genesis/"/>
    
    
    <category term="Hello" scheme="http://example.com/tags/Hello/"/>
    
  </entry>
  
</feed>
