<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Xavier's blog</title><meta name="description" content="while(true) me.Learn();"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/tom.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Xavier's blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Xavier's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Archives · 2021</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/resnet/cover.jpg" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/22/resnet/"><img class="post-cover-img js-img-fadeIn" src="/images/resnet/cover.jpg" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/papers"><i class="tag post-item-tag">papers</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a><a href="/tags/Residual-learning"><i class="tag post-item-tag">Residual-learning</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/22/resnet/">ResNet</a></h2><time class="has-text-grey" datetime="2021-11-22T08:36:44.959Z">2021-11-22</time><p class="is-flex-grow-2 mt-2">
Residual Network深层的神经网络通常很难进行训练，本文使用了一个残差学习网络结构来训练比以往的神经网络要深得多的模型。残差网络容易训练，并且在深层神经网络中表现出来较好的准确率。
在未使用残差网络的模型中，当网络层数变多时，训练误差以及测试误差均会升高。
Is learning better networks as easy as stacking more layers ?
当网络变得特别深时，会出现梯度爆炸或梯度消失
传统的解决方法是：参数在初始化时要做的好一点，不要太大也不要太小；加入一些Batch Normalization Layers
传统的解决方法使得神经网络能够收敛，但是网络的精度却变得更差，而这并非是模型变得复杂后导致的过拟合问题，因为模型的训练误差也变高了
正常来说，如果..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/22/resnet/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/gan/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/22/Gan0/"><img class="post-cover-img js-img-fadeIn" src="/images/gan/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/papers"><i class="tag post-item-tag">papers</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a><a href="/tags/Generative-Model"><i class="tag post-item-tag">Generative-Model</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/22/Gan0/">GAN #1</a></h2><time class="has-text-grey" datetime="2021-11-22T08:36:36.426Z">2021-11-22</time><p class="is-flex-grow-2 mt-2">
Generative Adversarial NetsGan利用对抗的方法，提出了生成模型的新框架。它需要同时训练两个模型（生成模型G和判别模型D），G用于捕获数据的分布，而D需要判别出一个样本是来自训练集还是生成模型。生成模型的目标是尽最大可能让判别模型犯错（无法成功判别数据的来源），G和D在本文中被定义为多层感知机，整个系统通过反向传播来进行训练。
AN analogy to GAN在生成对抗网络框架中，生成模型与判别互相对抗。可以把生成模型类比为造假币者，判别模型类比为警察。生成模型试图制造假币骗过判别模型，而判别模型努力区分假币。二者在这样的对抗中不断学习提升各自的水平，直到生成模型的造的假币和真的一模一样，判别器无法区分。另外，警察进步不能过大或过小。进步过大时，造假者直接被一锅端，无法继续造假..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/22/Gan0/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/CSAsynchronousProgramming/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/22/CSAsync3/"><img class="post-cover-img js-img-fadeIn" src="/images/CSAsynchronousProgramming/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Asynchronous-programming"><i class="tag post-item-tag">Asynchronous-programming</i></a><a href="/tags/CS-Threading"><i class="tag post-item-tag">CS-Threading</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/22/CSAsync3/">C# Asynchronous programming #3</a></h2><time class="has-text-grey" datetime="2021-11-22T08:36:21.830Z">2021-11-22</time><p class="is-flex-grow-2 mt-2">
C# Asynchronous programming #3ThreadUse Thread in Applications
在带界面的WPF、UWP、WinForm等程序种，若主线程执行耗时的操作，就会导致整个程序无响应。因为主线程同时还要处理消息循环，而渲染和鼠标键盘事件处理等工作都是在消息循环中执行的
针对这种耗时的操作，一种流行的做法使启用一个worker线程，执行完操作后再更新到UI
富客户端应用的线程模型通常是：
UI控件只能从创建他们的线程来进行访问（通常是主线程）
当想从worker线程更新UI时，应该把请求交给UI线程



Synchronization Contexts
在System.ComponentModel下有一个抽象类：SynchronizationContext，它使得T..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/22/CSAsync3/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/CSAsynchronousProgramming/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/22/CSAsync2/"><img class="post-cover-img js-img-fadeIn" src="/images/CSAsynchronousProgramming/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Asynchronous-programming"><i class="tag post-item-tag">Asynchronous-programming</i></a><a href="/tags/CS-Threading"><i class="tag post-item-tag">CS-Threading</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/22/CSAsync2/">C# Asynchronous programming #2</a></h2><time class="has-text-grey" datetime="2021-11-22T08:36:10.315Z">2021-11-22</time><p class="is-flex-grow-2 mt-2">
C# Asynchronous programming #2ThreadThread Safety
本地状态与共享状态
Local 本地独立：CLR为每个线程分配自己的内存栈，以便使本地变量保持独立
Shared 共享：如果多个线程引用到了同一个对象实例，那么他们就共享了数据；被Lambad表达式或匿名委托捕获的本地变量，会被编译器转化为字段（field），所以也会被共享；静态字段也会在线程间共享



class Program
{
    
    static void Main(string[] args)
    {
        /*本地变量不共享*/
        //打印六次hello...
        Thread thread = new Thread(GO);
       ..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/22/CSAsync2/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/CSAsynchronousProgramming/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/22/CSAsync1/"><img class="post-cover-img js-img-fadeIn" src="/images/CSAsynchronousProgramming/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Asynchronous-programming"><i class="tag post-item-tag">Asynchronous-programming</i></a><a href="/tags/CS-Threading"><i class="tag post-item-tag">CS-Threading</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/22/CSAsync1/">C# Asynchronous programming #1</a></h2><time class="has-text-grey" datetime="2021-11-22T08:35:34.837Z">2021-11-22</time><p class="is-flex-grow-2 mt-2">
C# Asynchronous programming #1Thread
What is Thread ?
线程是一个可执行路径，每一个线程可以独立于其他线程执行
每个线程在均进程(Process)内执行，在操作系统中，进行提供了程序运行的独立环境
单线程应用，在进程的独立环境中只跑一个线程，该线程具有独占权
多线程应用，单个进程中跑多个线程，多个线程共享当前的执行环境（尤其是内存）
共享：多个线程共同占有某种资源，如一个线程在后台读取数据，另一个线程在数据到达后进行展示。  



下面是c#中最简单的异步编程实例：
class Program
{
    static void Main(string[] args)
    {
        Thread.CurrentThread.Name = ..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/22/CSAsync1/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/d2l/1/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/22/d2l_1/"><img class="post-cover-img js-img-fadeIn" src="/images/d2l/1/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/22/d2l_1/">D2L: Matrix</a></h2><time class="has-text-grey" datetime="2021-11-22T08:34:36.985Z">2021-11-22</time><p class="is-flex-grow-2 mt-2">
MatrixBasic knowledge
标量
x=torch.tensor([1.0])
y=torch.tensor([2.0])

print(x+y,x*y,x/y,x**y)


向量
x=torch.arange(4)
print(x,x[3],len(x),x.shape,x.size())


矩阵
x=torch.eye(20)
x=torch.arange(20).view(5,4)
print(x,x.T,x.t())
print(torch.arange(24).reshape(2,3,4))


运算
A=torch.arange(20,dtype=torch.float32).view(5,4)
B=A.clone() #创建A的副本给B，A与B指向不同地址
C=B  #B与..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/22/d2l_1/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><a class="extend prev" rel="prev" href="/archives/2021/11/"><i class="iconfont icon-prev has-text-grey"></i></a><a class="page-number" href="/archives/2021/11/">1</a><span class="page-number current">2</span></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container archives-widget is-in-archive-page"><h3>Archives</h3><section><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a><span class="archive-list-count">2</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/xiaoyu2018"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Xavier 2021</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>