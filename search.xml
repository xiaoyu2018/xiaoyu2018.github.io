<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>You Only Look Once #1</title>
      <link href="/2022/03/02/YOLOv1/"/>
      <url>/2022/03/02/YOLOv1/</url>
      
        <content type="html"><![CDATA[<h1 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection"></a>You Only Look Once: Unified, Real-Time Object Detection</h1><h2 id="Prime-Algorithm"><a href="#Prime-Algorithm" class="headerlink" title="Prime Algorithm"></a><strong>Prime Algorithm</strong></h2><ul><li><p>概述<br><img src="/images/yolo/2.png"></p><ul><li><p>YOLO将输入图片划分为SxS个网格。每个网格预测B个边缘框，表示为(x,y,h,w,c)，分别代表中心点坐标、边缘框高宽和边缘框置信度(图片中越粗的线c越大)；同时，每个网格还要预测一组类别概率（softmax分类概率）来确定目标的类别</p></li><li><p>上述过程训练阶段和测试阶段具体方法有所不同，下文将两个阶段分别讨论</p></li></ul></li><li><p>预测阶段  </p><ul><li><p>在预测阶段，网络直接接受3x448x448的图片作为输入。图片经24层卷积提取特征，再经两个全连接层回归得到7x7x30（S=7，B=2，类别数=30-Bx5=20）的张量作为输出，即输出49个网格生成的98个边缘框的信息及每个网格所属20个类别分别的概率（<em>全部的信息都是由神经网络预测得出的</em>）。这也表明了YOLOv1最多只能检测SxS个目标，且难以检测小而密集的目标<br><img src="/images/yolo/1.png"></p></li><li><p>在获取YOLO提供的预测信息后，需要进行一系列的后处理（置信度过滤和非极大值抑制）得到目标检测结果。下图为SxS个网格生成的BxSxS个边缘框，颜色代表了其预测类别（一个网格生成的所有边缘框同属一个类别），粗细代表了其置信度，经后处理得到最终结果<br><img src="/images/yolo/3.png"></p></li><li><p>计算全概率：将每个边缘框的置信度与其所属网格的类别概率相乘得到BxSxS个类别数维的全概率向量</p></li><li><p>NMS：对所有类别分别做如下操作，将最大全概率的边缘框与其他所有非零的边缘框计算IoU，如果超过了threshold（如0.5），则认为他们重复预测了同一目标，将概率小的边缘框在这一类别的概率置零，再继续将第二高全概率的边缘框与其后（降序）所有其他边缘框比较并重复上述过程。如此反复，直到找不到新的非零概率边缘框。有二十个类别的情况下会做二十次的NMS<br><img src="/images/yolo/6.png"></p></li><li><p>完整后处理过程：将全概率向量中数值小于threshold的直接置零，然后按照某一类别将向量降序排列，并用NMS非极大值抑制删除多余边缘框，最后对每一个全概率向量（对应每一个边缘框）找出它最大概率的类别（非零），画出该边缘框并表明这一类别。如果向量此时为零向量，则代表该边缘框不包含任何目标，不画出该边缘框<br><img src="/images/yolo/7.png"></p></li></ul></li><li><p>训练阶段</p><ul><li><p>在训练阶段，训练集提供了已经标注好的ground truth，我们要让YOLO的输出去拟合ground truth。</p></li><li><p>真实边缘框的中心点落在哪一个网格内，就应当由哪一个网格负责去预测出一个边缘框来拟合该真实边缘框，并且此网格的类别也被决定。每一个网格将预测B个边缘框，选择与真实边缘框IoU大的去拟合，小的边缘框置信度越小越好<br><img src="/images/yolo/8.png"></p></li><li><p>对于没有真实边缘框中心点落入的网格，其预测边缘框的置信度越小越好</p></li><li><p>在训练时，也是整张图片作为输入，预测的中心点坐标值是0-1之间的数，表明了在某一个网格的坐标</p></li><li><p>损失函数如下（lambda是权重，因为不包含目标的边缘框一般远多于包含的边缘框）<br><img src="/images/yolo/9.png"></p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers </tag>
            
            <tag> Neural Network </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Object Detection</title>
      <link href="/2022/03/01/d2l_20/"/>
      <url>/2022/03/01/d2l_20/</url>
      
        <content type="html"><![CDATA[<h1 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><p>目标检测任务与图像分类不同，目标检测要在图片中识别出多个不同的目标并表明每个目标的位置</p><ul><li><p>边缘框</p><ul><li>边缘框表示了目标的位置，有两种坐标表示法<ul><li>(左上x，左上y，右下x，右下y)</li><li>(左上x，左上y，边框宽，边框高)</li></ul></li></ul></li><li><p>锚框</p><ul><li>锚框在目标检测算法中很常用，许多目标检测算法都用到了这项技术</li><li>锚框是对边缘框的一个猜测：<ul><li>提出多个被称为锚框的区域</li><li>预测每个锚框中是否含有目标物体</li><li>如果有，则继续预测从这个锚框到真实边缘框的偏移</li></ul></li><li>使用交并比（IoU）来计算两个框的相似度(预测框和标签框)<br><img src="/images/d2l/20/1.png"></li><li>赋予锚框标号<ul><li>在训练时，每个锚框都是一个训练样本，每次读取一张图片都要进行一次赋予锚框标号的操作</li><li>将每个锚框要么标注为背景，要么标注为与真实边缘框相关</li><li>可能生成大量锚框导致负样本过多</li><li>锚框可固定生成，或根据图片生成，甚至随机</li><li>假设一个图片有4个标签边缘框，生成了9个锚框，一种赋予标号的算法如下图<br><img src="/images/d2l/20/2.png"></li></ul></li><li>在预测时，使用非极大值抑制（NMS）输出<br><img src="/images/d2l/20/3.png"></li></ul></li><li><p>目标检测常用算法</p><ol><li><p>Faster R-CNN<br>图片进入一个CNN后分成两条路线，一条路线进入RPN（负责生成锚框），另一条路线经过RoI pooling（将不同大小的锚框提取为同一大小）后连接至全连接层，最后做出分类以及边缘框预测。Faster R-CNN相对其他算法来说还是很慢，但精度很高，适合刷榜。<br><img src="/images/d2l/20/4.png"></p></li><li><p>SSD（单发多框检测）<br>SSD由一个基础的网络来抽取特征，然后多个卷积层块来减半高宽。每段都会生成锚框，底部段拟合小物体，顶部段拟合大物体。每个锚框都会预测类别和边缘框<br><img src="/images/d2l/20/5.png"></p></li><li><p>YOLO<br>SSD中锚框有大量重叠，YOLO将图片均匀分成SxS个锚框，每个锚框预测B个边缘框。</p></li></ol></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li><p>边缘框实现</p>  <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>img=Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">"./1.jpg"</span>)<br>plt.imshow(img)<br>plt.show()<br>fig=plt.imshow(img)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">box_corner_to_center</span>(<span class="hljs-params">boxes</span>):</span><br>    <span class="hljs-string">"""(左上，右下)转换到(中间，宽度，高度)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        boxs : 一组边缘框 </span><br><span class="hljs-string">    """</span><br>    x1, y1, x2, y2 = boxes[:, <span class="hljs-number">0</span>], boxes[:, <span class="hljs-number">1</span>], boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">3</span>]<br>    cx = (x1 + x2) / <span class="hljs-number">2</span><br>    cy = (y1 + y2) / <span class="hljs-number">2</span><br>    w = x2 - x1<br>    h = y2 - y1<br>    <span class="hljs-comment"># 将新坐标堆叠起来变为二维Tensor，注意与cat不同</span><br>    boxes = torch.stack((cx, cy, w, h))<br>    <span class="hljs-comment"># 返回转置</span><br>    <span class="hljs-keyword">return</span> boxes.T<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">box_center_to_corner</span>(<span class="hljs-params">boxes</span>):</span><br>    <span class="hljs-string">"""(中间，宽度，高度)转换到(左上，右下)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        boxs : 一组边缘框 </span><br><span class="hljs-string">    """</span><br>    cx, cy, w, h = boxes[:, <span class="hljs-number">0</span>], boxes[:, <span class="hljs-number">1</span>], boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">3</span>]<br>    x1 = cx - <span class="hljs-number">0.5</span> * w<br>    y1 = cy - <span class="hljs-number">0.5</span> * h<br>    x2 = cx + <span class="hljs-number">0.5</span> * w<br>    y2 = cy + <span class="hljs-number">0.5</span> * h<br>    boxes = torch.stack((x1, y1, x2, y2))<br>    <span class="hljs-keyword">return</span> boxes.T<br><br>x=torch.tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]])<br>x=box_corner_to_center(x)<br><span class="hljs-built_in">print</span>(x)<br>x=box_center_to_corner(x)<br><span class="hljs-built_in">print</span>(x)<br><br><br>box1=torch.tensor([<span class="hljs-number">360</span>,<span class="hljs-number">170</span>,<span class="hljs-number">650</span>,<span class="hljs-number">650</span>])<br>box2=torch.tensor([<span class="hljs-number">120</span>,<span class="hljs-number">210</span>,<span class="hljs-number">350</span>,<span class="hljs-number">290</span>])<br><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_bboxes</span>(<span class="hljs-params">fig,bboxes,color</span>):</span><br>    <span class="hljs-string">"""显示带边缘框的图片</span><br><span class="hljs-string"></span><br><span class="hljs-string">    """</span> <br>    <span class="hljs-keyword">for</span> b,c <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(bboxes,color):<br>        fig.axes.add_patch(_bbox_to_rect(b, c))<br>    plt.show()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_bbox_to_rect</span>(<span class="hljs-params">bbox, color</span>):</span><br>        <span class="hljs-keyword">return</span> plt.Rectangle(xy=(bbox[<span class="hljs-number">0</span>], bbox[<span class="hljs-number">1</span>]), width=bbox[<span class="hljs-number">2</span>] - bbox[<span class="hljs-number">0</span>],<br>                                height=bbox[<span class="hljs-number">3</span>] - bbox[<span class="hljs-number">1</span>], fill=<span class="hljs-literal">False</span>,<br>                                edgecolor=color, linewidth=<span class="hljs-number">2</span>)<br><br><br><br>show_bboxes(fig,[box1,box2],[<span class="hljs-string">'red'</span>,<span class="hljs-string">'green'</span>])<br><br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Fine Tune</title>
      <link href="/2022/02/27/d2l_19/"/>
      <url>/2022/02/27/d2l_19/</url>
      
        <content type="html"><![CDATA[<h1 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine Tune"></a>Fine Tune</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>一般神经网络的架构都分为两块：</p><ul><li>特征抽取部分</li><li>线性分类部分<br><img src="/images/d2l/19/1.png"></li></ul></li><li><p>微调  </p><ul><li><p>一个模型特征提取部分可以作为上游模型训练其他任务<br><img src="/images/d2l/19/2.png"></p></li><li><p>pre-train的过程一般是一个大数据集上的正常训练任务，而在fine-tune的过程中使用更强的正则化、更小的学习率、更少的epoch</p></li><li><p>原数据集和目标数据集要相似，且原数据集比目标数据集要大</p></li></ul></li><li><p>微调的一些技巧</p><ul><li>重用分类器权重：原数据集中也可能有下游任务数据的部分标号，可以用预训练模型分类器中对应标号的对应向量来初始化下游任务分类部分</li><li>固定前面的层：神经网络中靠近输入的层更加通用，可以固定底部的一些层的参数不参与更新</li><li>对于个人或小企业来说，通常不会从头开始训练模型，而是进行微调</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models,transforms<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> d2l<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">64</span>)<br><br>train_aug=transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    transforms.RandomHorizontalFlip(),<br>    <span class="hljs-comment"># transforms.ToTensor(), #在load时已经将PIL转化为了Tensor</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>,<span class="hljs-number">0.456</span>,<span class="hljs-number">0.406</span>],[<span class="hljs-number">0.229</span>,<span class="hljs-number">0.224</span>,<span class="hljs-number">0.225</span>]),<br>    <br>])<br><br>test_aug=transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)),<br>    <span class="hljs-comment"># transforms.ToTensor(),</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>,<span class="hljs-number">0.456</span>,<span class="hljs-number">0.406</span>],[<span class="hljs-number">0.229</span>,<span class="hljs-number">0.224</span>,<span class="hljs-number">0.225</span>])<br>])<br><br><br><span class="hljs-comment"># 指定预训练模型</span><br>finetune_res=models.resnet18(pretrained=<span class="hljs-literal">True</span>,progress=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 将分类器部分重新设计，in_features属性记录了原模型的本层的输入特征数</span><br>finetune_res.fc=nn.Linear(finetune_res.fc.in_features,<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 初始化新分类器参数</span><br>nn.init.xavier_uniform_(finetune_res.fc.weight)<br><br>loss_f=nn.CrossEntropyLoss()<br><span class="hljs-comment"># 增加正则化并且学习率应该设得很小</span><br>opt=optim.Adam(finetune_res.parameters(),weight_decay=<span class="hljs-number">0.1</span>,lr=<span class="hljs-number">5e-5</span>)<br><br><span class="hljs-comment"># 不需要迭代很多轮</span><br>d2l.train(<br>    <span class="hljs-number">5</span>,loss_f,opt,finetune_res,train_iter,<br>    save_name=<span class="hljs-string">"res18_pretrained"</span>,device=torch.device(<span class="hljs-string">"cuda:0"</span>),<br>    aug=train_aug<br>    )<br><br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Data Augmentation</title>
      <link href="/2022/02/26/d2l_18/"/>
      <url>/2022/02/26/d2l_18/</url>
      
        <content type="html"><![CDATA[<h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><p>我们收集的训练数据通常很难覆盖到未来可能部署的全部场景（比如人脸识别的应用可能会部署到不同摄像头状况、天气、时间等的场景）。数据增强则在一个已有的数据上做数据变换，起到增大数据集的作用，使其有更好的多样性。</p><ul><li><p>数据增强</p><ul><li>数据增强只在训练时进行</li><li>一般采用在线生成的方式</li><li>数据增强假设测试环境中会出现增强后的数据，如果测试环境和训练集高度一致则没有必要做数据增强</li></ul></li><li><p>数据增强方法</p><ul><li>翻转（上下、左右翻转）</li><li>切割（随机高宽比、大小、位置切割一块，然后再变为固定大小）</li><li>颜色（色调、饱和度、亮度）</li><li>其他（高斯模糊、锐化、遮挡等）</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pytorch在提供transforms模块中提供了很多数据增广函数</span><br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_images</span>(<span class="hljs-params">imgs, num_rows, num_cols, titles=<span class="hljs-literal">None</span>, scale=<span class="hljs-number">1.5</span></span>):</span>  <br>    <br>    figsize = (num_cols * scale, num_rows * scale)<br>    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)<br>    axes = axes.flatten()<br>    <span class="hljs-keyword">for</span> i, (ax, img) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(axes, imgs)):<br>        <span class="hljs-keyword">if</span> torch.is_tensor(img):<br>            ax.imshow(transforms.ToPILImage()(img))<br>        <span class="hljs-keyword">else</span>:<br>            ax.imshow(img)<br>        ax.axes.get_xaxis().set_visible(<span class="hljs-literal">False</span>)<br>        ax.axes.get_yaxis().set_visible(<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">if</span> titles:<br>            ax.set_title(titles[i])<br>    plt.show()<br>    <span class="hljs-keyword">return</span> axes<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">4</span>,(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br>X,y=<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(test_iter))<br><br><span class="hljs-comment"># 显示tensor形式的图片</span><br><span class="hljs-comment"># X=X.view(4,224,224)</span><br><br>show_images(X,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,scale=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 数据增广</span><br><span class="hljs-comment"># trans=transforms.RandomCrop((20,20))</span><br><span class="hljs-comment"># trans=transforms.RandomHorizontalFlip()</span><br><span class="hljs-comment"># trans=transforms.GaussianBlur(5)</span><br>trans=transforms.RandomErasing(<span class="hljs-number">1</span>)<br>show_images(trans(X),<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,scale=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 显示使用PIL读进内存的图片</span><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br>test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        download=<span class="hljs-literal">True</span>,transform=transforms.ToTensor()<br>    )<br><br><span class="hljs-built_in">print</span>(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape)<br><span class="hljs-comment"># 将Tensor形式转换为PIL形式</span><br>image=transforms.ToPILImage()(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>image.show()<br><span class="hljs-keyword">import</span> torchvision<br>test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        download=<span class="hljs-literal">True</span><br>    )<br><span class="hljs-comment"># 第i张图片test_data[i][0]，test_data[i][1]是第一张图片的标签</span><br>d2l.show_images([test_data[i][<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>)], <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, scale=<span class="hljs-number">0.8</span>)<br><br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">r"./1.jpg"</span>)<br><span class="hljs-built_in">print</span>(image)<br><span class="hljs-comment"># 将PIL形式转换为Tensor形式</span><br><span class="hljs-built_in">print</span>(transforms.ToTensor()(image))<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: ResNet</title>
      <link href="/2022/02/26/d2l_17/"/>
      <url>/2022/02/26/d2l_17/</url>
      
        <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>加更多的层不一定总是改进精度</p><ul><li>新的层可能是使模型收敛范围偏差到一个不符合预期的区域</li><li>ResNet使各层更容易学会恒等变换，从而更容易使模型收敛范围达到Nested function classes<br><img src="/images/d2l/17/1.png"></li></ul></li><li><p>残差块</p><ul><li>基本的ResBlock结构如下，f(x)+x保证了包含原收敛范围<br><br><img src="/images/d2l/17/2.png"></li><li>具体使用时，ResBlock的设计细节<br><br><img src="/images/d2l/17/4.png"></li></ul></li><li><p>ResNet架构<br>一般来说现在的主流设计架构就是接入一个Stage（7x7Conv-3x3MP），之后再连接具体想要的网络架构，ResNet架构如下也是这种设计思想，具体架构如下<br>  <img src="/images/d2l/17/5.png"></p></li></ul><ul><li>Tricks<ul><li>实际应用中，Res34用的最多，达不到要求可以继续用Res50</li><li>Res152、Res101一般用来刷榜</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Residual</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,in_channels,out_channels,</span></span><br><span class="hljs-params"><span class="hljs-function">        use_1x1conv=<span class="hljs-literal">False</span>,stride=<span class="hljs-number">1</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1=nn.Conv2d(in_channels,out_channels,<span class="hljs-number">3</span>,stride,<span class="hljs-number">1</span>)<br>        self.conv2=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.bn1=nn.BatchNorm2d(out_channels)<br>        self.bn2=nn.BatchNorm2d(out_channels)<br>        <span class="hljs-comment"># inplace更省内存(显存)</span><br>        self.relu=nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.conv3=<span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span>(use_1x1conv):<br>            self.conv3=nn.Conv2d(in_channels,out_channels,<span class="hljs-number">1</span>,stride)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        Y=self.relu(self.bn1(self.conv1(X)))<br>        Y=self.bn2(self.conv2(Y))<br><br>        <span class="hljs-keyword">if</span>(self.conv3):<br>            X=self.conv3(X)<br>        <span class="hljs-keyword">return</span> self.relu(Y+X)<br><br>s1=nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),nn.BatchNorm2d(<span class="hljs-number">64</span>),nn.ReLU(),nn.MaxPool2d(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet_block</span>(<span class="hljs-params">input_channels, num_channels, num_residuals,</span></span><br><span class="hljs-params"><span class="hljs-function">                 first_block=<span class="hljs-literal">False</span></span>):</span><br>    blk = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>            blk.append(<br>                Residual(input_channels, num_channels, use_1x1conv=<span class="hljs-literal">True</span>,<br>                         stride=<span class="hljs-number">2</span>))<br>        <span class="hljs-keyword">else</span>:<br>            blk.append(Residual(num_channels, num_channels))<br>    <span class="hljs-keyword">return</span> blk<br><br>s2=nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">2</span>,<span class="hljs-literal">True</span>))<br>s3=nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>,<span class="hljs-number">2</span>))<br>s4=nn.Sequential(*resnet_block(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">2</span>))<br>s5=nn.Sequential(*resnet_block(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">2</span>))<br><br>device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>res_net=nn.Sequential(<br>    s1,s2,s3,s4,s5,<br>    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>)<br><br>x=torch.rand((<span class="hljs-number">20</span>,<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><span class="hljs-built_in">print</span>(res_net(x).shape)<br><br>opt=optim.Adam(res_net.parameters())<br>train_iter,val_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,nn.CrossEntropyLoss(),opt,</span><br><span class="hljs-comment">#     res_net,train_iter,save_name="res_net"</span><br><span class="hljs-comment">#     )</span><br><br>d2l.evaluate(res_net,val_iter,nn.CrossEntropyLoss(),<span class="hljs-string">"./params/res_net_2"</span>,device=torch.device(<span class="hljs-string">"cuda:0"</span>))<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vison in Transformer</title>
      <link href="/2022/02/21/VIT/"/>
      <url>/2022/02/21/VIT/</url>
      
        <content type="html"><![CDATA[<h1 id="An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><a href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale" class="headerlink" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"></a>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h1><p>从2012年AlexNet提出以来，卷积神经网络在计算机视觉领域一直占据统治地位。而本篇论文的研究表明，拥有足够多数据进行预训练的情况下，Transformer网络架构也可以把计算机视觉问题解决的很好。更进一步来说，这篇论文的提出打破了cv和nlp之间的壁垒，在多模态领域也产生了很大影响</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a><strong>Intro</strong></h2><ul><li>将每一张图片视作由许多16x16的patch组成，每个patch当作nlp领域中的单词，一张图片便可视作一个sequence</li><li>CV领域不需要局限于CNNs的结构，纯Transformer在预训练集足够大时在图像分类任务达到了目前CNNs的SOTA，并且需要更少的计算资源</li><li>使用Transformer架构，到目前为止还未出现增加数据和模型复杂度导致性能饱和的现象</li><li>CNNs具有人为规定的两个先验信息（局部性、平移不变性），而Transformer是缺少这样的信息的，所以在数据集不够的时候时VIT比CNNs的SOTA要差一点（因为VIT需要自己去学习两个先验信息），进一步扩大数据集后，效果达到SOTA</li></ul><h2 id="Problems-and-Purposes"><a href="#Problems-and-Purposes" class="headerlink" title="Problems and Purposes"></a><strong>Problems and Purposes</strong></h2><ul><li>受Transformer在nlp领域可扩展性的成功，本文想尽量少地修改Transformer架构，将这种可扩展性带到CV领域</li><li>如何把2d的图片变成1d的序列</li><li>输入transformer的长一般为512、1024这个量级，要考虑计算性能，序列过长复杂度无法接受</li><li>本文的解决方案：将原图划分为多个16x16地patch，再将patch组成sequence，大大减小了序列长度</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a><strong>Method</strong></h2><p><img src="/images/VIT/cover.jpg" alt="$cover"><br>将原始图片划分出多个patches，patches经过线性投射层展平为一维向量，为每个向量加入位置编码后输入Transformer Encoder。另外引入了一个额外的token（第0位向量），并以该对应的Transformer Encoder输出作为分类的依据</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers </tag>
            
            <tag> Neural Network </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Batch Normalization</title>
      <link href="/2022/02/21/d2l_16/"/>
      <url>/2022/02/21/d2l_16/</url>
      
        <content type="html"><![CDATA[<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>问题<ul><li>损失出现在最后，由BP算法和梯度消失，后面的层训练的会更快</li><li>数据在最前面，前面的层训练的慢且前面的层变化后面的层也要跟着变（抽取的底层信息变化让后面的层要重新学），所以后面的层要重新学习很多次，导致收敛变慢</li><li>考虑在学习底部层时避免变化顶部层</li></ul></li><li>批量归一化<ul><li>固定小批量里面的均值和方差，然后再做额外的调整（可学习的参数gama和beta）<br><img src="/images/d2l/16/1.png"></li><li>是线性变换</li><li>作用在<ul><li>全连接层和卷积层输出后，激活函数前</li><li>全连接层和卷积层输入前</li></ul></li><li>对于全连接层作用于特征维</li><li>对于卷积层作用于通道维（将每一个像素都当作一个样本，通道数就是一个样本的特征数）</li></ul></li><li>批量归一化在做什么？<ul><li>最初的论文是想用它来减少内部协变量转移（使每一层的输出分布变化不那么剧烈）</li><li>后续有论文指出，批量归一化可能只是在小批量中加入噪声控制模型复杂度</li></ul></li><li>总结<ul><li>批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放</li><li>批量归一化可以加速收敛（可以设置更大的学习率），一般不改变模型精度</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch<br><br>net=nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),nn.BatchNorm2d(<span class="hljs-number">6</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),nn.BatchNorm2d(<span class="hljs-number">16</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.BatchNorm1d(<span class="hljs-number">120</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.BatchNorm1d(<span class="hljs-number">84</span>),<br>    nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters(),lr=<span class="hljs-number">1.0</span>)<br><br><span class="hljs-keyword">import</span> d2l<br><br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,nn.CrossEntropyLoss(),</span><br><span class="hljs-comment">#     optim.Adam(net.parameters()),</span><br><span class="hljs-comment">#     net,train_iter,save_name="LeNet_bn",</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"))</span><br>d2l.evaluate(<br>    net,test_iter,nn.CrossEntropyLoss(),<br>    param_path=<span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/LeNet_bn_10"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic SQL</title>
      <link href="/2022/02/21/sql/"/>
      <url>/2022/02/21/sql/</url>
      
        <content type="html"><![CDATA[<h1 id="Basic-SQL"><a href="#Basic-SQL" class="headerlink" title="Basic SQL"></a>Basic SQL</h1><p>数据库语言可分为两个部分，DDL和DML。</p><ul><li>DDL（Data Definitoin Language）用于描述数据库中要存储的现实世界实体，操作数据库的结构等</li><li>DML（Data Manipulation Language）用于数据库操作，操作数据库存储的对象或数据</li></ul><h2 id="SQL-DDL-数据定义"><a href="#SQL-DDL-数据定义" class="headerlink" title="SQL DDL:数据定义"></a><strong>SQL DDL:数据定义</strong></h2><h3 id="数据库相关"><a href="#数据库相关" class="headerlink" title="数据库相关"></a>数据库相关</h3><ul><li>查询所有数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">show</span> databases;<br></code></pre></td></tr></tbody></table></figure></li><li>创建新数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> database 数据库名称;<br></code></pre></td></tr></tbody></table></figure></li><li>删除数据库  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">drop</span> database 数据库名称;<br></code></pre></td></tr></tbody></table></figure><h3 id="表相关"><a href="#表相关" class="headerlink" title="表相关"></a>表相关</h3></li><li>查询当前数据库下所有表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">show</span> tables;<br></code></pre></td></tr></tbody></table></figure></li><li>创建新表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql">   <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> 表名称<br>   (<br>列名称<span class="hljs-number">1</span> 数据类型[(最大位数) 约束]，<br>列名称<span class="hljs-number">2</span> 数据类型[(最大位数) 约束]，<br>列名称<span class="hljs-number">3</span> 数据类型[(最大位数) 约束],<br>...    <br>   )<br></code></pre></td></tr></tbody></table></figure></li><li>删除表  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">drop</span> <span class="hljs-keyword">table</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改已有表的列  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 在表中添加列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名称<br><span class="hljs-keyword">add</span> 列名称 数据类型;<br><br># 删除表中的列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名称<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">column</span> 列名称;<br></code></pre></td></tr></tbody></table></figure><h2 id="SQL-DML-数据操作"><a href="#SQL-DML-数据操作" class="headerlink" title="SQL DML:数据操作"></a><strong>SQL DML:数据操作</strong></h2></li><li>从表中获取数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 获取表中某一列数据<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br># 或获取表中所有数据<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>修改表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 更新符合要求的行中某列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称 <span class="hljs-operator">=</span> 新值 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br># 更新符合要求的行中若干列的值<br>update 表名称 <span class="hljs-keyword">set</span> 列名称<span class="hljs-number">1</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-operator">=</span> 新值<span class="hljs-number">2</span> <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 某值;<br></code></pre></td></tr></tbody></table></figure></li><li>删除表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"># 删除表中的某行<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列名称 <span class="hljs-operator">=</span> 值;<br># 删除所有行（在不删除表的情况下删除所有的行。这意味着表的结构、属性和索引都是完整的）<br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名称;<br><span class="hljs-keyword">delete</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>向表中插入数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称 <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br># 指定要插入的列<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 表名称(列<span class="hljs-number">1</span>,列<span class="hljs-number">2</span>,...) <span class="hljs-keyword">values</span> (值<span class="hljs-number">1</span>,值<span class="hljs-number">2</span>,...);<br></code></pre></td></tr></tbody></table></figure></li><li>去重后从表中获取某列中值  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> 列名称 <span class="hljs-keyword">from</span> 表名称;<br></code></pre></td></tr></tbody></table></figure></li><li>根据条件获取表中数据  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">and</span> 列 运算符 值;<br><span class="hljs-keyword">select</span> 列名称 <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">where</span> 列 运算符 值 <span class="hljs-keyword">or</span> 列 运算符 值;<br></code></pre></td></tr></tbody></table></figure></li><li>根据指定的列对结果集进行排序  <figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br># 先按列名称<span class="hljs-number">1</span>排序，然后以列名称<span class="hljs-number">3</span>排序<br><span class="hljs-keyword">select</span> 列名称<span class="hljs-number">1</span>,列名称<span class="hljs-number">2</span>,列名称<span class="hljs-number">3</span> <span class="hljs-keyword">from</span> 表名称 <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 列名称<span class="hljs-number">1</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)],列名称<span class="hljs-number">3</span> [<span class="hljs-keyword">asc</span>(<span class="hljs-keyword">desc</span>)];<br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer</title>
      <link href="/2022/01/29/Transformer/"/>
      <url>/2022/01/29/Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="Attention-Is-All-Your-Need"><a href="#Attention-Is-All-Your-Need" class="headerlink" title="Attention Is All Your Need"></a>Attention Is All Your Need</h1><p>以往主流的序列转录(seq2seq)模型中常常基于包含Encoder与Decoder的复杂RNN或CNN，这些模型也会在Encoder与Decoder中使用Attention机制。<br>Transformer仅仅使用了Attention机制，完全没用到循环和卷积，将循环层换为了Multi-headed Attetion。Transformer训练速度更快，预测能力更好。  </p><hr><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>RNN难以并行计算计算效率低，Transformer可并行。</li><li>RNN带有时序信息，但在序列较长时，早期的信息可能在后期丢失。Attention可通过在输入序列中加入index增加时序，并且不会存在信息丢失问题。</li><li>用CNN可以替换掉RNN实现并行计算，但由于感受野的限制其依然存在难以对长序列进行建模的问题。Transformer中的Attention机制一次性看到所有的序列，消除了这一问题。</li><li>CNN可利用多个输出通道识别不一样的模式，在Transformer中使用Multi-headed Attetion，也实现了这样的特性。</li></ul><hr><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Transformer整体分为Encoder与Decoder两大部分。  </p><ul><li>Input Embedding/Output Embedding将词映射到向量。</li><li>Postoinal Encoding</li><li>Nx指有N个该块叠在一起。</li><li>Add表示残差连接，Norm表示正则处理。</li><li>在训练时，解码器的输入（outputs）是真实值(Ground Truth)；在测试时，输入（outputs）是前一时刻的输出。<br><img src="/images/transformer/architecture.png" alt="architecture"></li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>N=6，out_dim=input_dim=512。由两个子层组成，两个子层分别为Multi-headed Attetion和简单的MLP。每个子层使用残差连接和layer normalization。</p><blockquote><p>layer normalization and batch normalization</p><blockquote><p>batch:对每一个feature，将这个feature在一个batch中的所有数据的均值变为0方差变为1<br><img src="/images/transformer/batch_norm.png" alt="batch_norm"><br>layer:对每一个样本，将这个样本所有的特征均值变为0方差变为1<br><img src="/images/transformer/layer_norm.png" alt="layer_norm"><br>成sequence的normalization:<br>蓝为batch，黄为layer，取所有数据去做norm。阴影部分为样本实际长度（即该样本序列的seq_len），在实际长度外取全0。<br><img src="/images/transformer/seq_norm.png" alt="seq_norm"><br>使用layernormalization相对稳定一些。每个样本自己做均值方差再去norm。</p></blockquote></blockquote><hr><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>N=6,由三个子层组成，其中后两个子层Encoder一致，而第一个子层有所不同，其使用了Masked Multi-headed Attetion。在Decoder中还使用了自回归。当前的outputs输入是上一次的输出。在训练过程中，Decoder的输入outputs为ground truth，但在t时刻的输入不应包含t时刻之后的输入，所以第一个子层引入了masked机制。</p><hr><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><blockquote><p>Transformer使用了Scaled Dot-Product Attention。其将输入的一个seq进行融合输出一个等长的seq。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>在融合时，每一个输出都考虑了其对应位置输入元素和seq中其他元素的相关度。Wq与Wk是两个参数矩阵，原始输入做矩阵运算后得到两个向量q、k，最后用内积运算即得到了原始输入的相关性（内积运算代表了余弦相似度）。<br><img src="/images/transformer/dot-product.png" alt="dot-product"><br>同理可计算出a1与自身及序列中所有元素的相关性，全部计算完成后输入到softmax层进行归一化。<br><img src="/images/transformer/to-softmax.png" alt="to-softmax"><br>利用一个新的参数矩阵Wv获得v1-v4。v与相关性系数的带权和得到b1。<br><img src="/images/transformer/b1.png" alt="b1"><br>同理，可以得到b2、b3、b4。<br><img src="/images/transformer/panaroma.png" alt="panaroma"><br>上述步骤可表示为矩阵运算。<br><img src="/images/transformer/use_matrix.png" alt="use_matrix"><br>最终的计算公式可如下表示。这里多了一个除以根号dk(input_dim)，这是因为向量(Transformer中dk=512)比较长时，内积绝对值可能会出现比较大的情况，这对梯度下降是不利的。<br><img src="/images/transformer/formula.png" alt="formula"> </p></blockquote><blockquote><p>Multi-headed Attetion可以看作是多通道的Attention。以2heads为例，计算过程如下，增加了多个参数矩阵。<br><img src="/images/transformer/multi-head.png" alt="multi-head"><br>在最后，将bi1与bi2一起其他的相应多通道b在特征维度拼接起来，为保证dim不变，利用一个新的参数矩阵Wo将输出元素的维度变为和输入相同。<br><img src="/images/transformer/concat.png" alt="concat"><br>一个线性层就可看作一个参数矩阵，所以上述两步操作可看作如下的过程。<br><img src="/images/transformer/liner-representatoin.png" alt="liner-representatoin"> </p></blockquote><hr><h3 id="Cross-Attention-in-Transformer"><a href="#Cross-Attention-in-Transformer" class="headerlink" title="Cross Attention in Transformer"></a>Cross Attention in Transformer</h3><p>Cross Attention指的是Encoder与Decoder之间的Attention机制。其V和K来自于Encoder，而Q来自于masked Attention。由于是masked，向Q输入的部分其seq_len会与V和K的不同，又因为其作为Q输入，所以cross-attention输出的seq_len会与之相同，所以Decoder的输入输出seq_len是相同的。<br><img src="/images/transformer/cross-attention.png" alt="cross-attention">  </p><hr><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed-Forward"></a>Feed-Forward</h3><p>只有一个MLP分别去作用于seq中的每个词，<br>图中MLP权重是相同的，也不需要把Encoder的输出合并输入到大的MLP。因为这里只是想要把原始维度投影到想要的另一个维度，其信息融合已经在Encoder中做完了。<br><img src="/images/transformer/feed-forward.png" alt="feed-forward"></p><hr><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>编码器要有embedding，解码器要有embedding，softmax层之前有一个Liner层，这三个层共享权重。</p><hr><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>为了给Attention加上时序信息，给输入加上位置信息。</p><hr>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers </tag>
            
            <tag> Neural-Network </tag>
            
            <tag> Seq2Seq-Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ResNet</title>
      <link href="/2022/01/29/resnet/"/>
      <url>/2022/01/29/resnet/</url>
      
        <content type="html"><![CDATA[<h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><p>深层的神经网络通常很难进行训练，本文使用了一个残差学习网络结构来训练比以往的神经网络要深得多的模型。残差网络容易训练，并且在深层神经网络中表现出来较好的准确率。</p><p><img src="/images/resnet/plain_error.png" alt="plain_error"><br>在未使用残差网络的模型中，当网络层数变多时，训练误差以及测试误差均会升高。</p><h2 id="Is-learning-better-networks-as-easy-as-stacking-more-layers"><a href="#Is-learning-better-networks-as-easy-as-stacking-more-layers" class="headerlink" title="Is learning better networks as easy as stacking more layers ?"></a><strong>Is learning better networks as easy as stacking more layers ?</strong></h2><ul><li>当网络变得特别深时，会出现梯度爆炸或梯度消失</li><li>传统的解决方法是：参数在初始化时要做的好一点，不要太大也不要太小；加入一些Batch Normalization Layers</li><li>传统的解决方法使得神经网络能够收敛，但是网络的精度却变得更差，而这并非是模型变得复杂后导致的过拟合问题，因为模型的训练误差也变高了</li><li>正常来说，如果在一个浅层的神经网络后直接加入更多的层，这些层只做identity mapping，那么这个深层神经网络的误差绝不会高于浅层的神经网络，但是传统的神经网络模型并未找到这样的解（或更好的解）</li><li>如果深层网络后面的层都是是恒等映射，那么模型就可以转化为一个浅层网络</li></ul><h2 id="Deep-residual-learning-framework"><a href="#Deep-residual-learning-framework" class="headerlink" title="Deep residual learning framework"></a><strong>Deep residual learning framework</strong></h2><ul><li>将残差块的输入与块内最后一个神经网络层的线性输出求和后在进行激活，得到残差块的输出<br><img src="/images/resnet/residual_block.png" alt="plain_error"></li><li>残差块只简单地做了shortcut connections，没有引入额外的训练参数，不会增加网络复杂度</li><li>已有的神经网络很难拟合潜在的恒等映射函数H(x) = x，但是ResNet将残差块设计为H(x) = F(x) + x，其直接把恒等映射作为网络的一部分，只要F(x) = 0，便得到了恒等映射。而此时F(x) = H(x) - x 称为残差函数，就是当前残差块的学习目标（学习出这样一个F(x)函数满足如图输出）</li><li>值得一提的是，一个残差块中应该至少有两层（中间要包含一个非线性激活），否则就会出现如下情况，这显然是没有用的工作<br><img src="/images/resnet/no_use.png" alt="plain_error"></li></ul><h2 id="Deeper-bottleneck-architecture"><a href="#Deeper-bottleneck-architecture" class="headerlink" title="Deeper bottleneck architecture"></a><strong>Deeper bottleneck architecture</strong></h2><ul><li>当神经网络层数进一步增多时，参数的增长会带来很大的计算开销。此时可以考虑使用1*1的卷积核暂时减少通道数来减少整个网络的数据规模<br><img src="/images/resnet/deeper.png" alt="plain_error"></li></ul><h2 id="Analysis-of-Deep-Residual-Networks"><a href="#Analysis-of-Deep-Residual-Networks" class="headerlink" title="Analysis of Deep Residual Networks"></a><strong>Analysis of Deep Residual Networks</strong></h2><ul><li>基本BP算法流程如下<br><img src="/images/resnet/bp.jpg" alt="plain_error"></li><li>残差块的反向传播过程较好地解释了残差网络避免梯度消失原因，具体推导过程如下</li><li>推导中忽略偏置项和激活函数<br><img src="/images/resnet/res_bp.png" alt="plain_error"></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers </tag>
            
            <tag> Neural-Network </tag>
            
            <tag> Residual-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN #1</title>
      <link href="/2022/01/29/Gan0/"/>
      <url>/2022/01/29/Gan0/</url>
      
        <content type="html"><![CDATA[<h1 id="Generative-Adversarial-Nets"><a href="#Generative-Adversarial-Nets" class="headerlink" title="Generative Adversarial Nets"></a>Generative Adversarial Nets</h1><p>Gan利用对抗的方法，提出了生成式模型的新框架。它需要同时训练两个模型（生成模型G和判别模型D），G用于捕获数据的分布，而D需要判别出一个样本是来自训练集还是生成模型。生成模型的目标是尽最大可能让判别模型犯错（无法成功判别数据的来源），G和D在本文中被定义为多层感知机，整个系统通过反向传播来进行训练。</p><h2 id="AN-analogy-to-GAN"><a href="#AN-analogy-to-GAN" class="headerlink" title="AN analogy to GAN"></a><strong>AN analogy to GAN</strong></h2><p>在生成对抗网络框架中，生成模型与判别互相对抗。可以把生成模型类比为造假币者，判别模型类比为警察。生成模型试图制造假币骗过判别模型，而判别模型努力区分假币。二者在这样的对抗中不断学习提升各自的水平，直到生成模型的造的假币和真的一模一样，判别器无法区分。另外，警察进步不能过大或过小。进步过大时，造假者直接被一锅端，无法继续造假钞；进步过小时，造假者不需进步也能骗过警察，则没有动力进步</p><h2 id="Adversarial-nets"><a href="#Adversarial-nets" class="headerlink" title="Adversarial nets"></a><strong>Adversarial nets</strong></h2><ul><li><p>对抗网络认为数据集代表着一个联合分布，每一个样本都可以由高维随机变量表示。假设数据集是许多张2*2大小的黑白图片，每张图片有四个像素点，于是将该分布看作四维随机变量的分布，每一维代表着一个像素的取值（黑白只取0或1）<br><img src="/images/gan/mrv.jpg"></p></li><li><p>在让生成模型学习数据集代表了分布之前，首先定义一个符合高斯分布的先验噪声Z，生成模型就是要学习把Z映射成为数据集代表的分布，MLP理论上可以拟合这样一个函数来完成目标。生成器以Z为输入，输出一个尽量符合数据集分布的样本  </p></li><li><p>判别器也是一个MLP，判别器以数据集或生成器生产的样本为输入，输出一个标量（0：生成器生成的样本，1：真实样本，判别器的输出介于[0,1]），判别样本的真伪。判别器要学习一个二分类任务  </p></li><li><p>D与G对于同一个目标函数采取相反的优化方式：<br><img src="/images/gan/obj_func.png"><br>在不同分布里采样计算后得到期望和，生成器和鉴别器分别要调整参数最大化和最小化这一期望和，体现了对抗的过程，D尽量区分生成的数据和真实数据，G尽量使得生成的数据和真实数据难以区分</p></li><li><p>下图展示了对抗网络训练过程中，各个成分的变化：<br><img src="/images/gan/graph_proc.png"><br>Z为高斯噪声，绿色线代表G生成数据的概率密度函数，黑色线代表真实数据的概率密度函数，蓝色代表D输出标量的函数<br>（a）G将Z随即映射为另一个分布，与真实分布存在一定差距，此时D未经学习，分类能力弱<br>（b）固定G，训练D，D分类能力明显上升<br>（c）固定D，训练G，G学习到把Z向真实数据分布映射<br>（d）反复多轮后，G映射的分布与真实数据相同，判别器无法区分</p></li><li><p>对抗网络算法流程如下：<br><img src="/images/gan/algorithm.png"><br>先更新D，再更新G，G只与目标函数中后半段有关。k是一个超参数，不能太大也不能大小。取太大判别器训练得太好，取太小判别器变化太小</p></li></ul><h2 id="Theoretical-results"><a href="#Theoretical-results" class="headerlink" title="Theoretical results"></a><strong>Theoretical results</strong></h2><ul><li>理论上，对抗网络存在全局最优解：生成器映射的分布等于真实数据分布</li><li>对抗网络算法可以求解目标函数</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers </tag>
            
            <tag> Neural-Network </tag>
            
            <tag> Generative-Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: GoogLeNet</title>
      <link href="/2022/01/29/d2l_15/"/>
      <url>/2022/01/29/d2l_15/</url>
      
        <content type="html"><![CDATA[<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>Inception块<ul><li>4个路径从不同层面抽取信息，然后再输出通道合并，最终输出高宽与输入相等，要把更多的通道数留给比较重要的通道<br><img src="/images/d2l/15/1.png"></li><li>要达到相同的输出通道数，Inception块与直接的3x3或5x5卷积相比，参数和计算复杂度更低</li></ul></li><li>GoogLeNet<ul><li>5个stage（高宽减半一次就是一个stage），9个Inception块<br><img src="/images/d2l/15/2.png"></li></ul></li><li>Inception后续具有多个变种<ul><li>Inception-BN(v2)：使用batch normalization</li><li>Inception-v3：修改了inception块</li><li>Inception-v4：使用了残差连接</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inception</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>        self.relu=nn.ReLU()<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        p1 = self.relu(self.p1_1(x))<br>        p2 = self.relu(self.p2_2(self.relu(self.p2_1(x))))<br>        p3 = self.relu(self.p3_2(self.relu(self.p3_1(x))))<br>        p4 = self.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 以通道维拼接张量</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br><br>b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>,<br>                                           padding=<span class="hljs-number">1</span>))<br><br>b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>), nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,resize=<span class="hljs-number">96</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"),</span><br><span class="hljs-comment">#     save_name="GoogLeNet"</span><br><span class="hljs-comment"># )</span><br><br>d2l.evaluate(<br>    net,test_iter,loss_f,<br>    <span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/GoogLeNet_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Convolution and Pooling</title>
      <link href="/2022/01/29/d2l_10/"/>
      <url>/2022/01/29/d2l_10/</url>
      
        <content type="html"><![CDATA[<h1 id="Convolution-and-Pooling"><a href="#Convolution-and-Pooling" class="headerlink" title="Convolution and Pooling"></a>Convolution and Pooling</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li><p>卷积</p><ul><li>平移不变性和局部性是在图片中寻找某种模式的原则。<ul><li>因为模式不会随着其在图片中位置改变而改变，所以一个识别器（卷积核）被设计为具有平移不变性（即参数只与输入的像素值有关，而与像素在图片的位置无关），去学习图片中的一种模式</li><li>模式与其相邻的局部相关，识别器每次仅去看图片的一部分</li><li>对全连接层使用平移不变性和局部性得到卷积层</li></ul></li></ul></li><li><p>卷积层<br><img src="/images/d2l/10/1.png"></p><ul><li>不同卷积核（值）会对图片带来不同的效果，当某种效果对任务有帮助时，网络很有可能就会学习出这种卷积核<br><img src="/images/d2l/10/2.png"></li></ul></li><li><p>填充与步幅</p><ul><li>在输入的四周加入额外的行和列以控制卷积后的输出图像大小，卷积核大小一般选奇数，能上下对称地填充图片来保证输入输出图片大小不变<br><img src="/images/d2l/10/3.png"></li><li>增大卷积步幅，快速缩小图片</li><li>总结<br><img src="/images/d2l/10/4.png"></li></ul></li><li><p>通道</p><ul><li>每个通道有自己的卷积核，输入通道不同通道的对应卷积后直接相加后再加偏置项，最后输出一个单通道</li><li>多输出通道就是多个上述操作输出的多个单通道</li><li>每个输出通道可以识别特定的模式，输入通道识别并组合（加权相加）输入中的模式</li></ul></li><li><p>池化</p><ul><li>池化层缓解卷积对位置的敏感性</li><li>池化层不学习任何参数，有最大池化、平均池化等</li><li>池化层也可以调整填充与步幅</li><li>经过池化层不会改变通道数</li></ul></li><li><p>tricks</p><ul><li>填充：一般填充就是为了使图小大小不变</li><li>步幅：一般设置为1，计算量太大时增大步幅</li><li>最终图像大小：一般为3x3、5x5、7x7</li><li>1x1卷积层：不识别空间模式，而是用来改变通道数</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: LeNet</title>
      <link href="/2022/01/29/d2l_11/"/>
      <url>/2022/01/29/d2l_11/</url>
      
        <content type="html"><![CDATA[<h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>最早是用于手写数字识别，识别信件上的邮政编码</li><li>网络结构<br><img src="/images/d2l/11/1.png" alt="$cover"></li><li>提出了一个数据集：MNIST<ul><li>5w个训练数据</li><li>1w个测试数据</li><li>图像大小 28x28</li><li>10类</li></ul></li><li>总结<ul><li>LeNet是早期成功的神经网络</li><li>先使用卷积层学习图片空间信息</li><li>然后使用全连接层转换到类别空间</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-comment"># 定义Reshape层</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Reshape</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br><br>net=nn.Sequential(<br>    Reshape(),<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),nn.Sigmoid(),nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br><br>x=torch.rand((<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>),dtype=torch.float32)<br><br><span class="hljs-comment"># 观察每一层输出的tensor尺寸</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    x=layer(x)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">':\t'</span>,x.size())<br><br><span class="hljs-comment"># 在Fashion-MINST上的表现</span><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     25,loss_f,opt,net,train_iter,</span><br><span class="hljs-comment">#     param_name="LeNet",device=torch.device("cuda:0")</span><br><span class="hljs-comment">#     )</span><br>d2l.evaluate(net,test_iter,loss_f,<span class="hljs-string">".\params\LeNet_25"</span>)   <br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: AlexNet</title>
      <link href="/2022/01/29/d2l_12/"/>
      <url>/2022/01/29/d2l_12/</url>
      
        <content type="html"><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>在深度学习之前：<ul><li>核方法<ul><li>特征提取</li><li>选择核函数</li><li>凸优化问题</li><li>漂亮的定理</li></ul></li><li>几何学<ul><li>抽取特征</li><li>将计算机视觉问题描述为几何问题（如多相机）</li><li>凸优化</li><li>漂亮的定理</li><li>建立假设模型，若假设满足，效果会很好</li></ul></li><li>特征工程<ul><li>特征工程（人工特征提取）是关键，不太关心机器学习模型</li><li>特征描述子（SIFT、SURF）</li><li>视觉词袋（聚类）</li><li>最后一般用SVM</li></ul></li></ul></li><li>AlexNet赢得了2012年ImageNet竞赛的冠军，引起了深度学习的热潮，其本质上是一个更深更大的LeNet<ul><li>主要改进：<ul><li>丢弃法</li><li>ReLU</li><li>MaxPooling</li><li>数据增强（截取、调亮度、调色温等）</li><li>更深更大</li></ul></li></ul></li><li>网络结构与复杂度<br><img src="/images/d2l/12/1.png"></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">import</span> d2l<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AlexNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1=nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,kernel_size=<span class="hljs-number">11</span>,stride=<span class="hljs-number">4</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv2=nn.Conv2d(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>)<br>        self.conv3=nn.Conv2d(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv4=nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">384</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.conv5=nn.Conv2d(<span class="hljs-number">384</span>,<span class="hljs-number">256</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.liner1=nn.Linear(<span class="hljs-number">6400</span>,<span class="hljs-number">4096</span>)<br>        self.liner2=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">4096</span>)<br>        self.liner3=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)<br><br>        self.pool=nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>)<br>        self.flatten=nn.Flatten()<br>        self.relu=nn.ReLU()<br>        self.dropout=nn.Dropout(p=<span class="hljs-number">0.5</span>)<br>    <br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        <br>        x=self.relu(self.pool(self.conv1(x)))<br>        <br>        x=self.relu(self.pool(self.conv2(x)))<br>        x=self.relu(self.conv3(x))<br>        x=self.relu(self.conv4(x))<br>        x=self.relu(self.conv5(x))<br>        x=self.pool(x)<br>        x=self.flatten(x)<br>        x=self.dropout((self.liner1(x)))<br>        x=self.dropout((self.liner2(x)))<br>        x=self.liner3(x)<br><br>        <span class="hljs-keyword">return</span> x<br><br>net=AlexNet().to(torch.device(<span class="hljs-string">"cuda:0"</span>))<br><br><span class="hljs-comment"># x=torch.rand((1,1,224,224)).cuda()</span><br><span class="hljs-comment"># print(net(x).size())</span><br><br><span class="hljs-comment"># 读取Fashion-MNIST，将图片直接拉伸为224x224</span><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">256</span>,<span class="hljs-number">224</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     25,nn.CrossEntropyLoss(),</span><br><span class="hljs-comment">#     optim.Adam(net.parameters()),</span><br><span class="hljs-comment">#     net,train_iter,save_name="AlexNet",</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"))</span><br>d2l.evaluate(<br>    net,test_iter,nn.CrossEntropyLoss(),<br>    param_path=<span class="hljs-string">"D:\code\machine_learning\limu_d2l\params\AlexNet_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: NiN</title>
      <link href="/2022/01/29/d2l_14/"/>
      <url>/2022/01/29/d2l_14/</url>
      
        <content type="html"><![CDATA[<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>全连接层的问题：<ul><li>全连接层参数比卷积层的参数多很多，导致很多的内存（显存）及计算带宽占用</li><li>全连接层容易带来过拟合</li></ul></li><li>NiN思想：完全不要全连接层</li><li>NiN块：<ul><li>一个卷积层后跟两个起到全连接层的作用的卷积层</li><li>起到全连接层的作用的卷积层为1x1步幅为1无填充的卷积层</li></ul></li><li>NiN架构<ul><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层</li><li>最后使用全局平均池化层得到输出（通道数是类别数）<br><img src="/images/d2l/14/1.png"></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NiNBlock</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,in_channels,out_channels,</span></span><br><span class="hljs-params"><span class="hljs-function">        kernel_size,stride,padding</span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv=nn.Conv2d(<br>            in_channels,out_channels,kernel_size,<br>            stride=stride,padding=padding<br>        )<br>        self.f1=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">1</span>)<br>        self.f2=nn.Conv2d(out_channels,out_channels,<span class="hljs-number">1</span>)<br>        self.relu=nn.ReLU()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><br>        x=self.relu(self.conv(x))<br>        x=self.relu(self.f1(x))<br>        x=self.relu(self.f2(x))<br>        <span class="hljs-keyword">return</span> x<br><br>nin_net=nn.Sequential(<br>    NiNBlock(<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,<span class="hljs-number">11</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),<br>    NiNBlock(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),<br>    NiNBlock(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>),nn.Dropout(),<br>    NiNBlock(<span class="hljs-number">384</span>,<span class="hljs-number">10</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<br>    <span class="hljs-comment"># 目标输出size为1x1，也就是全局池化</span><br>    nn.AdaptiveAvgPool2d(<span class="hljs-number">1</span>),<br>    nn.Flatten()<br>)<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(nin_net.parameters())<br><br><span class="hljs-keyword">import</span> d2l<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">128</span>,resize=<span class="hljs-number">224</span>)<br><br><span class="hljs-comment"># d2l.train(</span><br><span class="hljs-comment">#     10,loss_f,opt,nin_net,train_iter,</span><br><span class="hljs-comment">#     device=torch.device("cuda:0"),</span><br><span class="hljs-comment">#     save_name="NIN"</span><br><span class="hljs-comment"># )</span><br>d2l.evaluate(<br>    nin_net,test_iter,loss_f,<br>    <span class="hljs-string">"D:/code/machine_learning/limu_d2l/params/NIN_5"</span>,<br>    device=torch.device(<span class="hljs-string">"cuda:0"</span>)<br>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: VGGNet</title>
      <link href="/2022/01/29/d2l_13/"/>
      <url>/2022/01/29/d2l_13/</url>
      
        <content type="html"><![CDATA[<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>AlexNet的设计很随意，如何变大变深无规律性，VGG探讨了如何对CNN进行扩展</li><li>如何更深更大？<ul><li>更多全连接层（太贵）</li><li>更多的卷积层</li><li>将卷积层组合成块（VGG）</li></ul></li><li>VGG块<ul><li>使用小卷积核深网络比大小卷积核浅网络效果好</li><li>3x3卷积层（n层、m通道）</li><li>2x2最大池化层<br><img src="/images/d2l/13/2.png"></li></ul></li><li>VGG架构<ul><li>多个VGG块后接全连接层</li><li>不同次数的重复块得到不同架构（VGG-16、VGG-19等）<br><img src="/images/d2l/13/1.png"></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> dropout, nn,optim<br><span class="hljs-keyword">import</span> d2l<br><span class="hljs-comment"># 返回VGG块</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_block</span>(<span class="hljs-params">num_convs,in_channels,out_channels</span>):</span><br>    layers=[]<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(<br>            nn.Conv2d(in_channels,out_channels,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>            )<br>        layers.append(nn.ReLU())<br>        in_channels=out_channels<br><br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>    <br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_architecture</span>(<span class="hljs-params">num_blocks,in_channels</span>):</span><br>    blocks=[]<br>    out_channels=<span class="hljs-number">16</span><br>    blocks.append(vgg_block(<span class="hljs-number">1</span>,in_channels,out_channels))<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks-<span class="hljs-number">1</span>):<br>        in_channels=out_channels<br>        out_channels*=<span class="hljs-number">2</span><br>        blocks.append(vgg_block(<span class="hljs-number">1</span>,in_channels,out_channels))<br>    <br>    <span class="hljs-keyword">return</span> nn.Sequential(*blocks)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vgg_5</span>(<span class="hljs-params">in_channels</span>):</span><br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        vgg_architecture(<span class="hljs-number">5</span>,in_channels),<br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">1024</span>),<br>        nn.Dropout(),<br>        nn.Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">512</span>),<br>        nn.Dropout(),<br>        nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>)<br>    )<br><br>vgg=vgg_5(<span class="hljs-number">1</span>).to(torch.device(<span class="hljs-string">"cuda:0"</span>))<br><br>train_iter,test_iter=d2l.load_data_fashion_mnist(<span class="hljs-number">64</span>,<span class="hljs-number">224</span>)<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(vgg.parameters())<br><br>d2l.train(<span class="hljs-number">10</span>,loss_f,opt,vgg,train_iter,save_name=<span class="hljs-string">"vgg_5"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Sequential Model</title>
      <link href="/2022/01/29/d2l_9/"/>
      <url>/2022/01/29/d2l_9/</url>
      
        <content type="html"><![CDATA[<h1 id="Sequential-Model"><a href="#Sequential-Model" class="headerlink" title="Sequential Model"></a>Sequential Model</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><p>序列模型是考虑时间信息的模型</p><ul><li><p>序列数据</p><ul><li>数据带有时序结构，如电影的评价随时间变化<ul><li>电影拿奖后评分上升</li><li>导演、演员负面报道后评分下降</li></ul></li></ul></li><li><p>统计工具  </p><ul><li>将序列中每个元素看作随机变量，显然他们不是独立的<br><img src="/images/d2l/9/1.png"></li><li>在实际操作中，时序序列一般只能正向建模去预测<br><img src="/images/d2l/9/2.png"></li><li>要使用序列模型预测T时刻x的概率，核心是求T时刻的条件概率（似然），这里的f可看作神经网络，神经网络将训练集建模。自回归指的是用数据对见过的数据建模（因为最后预测也是在预测相同的数据），与非序列模型用数据对独立于数据的标签建模不同。<br><img src="/images/d2l/9/3.png"></li><li>具体如何建模？<ul><li>马尔科夫假设<br>当前预测的数据只跟过去的tau个数据相关，tau是一个固定常数。假设x是标量数据，此时只需要将其看作回归问题，使用MLP把tau个x当作特征训练得到t时刻标量x。MLP进行梯度优化的过程便是最大化似然概率的过程<br><img src="/images/d2l/9/4.png"></li><li>潜变量模型<br>引入一个可不断更新的潜变量用于概括历史信息，使得建模更加简单（RNN）<br><img src="/images/d2l/9/5.png"></li></ul></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>马尔可夫假设+MLP<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 使用正弦函数加上噪声生成序列数据</span><br>T=<span class="hljs-number">1000</span><br><br>time=torch.arange(<span class="hljs-number">1</span>,T+<span class="hljs-number">1</span>,dtype=torch.float32)<br>data=torch.sin(<span class="hljs-number">0.01</span>*time)+torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.1</span>,time.shape)<br><br><span class="hljs-comment"># 将数据映射为数据对</span><br>tau=<span class="hljs-number">4</span><br>labels=data[tau:].view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>features=torch.zeros(T-tau,tau)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau):<br>    features[:,i]=data[i:T-tau+i]<br><br>train_dataset=TensorDataset(features[:<span class="hljs-number">600</span>],labels[:<span class="hljs-number">600</span>])<br>test_dataset=TensorDataset(features[<span class="hljs-number">600</span>:],labels[<span class="hljs-number">600</span>:])<br><br>train_iter=DataLoader(train_dataset,batch_size=<span class="hljs-number">16</span>)<br>test_iter=DataLoader(test_dataset,batch_size=<span class="hljs-number">16</span>)<br><br>net=nn.Sequential(<br>    nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">32</span>),<br>    nn.Dropout(<span class="hljs-number">0.1</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">16</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">16</span>,<span class="hljs-number">1</span>)<br>    )<br>loss_f=nn.MSELoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-keyword">try</span>:<br>    net.load_state_dict(torch.load(<span class="hljs-string">"9.params"</span>))<br><br><span class="hljs-keyword">except</span>:<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>        train_loss=[]<br>        test_loss=[]<br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            out=net(X)<br>            l=loss_f(out,y)<br><br>            l.backward()<br>            train_loss.append(l.item())<br>            opt.step()<br>            opt.zero_grad()<br><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> test_iter:<br>                out=net(X)<br>                l=loss_f(out,y)<br>                test_loss.append(l.item())<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>,<span class="hljs-subst">{<span class="hljs-built_in">sum</span>(train_loss)}</span>  <span class="hljs-subst">{<span class="hljs-built_in">sum</span>(test_loss)}</span>"</span>)<br>    torch.save(net.state_dict(),<span class="hljs-string">"9.params"</span>)<br><br><span class="hljs-comment"># 使用测试集预测</span><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>t=<span class="hljs-number">600</span><br>steps=<span class="hljs-number">396</span><br><br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],net(features[<span class="hljs-number">600</span>:]).view(-<span class="hljs-number">1</span>).detach().numpy())<br><br><span class="hljs-comment"># 使用预测值进行多步预测</span><br><br>win=[math.sin(i*<span class="hljs-number">0.01</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t-<span class="hljs-number">4</span>,t)]<br>true=[]<br>pred=[]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):<br>    X=torch.tensor(win,dtype=torch.float32)<br>    out=net(X)<br>    truth=math.sin((t+i)*<span class="hljs-number">0.01</span>)<br>    true.append(truth);pred.append(out.item())<br>    <span class="hljs-comment"># print(out.item(),t)</span><br>    win.pop(<span class="hljs-number">0</span>)<br>    win.append(out)<br><br><br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],pred)<br>plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t,t+steps)],true)<br>plt.show()<br></code></pre></td></tr></tbody></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Basic Pytorch</title>
      <link href="/2022/01/29/d2l_8/"/>
      <url>/2022/01/29/d2l_8/</url>
      
        <content type="html"><![CDATA[<h1 id="Basic-Pytorch"><a href="#Basic-Pytorch" class="headerlink" title="Basic Pytorch"></a>Basic Pytorch</h1><ul><li>模型构造</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>net=nn.Sequential(nn.Linear(<span class="hljs-number">20</span>,<span class="hljs-number">256</span>),nn.ReLU(),nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>))<br>X=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>))<br><span class="hljs-built_in">print</span>(net(X))<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.rand_weight=torch.randn((<span class="hljs-number">20</span>,<span class="hljs-number">64</span>),requires_grad=<span class="hljs-literal">False</span>,dtype=torch.float32)<br>        self.hidden=nn.Sequential(nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>),nn.ReLU(),nn.Linear(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>))<br>        self.out=nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        X=X@self.rand_weight+<span class="hljs-number">1</span><br>        X=nn.ReLU()(self.hidden(X))<br>        X=self.out(X)<br>        <span class="hljs-comment"># 下面这个过程流不会计入计算图</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">while</span> torch.<span class="hljs-built_in">abs</span>(X).<span class="hljs-built_in">sum</span>()&gt;<span class="hljs-number">1</span>:<br>                X/=<span class="hljs-number">2</span><br>        <span class="hljs-keyword">return</span> X.<span class="hljs-built_in">sum</span>()<br><br>net=MLP()<br><span class="hljs-built_in">print</span>(net(X))<br></code></pre></td></tr></tbody></table></figure><ul><li>参数管理</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>net=nn.Sequential(nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">8</span>),nn.ReLU(),nn.Linear(<span class="hljs-number">8</span>,<span class="hljs-number">1</span>))<br>X=torch.rand(size=(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>))<br><br><span class="hljs-comment"># 访问参数</span><br><span class="hljs-comment"># 访问网络中每一层，以及如何访问层中的参数</span><br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">0</span>].state_dict(),net[<span class="hljs-number">1</span>],net[<span class="hljs-number">2</span>].bias.data)<br><span class="hljs-comment"># 网络中的参数包括data和grad两部分，在这没做反向传播呢，所以梯度为None</span><br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>].weight.grad==<span class="hljs-literal">None</span>)<br><span class="hljs-comment"># 将整个网络信息打印</span><br><span class="hljs-built_in">print</span>(net)<br><br><span class="hljs-comment"># 初始化参数</span><br><span class="hljs-comment"># pytorch已经为我们做了比较好的默认初始化</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_xavier</span>(<span class="hljs-params">m</span>):</span><br>    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">type</span>(m)==nn.Linear):<br>        nn.init.xavier_normal_(m.weight)<br>        nn.init.zeros_(m.bias)<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_42</span>(<span class="hljs-params">m</span>):</span><br>    nn.init.constant_(m.weight,<span class="hljs-number">42</span>)<br>    nn.init.constant_(m.bias,<span class="hljs-number">42</span>)<br><span class="hljs-comment"># 将初始化函数应用到net每一个子层，不止可以用在初始化上</span><br>net[<span class="hljs-number">0</span>].apply(init_xavier)<br><span class="hljs-built_in">print</span>([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> net[<span class="hljs-number">0</span>].parameters()])<br>net[<span class="hljs-number">2</span>].apply(init_42)<br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>].weight,net[<span class="hljs-number">2</span>].bias)<br><br><span class="hljs-comment"># 更暴力的方法</span><br>net[<span class="hljs-number">0</span>].weight.data[:]+=<span class="hljs-number">100</span><br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">0</span>].weight.data)<br><br><span class="hljs-comment"># 参数绑定</span><br><span class="hljs-comment"># 让某几层共享同样的参数</span><br>shared=nn.Linear(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>)<br>net=nn.Sequential(nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">8</span>),nn.ReLU(),shared,nn.ReLU(),shared,nn.Sigmoid(),nn.Linear(<span class="hljs-number">8</span>,<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>]==shared)<br><span class="hljs-built_in">print</span>(net[<span class="hljs-number">2</span>].weight.data==net[<span class="hljs-number">4</span>].weight.data)<br></code></pre></td></tr></tbody></table></figure><ul><li>自定义层</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment"># 自定义层和自定义模型没本质区别</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CenteredLayer</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        <br>        <span class="hljs-keyword">return</span> X-X.mean()<br><br>layer=CenteredLayer()<br><span class="hljs-built_in">print</span>(layer(torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],dtype=torch.float32)))<br><br><span class="hljs-comment"># 带有参数的层</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PrameterizedLayer</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_dim,out_dim</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.w=nn.Parameter(torch.randn((in_dim,out_dim)))<br>        self.b=nn.Parameter(torch.randn((out_dim)))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,X</span>):</span><br>        <br>        <span class="hljs-keyword">return</span> X@self.w.data+self.b.data<br><br>net=PrameterizedLayer(<span class="hljs-number">2</span>,<span class="hljs-number">8</span>)<br><span class="hljs-built_in">print</span>(net(torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]],dtype=torch.float32)))<br></code></pre></td></tr></tbody></table></figure><ul><li>读写文件</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>x=torch.arange(<span class="hljs-number">4</span>)<br><span class="hljs-comment"># 加载和保存张量</span><br>torch.save(x,<span class="hljs-string">'x_file'</span>)<br>y=torch.load(<span class="hljs-string">'x_file'</span>)<br><span class="hljs-built_in">print</span>(x==y)<br><br><span class="hljs-comment"># 加载和保存张量组成的数据结构</span><br>a=torch.arange(<span class="hljs-number">5</span>)<br>b=torch.arange(<span class="hljs-number">5</span>)<br>torch.save([a,b],<span class="hljs-string">"list"</span>)<br>torch.save({<span class="hljs-string">'a'</span>:a,<span class="hljs-string">'b'</span>:b},<span class="hljs-string">"dict"</span>)<br><span class="hljs-comment"># 读取到内存会保持原数据结构</span><br><span class="hljs-built_in">print</span>(torch.load(<span class="hljs-string">"list"</span>))<br><span class="hljs-built_in">print</span>(torch.load(<span class="hljs-string">"dict"</span>))<br><br><span class="hljs-comment"># 加载和保存模型的参数</span><br>net=nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>),nn.Flatten(),nn.Linear(<span class="hljs-number">288</span>,<span class="hljs-number">64</span>))<br><span class="hljs-built_in">print</span>(net(torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.5</span>,size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">16</span>,<span class="hljs-number">16</span>))))<br><span class="hljs-built_in">print</span>(net.state_dict())<br>torch.save(net.state_dict(),<span class="hljs-string">'net.params'</span>)<br>new_net=nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>),nn.Flatten(),nn.Linear(<span class="hljs-number">288</span>,<span class="hljs-number">64</span>))<br>new_net.load_state_dict(torch.load(<span class="hljs-string">'net.params'</span>))<br><span class="hljs-built_in">print</span>(new_net==net)<br><br>X=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.5</span>,size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">16</span>,<span class="hljs-number">16</span>))<br><span class="hljs-built_in">print</span>(new_net(X)==net(X))<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Numerical Stability &amp; Initialization</title>
      <link href="/2022/01/29/d2l_7/"/>
      <url>/2022/01/29/d2l_7/</url>
      
        <content type="html"><![CDATA[<h1 id="D2L-Numerical-Stability-amp-Initialization"><a href="#D2L-Numerical-Stability-amp-Initialization" class="headerlink" title="D2L: Numerical Stability &amp; Initialization"></a>D2L: Numerical Stability &amp; Initialization</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>数值的稳定性<ul><li>神经网络的梯度<br>求某一层的参数的梯度，直接就对损失函数关于该层参数求导,然后通过链式法则，化成d-t次的矩阵乘法<br><img src="/images/d2l/7/chain_d.png"></li><li>梯度爆炸与梯度消失<br>上述连续的乘法运算会带来两个问题：梯度爆炸与梯度消失<br><img src="/images/d2l/7/e_v.png"><br>梯度爆炸带来的问题：梯度值超过计算机可表示大小、对学习率敏感<br>梯度消失带来的问题：梯度值变为0（超出计算可表示精度的小浮点数）、无论如何选择学习率训练都没有进展、神经网络无法做到更深</li></ul></li><li>模型初始化<ul><li>如何让训练更加稳定？（不产生梯度消失和梯度爆炸）<br>要让梯度值保持在合理的范围内，一般有如下方法：<ul><li>将乘法变为加法（ResNet、LSTM）</li><li>归一化（梯度归一化、梯度裁剪）</li><li>选定合适的激活函数</li><li>合理的初始参数</li></ul></li><li>让每层的方差是一个常数<br><img src="/images/d2l/7/bn.png"></li><li>合理的权重初始化<ul><li>需要在一个合理值区间里随机初始参数<ul><li>远离最优解的地方损失函数很复杂（梯度很大）</li><li>最优解附近比较平缓 </li></ul></li></ul></li><li>Xavier初始化<br>使得输入空间的方差和输出空间的方差尽量相等</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Weight Decay &amp; Dropout</title>
      <link href="/2022/01/29/d2l_6/"/>
      <url>/2022/01/29/d2l_6/</url>
      
        <content type="html"><![CDATA[<h1 id="Weight-Decay-amp-Dropout"><a href="#Weight-Decay-amp-Dropout" class="headerlink" title="Weight Decay &amp; Dropout"></a>Weight Decay &amp; Dropout</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>权重衰退<br>权重衰退可以控制模型复杂度，使其复杂度不会太大，从而一定程度上避免过拟合<ul><li>使用均方范数作为硬性限制<br>通过限制参数的取值范围来控制模型容量，具体有如下例子<br>在限制参数向量范数的情况下优化损失函数，一般不会使用这种正则方式<br><img src="/images/d2l/6/hard_res.png"></li><li>使用均方范数作为柔性限制<br>上述硬性限制有一个等价方案，具体如下，这就是一般的正则化方法，其作用同样也是使得参数被限制在一个较小的范围<br><img src="/images/d2l/6/soft_res.png"><br>下面是正则项对最优解影响的一个演示<br>坐标轴分别是w的各个分量，圆线是等高线。<br>正则项给了另外一个梯度，把原始的损失函数算出的最优解往原点拉，必然会导致W的取值范围变小从而使模型复杂度降低，也就减小了过拟合。另外一种理解，正则项加入后优化目标就不再全局最优点了，所以肯定会减小训练集拟合程度，也就减小了过拟合<br>考虑为什么λ控制了正则项的重要程度，因为求偏导时λ会变成梯度前的常数项<br><img src="/images/d2l/6/effect.png"></li><li>参数更新<br>带正则项后参数更新过程如下，这也说明了为什么这种方法叫权重衰退：更新前先把权重减小，然后继续更新梯度<br><img src="/images/d2l/6/update.png"></li><li>注意<br>权重衰减也只在训练过程中使用，用来限制训练过程中的参数，在最终的验证过程中，指标还是原先的损失函数</li></ul></li><li>丢弃法<ul><li>动机<br>一个好的模型需要对输入数据加入扰动鲁棒，使用有噪音的数据等价于Tikhonov正则。丢弃法就是在层之间加入噪音，丢弃法也可以看作一个正则</li><li>无偏差地加入噪音<br>丢弃法就是加入了无偏噪音<br><img src="/images/d2l/6/noise.png"></li><li>使用丢弃法  <ul><li>对其发作用域隐藏层地输出上，即随机将某些神经元的输出置零且其它输出按上述公式增大</li><li>丢弃法只在训练过程中使用，测试和验证过程中不使用，这样保证了确定的输出</li><li>丢弃法常用于全连接层</li></ul></li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>权重衰减从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 训练数据设置比较小，容易过拟合</span><br>num_train=<span class="hljs-number">20</span><br>num_test=<span class="hljs-number">100</span><br>num_inputs=<span class="hljs-number">200</span><br>batch_size=<span class="hljs-number">5</span><br><br>true_w,true_b=torch.ones(num_inputs)*<span class="hljs-number">0.01</span>,<span class="hljs-number">0.05</span><br><br>train_data=synthetic_data(true_w,true_b,num_train)<br>train_iter=data_loader(train_data,batch_size)<br>test_data=synthetic_data(true_w,true_b,num_test)<br>test_iter=data_loader(test_data,batch_size)<br><br><span class="hljs-comment"># for X,y in train_iter:</span><br><span class="hljs-comment">#     print(X,y)</span><br>w=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=true_w.shape,requires_grad=<span class="hljs-literal">True</span>)<br>b=torch.zeros(<span class="hljs-number">1</span>,requires_grad=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># L2正则</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L2_penalty</span>(<span class="hljs-params">lambada,w</span>):</span><br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(w**<span class="hljs-number">2</span>)/<span class="hljs-number">2</span>*lambada<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">liner_reg</span>(<span class="hljs-params">w,b,X</span>):</span><br>    <span class="hljs-keyword">return</span> torch.matmul(X,w)+b<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net</span>(<span class="hljs-params">X</span>):</span><br>    <span class="hljs-keyword">return</span> liner_reg(w,b,X)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">squared_loss_L2</span>(<span class="hljs-params">y_hat,y</span>):</span><br>    loss=(y_hat.view(y.shape)-y)**<span class="hljs-number">2</span>/<span class="hljs-number">2</span>/<span class="hljs-built_in">len</span>(y)<br>    <span class="hljs-keyword">return</span> (loss+L2_penalty(<span class="hljs-number">0.5</span>,w)).<span class="hljs-built_in">sum</span>()<br><br>opt=optim.SGD([w,b],lr=<span class="hljs-number">0.01</span>)<br><br><br><br>train(<span class="hljs-number">100</span>,squared_loss_L2,opt,net,train_iter,test_iter)<br></code></pre></td></tr></tbody></table></figure><ul><li>权重衰减简洁实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">net=nn.Sequential(nn.Linear(num_inputs,<span class="hljs-number">1</span>))<br>loss_f=nn.MSELoss()<br><span class="hljs-comment"># weight_decay代表了L2范数前面的λ系数</span><br><span class="hljs-comment"># 权重衰减系数很小时，在当前数据集下过拟合非常明显</span><br><span class="hljs-comment"># 权重衰减系数很大时，欠拟合则会非常明显</span><br>opt=optim.SGD(net.parameters(),lr=<span class="hljs-number">0.01</span>,weight_decay=<span class="hljs-number">1.2</span>)<br><br>train(<span class="hljs-number">100</span>,loss_f,opt,net,train_iter,test_iter)<br><span class="hljs-comment"># 其它范数的正则pytorch没有直接的实现</span><br><span class="hljs-comment"># 手动实现也很简单</span><br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Modle Selection</title>
      <link href="/2022/01/29/d2l_5/"/>
      <url>/2022/01/29/d2l_5/</url>
      
        <content type="html"><![CDATA[<h1 id="Modle-Selection"><a href="#Modle-Selection" class="headerlink" title="Modle Selection"></a>Modle Selection</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>训练误差和泛化误差  <ul><li>训练误差：模型在训练数据上的误差（不太关心）</li><li>泛化误差：模型在新数据上的误差（很关心）</li></ul></li><li>验证数据集和测试数据集<ul><li>验证数据集<br>用于在训练过程中评估模型好坏的数据集，一般从训练集中划分出一部分，验证数据集不能作为训练集让模型训练，用来动态调整模型超参数</li><li>测试数据集<br>模型最终训练完毕后，使用测试集测试模型泛化能力，<strong>不能使用测试集来调整模型超参数</strong>，大多数情况下不会被打上标签</li></ul></li><li>K-折交叉验证<br>通常情况下，我们都没有足够富裕的数据去从训练集中划分验证集，这是使用K-折交叉验证能较简单的解决问题<ul><li>思想：一般情况将K折交叉验证用于模型调优，找到使得模型泛化性能最优的超参值。找到后，在全部训练集上重新训练模型，并使用独立验证集对模型性能做出最终评价。</li><li>算法：K折就将训练集分为K块，训练代价为原来的K倍<ol><li>将原始数据集划分为相等的K部分（“折”）</li><li>将第i部分作为验证集，其余作为训练集</li><li>训练模型，计算模型在验证集上的准确率</li><li>每次用不同的部分i作为验证集，重复步骤2和3 K次</li><li>将平均准确率作为使用当前超参时的模型准确率</li><li>找到一个较好的超参数后，再用全部训练集训练模型，并在一个全新的验证集上验证，不用调超参数，达到一个较好的验证准确率时，直接去测试</li></ol></li></ul></li><li>过拟合和欠拟合<br><img src="/images/d2l/5/fitting.png"><ul><li>模型容量的影响<br>数据集复杂程度应该与模型复杂程度正相关，否则就会出现过拟合与欠拟合。举例来说，当模型很复杂而数据很简单时，模型可以直接就把所有数据记住而丧失泛化能力；而模型过于简单时，如感知机模型，无法正确划分异或的数据<br>模型足够复杂时，有其他手段减少过拟合；模型太简单没前途<br><img src="/images/d2l/5/capacity.png"></li></ul></li><li>估计模型容量<br>模型种类确定时（如神经网络），模型容量由两个因素估计：参数个数、参数取值范围 </li><li>数据复杂度<br>有多个重要因素：<ul><li>样本个数</li><li>特征个数</li><li>时间、空间结构</li><li>多样性</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Perceptron</title>
      <link href="/2022/01/29/d2l_4/"/>
      <url>/2022/01/29/d2l_4/</url>
      
        <content type="html"><![CDATA[<h1 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>感知机  <ul><li>模型<br>感知机只比线性分类多了一个激活函数，激活函数为单层感知机带来了分类能力，为多层感知机带来了非线性因素<br><img src="/images/d2l/4/perceptron.png"></li><li>训练<br>训练感知机等价于批量大小为1的梯度下降，按顺序逐个取样本，与随机梯度下降不同<br><img src="/images/d2l/4/train_p.png"></li><li>单层感知机无法解决异或问题，他只能产生线性分割面，这导致了第一次AI寒冬</li></ul></li><li>多层感知机<ul><li>多层感知机由多个感知机组成，分为输入层、隐藏层、输出层，层内不连接，层间全连接<br><img src="/images/d2l/4/mlp.png"></li><li>每个感知机输出后要经过一个非线性的激活函数，否则多层感知机等价于单层感知机</li><li>常用激活函数：Sigmoiod、Tanh、ReLU，性能都没太大区别，ReLU计算更容易，如果没有特别的想法，用ReLU就行</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><br>train_iter,_=LoadData(<span class="hljs-number">256</span>)<br>num_in,num_out,num_h=<span class="hljs-number">784</span>,<span class="hljs-number">10</span>,<span class="hljs-number">256</span><br><span class="hljs-comment"># 隐层参数</span><br>w1=torch.randn(num_in,num_h,requires_grad=<span class="hljs-literal">True</span>)<br>b1=torch.zeros(num_h,requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 输出层参数</span><br>w2=torch.randn(num_h,num_out,requires_grad=<span class="hljs-literal">True</span>)<br>b2=torch.zeros(num_out,requires_grad=<span class="hljs-literal">True</span>)<br>params=[w1,b2,w1,b2]<br><br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.SGD(params,lr=<span class="hljs-number">0.001</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReLU</span>(<span class="hljs-params">X</span>):</span><br>    a=torch.zeros_like(X)<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">max</span>(X,a)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Net</span>(<span class="hljs-params">X:Tensor</span>):</span><br>    X=X.view(-<span class="hljs-number">1</span>,num_in)<br>    <span class="hljs-comment"># @被重载为了矩阵乘法</span><br>    X=ReLU(X@w1+b1)<br>    <span class="hljs-keyword">return</span> X@w2+b2<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Train</span>():</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>        loss=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            X=X.view(-<span class="hljs-number">1</span>,<span class="hljs-number">784</span>)<br>            out=Net(X)<br>            l=loss_f(out,y)<br><br>            l.backward()<br>            opt.step()<br>            opt.zero_grad()<br>            loss=l.item()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch}</span>,<span class="hljs-subst">{loss}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Softmax Regression</title>
      <link href="/2022/01/29/d2l_3/"/>
      <url>/2022/01/29/d2l_3/</url>
      
        <content type="html"><![CDATA[<h1 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>softmax操作子<br>将输出变为一个概率分布（保证非负性与归一性）<br><img src="/images/d2l/3/softmax.png"></li><li>交叉熵损失<br>用于衡量两个概率分布的区别,将softmax输出的分布与one-hot形式的标签作为两个分布<br><img src="/images/d2l/3/cross_entropy.png"></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>数据集</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LoadData</span>(<span class="hljs-params">batch_size,resize=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-comment"># 用于把图片转为Tensor，会自动归一化</span><br>    trans=[transforms.ToTensor()]<br>    <span class="hljs-keyword">if</span> resize:<br>        trans.insert(<span class="hljs-number">0</span>,transforms.Resize(resize))<br>    trans=transforms.Compose(trans)<br>    <br>    train_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">True</span>,<br>        transform=trans,download=<span class="hljs-literal">True</span><br>    )<br>    test_data=torchvision.datasets.FashionMNIST(<br>        root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-literal">False</span>,<br>        transform=trans,download=<span class="hljs-literal">True</span><br>    )<br><br>    <span class="hljs-comment"># print(test_data[0])</span><br>    <br>    <span class="hljs-keyword">return</span> (data.DataLoader(train_data,batch_size=batch_size,shuffle=<span class="hljs-literal">True</span>),<br>        data.DataLoader(test_data,batch_size=batch_size,shuffle=<span class="hljs-literal">False</span>))<br>    train_iter,test_iter=LoadData(<span class="hljs-number">256</span>) <br></code></pre></td></tr></tbody></table></figure><ul><li>从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将图片展平</span><br>num_inputs=<span class="hljs-number">1</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br><span class="hljs-comment"># 共有10类</span><br>num_outputs=<span class="hljs-number">10</span><br><br>w=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,size=(num_inputs,num_outputs),requires_grad=<span class="hljs-literal">True</span>)<br>b=torch.zeros(num_outputs,requires_grad=<span class="hljs-literal">True</span>)<br><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Softmax</span>(<span class="hljs-params">X</span>):</span><br>    <span class="hljs-comment"># 成batch分子组成的向量</span><br>    X_exp=torch.exp(X)<br>    <span class="hljs-comment"># 成batch个分母</span><br>    partition=X_exp.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>,keepdim=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 应用广播机制</span><br>    <span class="hljs-keyword">return</span> X_exp/partition<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">CrossEntropy</span>(<span class="hljs-params">y_hat,y</span>):</span><br>    <span class="hljs-comment"># 只有y为下标那一项起作用</span><br>    <span class="hljs-comment"># 按正常one_hot编码为0的项直接就没算了</span><br>    <span class="hljs-comment"># 一个batch的y_hat都取对应y中的值为下标那个</span><br>    <span class="hljs-comment"># 最终得到了整个batch中所有样本的损失</span><br>    <span class="hljs-keyword">return</span> -torch.log(y_hat[<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(y_hat)),y]).<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Net</span>(<span class="hljs-params">w,b,X</span>):</span><br>    <span class="hljs-keyword">return</span> Softmax(torch.matmul(X,w)+b)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Sgd</span>(<span class="hljs-params">params,lr</span>):</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> params:<br>            i-=lr*i.grad<br>            i.grad.zero_()<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Train</span>():</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>        loss=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>            X=X.view(-<span class="hljs-number">1</span>,<span class="hljs-number">784</span>)<br>            out=Net(w,b,X)<br>            l=CrossEntropy(out,y)<br><br>            l.backward()<br>            <span class="hljs-comment"># 学习率设置这么小是因为在损失函数或优化器中</span><br>            <span class="hljs-comment"># 没有除以batch_size,导致求的梯度会很大</span><br>            Sgd([w,b],<span class="hljs-number">0.0001</span>)<br>            loss=l.item()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch}</span>,<span class="hljs-subst">{loss}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure><ul><li>简洁实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">train_iter,test_iter=LoadData(<span class="hljs-number">256</span>)<br><br><span class="hljs-comment"># 将图片展平</span><br>num_inputs=<span class="hljs-number">1</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br><span class="hljs-comment"># 共有10类</span><br>num_outputs=<span class="hljs-number">10</span><br><br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn,optim<br><span class="hljs-comment"># Flatten() 保留第零维度，其他全部展平</span><br>net=nn.Sequential(nn.Flatten(),nn.Linear(num_inputs,num_outputs))<br>loss_f=nn.CrossEntropyLoss()<br>opt=optim.Adam(net.parameters())<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>    loss=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>        out=net(X)<br>        l=loss_f(out,y)<br>        l.backward()<br>        opt.step()<br>        opt.zero_grad()<br>        loss=l.item()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch}</span>,<span class="hljs-subst">{loss}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Linear Regression</title>
      <link href="/2022/01/29/d2l_2/"/>
      <url>/2022/01/29/d2l_2/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a><strong>Basic Knowledge</strong></h2><ul><li>线性模型  <ul><li>基本的线性模型可抽象为如下表示,其可以看作一个单层单神经元的神经网络<br><img src="/images/d2l/2/l_model.png"></li><li>线性模型有显示解<br><img src="/images/d2l/2/ex_solution.png"></li></ul></li><li>优化<ul><li>梯度下降<br>需要注意的是梯度是t-1时刻得来的，而且梯度是t-1时刻样本点、标签值以及W参数值对应的梯度，因为他们都是损失函数中的因变量<br><img src="/images/d2l/2/grad_desc.png"></li><li>成批计算梯度<br>batch_size不能太大也不能太小。太小：并行计算难以发挥效果；太大：内存消耗增加、易陷入局部最优。batch_size是另一个重要的超参数。</li></ul></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><ul><li>从零实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader<br><br>true_w=torch.tensor([<span class="hljs-number">2.2</span>,<span class="hljs-number">3</span>])<br>true_b=torch.tensor([-<span class="hljs-number">1.9</span>])<br>num_examples=<span class="hljs-number">2500</span><br>batch_size=<span class="hljs-number">50</span><br>epoch=<span class="hljs-number">100</span><br>learning_rate=<span class="hljs-number">0.01</span><br><span class="hljs-comment"># 初始化参数</span><br>w=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,true_w.shape,requires_grad=<span class="hljs-literal">True</span>)<br>b=torch.zeros(true_b.shape,requires_grad=<span class="hljs-literal">True</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SyntheticData</span>(<span class="hljs-params">w,b,num_examples</span>):</span><br>    X=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=(num_examples,<span class="hljs-built_in">len</span>(w)))<br>    <span class="hljs-comment"># 支持多维度、不同维度的矩阵乘法</span><br>    <span class="hljs-comment"># 直接加b用了广播机制</span><br>    y=torch.matmul(X,w) + b<br>    <span class="hljs-comment"># 增加噪声，使得不能完全拟合</span><br>    y+=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,y.shape)<br><br>    <span class="hljs-keyword">return</span> X,y.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 生成小批量</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DataIter</span>(<span class="hljs-params">batch_size,features,labels</span>):</span><br>    num_examples=<span class="hljs-built_in">len</span>(features)<br>    <br>    indices=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(num_examples))<br><br>    <span class="hljs-comment"># 打乱顺序</span><br>    random.shuffle(indices)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,num_examples,batch_size):<br>        batch_indices=torch.tensor(<br>            indices[i:<span class="hljs-built_in">min</span>(i+batch_size,num_examples)]<br>            )<br>        <br>        <span class="hljs-comment"># Tensor可以接受一个列表的下标来取值</span><br>        <span class="hljs-keyword">yield</span> features[batch_indices],labels[batch_indices]<br><br><span class="hljs-comment"># 定义模型</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LinReg</span>(<span class="hljs-params">X,w,b</span>):</span><br>    <br>    <span class="hljs-keyword">return</span> torch.matmul(X,w)+b<br><br><span class="hljs-comment"># 定义损失函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SquaredLoss</span>(<span class="hljs-params">y_hat,y</span>):</span><br>    <span class="hljs-keyword">return</span> (y_hat-y.view(y_hat.shape))**<span class="hljs-number">2</span>/<span class="hljs-number">2</span>/batch_size<br><br><span class="hljs-comment"># 优化算法</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sgd</span>(<span class="hljs-params">params,lr,batch_size</span>):</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param-=lr*param.grad<br>            param.grad.zero_()<br>    <br><br>features,labels=SyntheticData(true_w,true_b,num_examples)<br><br><span class="hljs-comment"># 训练</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Train</span>():</span><br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        e_loss=[]<br>        <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> DataIter(batch_size,features,labels):<br>            out=LinReg(X,w,b)<br>            <span class="hljs-comment"># 将batch_size个损失求和</span><br>            l=SquaredLoss(y,out).<span class="hljs-built_in">sum</span>()<br>            l.backward()<br>            sgd([w,b],learning_rate,batch_size)<br><br>            e_loss.append(l.item())<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{i+<span class="hljs-number">1</span>}</span>,loss:<span class="hljs-subst">{<span class="hljs-built_in">sum</span>(e_loss)/num_examples}</span>"</span>)<br><br><br>Train()<br><span class="hljs-comment"># 简单验证</span><br>x=torch.tensor([<span class="hljs-number">108</span>,<span class="hljs-number">2.666</span>])<br><span class="hljs-built_in">print</span>(true_w,w)<br><span class="hljs-built_in">print</span>(true_b,b)<br><span class="hljs-built_in">print</span>(LinReg(x,true_w,true_b).item())<br><span class="hljs-built_in">print</span>(LinReg(x,w,b).item())<br></code></pre></td></tr></tbody></table></figure><ul><li>简洁实现</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><br>true_w=torch.tensor([<span class="hljs-number">2.2</span>,<span class="hljs-number">3</span>])<br>true_b=torch.tensor([-<span class="hljs-number">1.9</span>])<br>num_examples=<span class="hljs-number">5000</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SyntheticData</span>(<span class="hljs-params">w,b,num_examples</span>):</span><br>    X=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=(num_examples,<span class="hljs-built_in">len</span>(w)))<br>    y=torch.matmul(X,w)+b<br><br>    y+=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,y.shape)<br><br>    <span class="hljs-keyword">return</span> X,y.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br><br>features,labels=SyntheticData(true_w,true_b,num_examples)<br><br><span class="hljs-comment"># 使第一维成为两个Tensor的共同索引，所以两个Tensor的第一维size要相同</span><br><span class="hljs-comment"># 用来组成dataloader可处理的形式</span><br>dataset=data.TensorDataset(features,labels)<br>data_iter=data.dataloader.DataLoader(dataset,shuffle=<span class="hljs-literal">True</span>,batch_size=<span class="hljs-number">150</span>)<br><br>net=nn.Sequential(nn.Linear(<span class="hljs-built_in">len</span>(true_w),<span class="hljs-number">1</span>,bias=<span class="hljs-literal">True</span>))<br>loss_f=nn.MSELoss()<br>opt=optim.SGD(net.parameters(),lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    e_loss=[]<br><br>    <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> data_iter:<br>        out=net(X)<br>        l=loss_f(out,y)<br><br>        e_loss.append(l.item())<br>        l.backward()<br>        opt.step()<br>        opt.zero_grad()<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span> <span class="hljs-subst">{<span class="hljs-built_in">sum</span>(e_loss)}</span>"</span>)<br><br>x=torch.tensor([<span class="hljs-number">108</span>,<span class="hljs-number">2.666</span>])<br><span class="hljs-built_in">print</span>(true_w,true_b)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> net.parameters():<br>    <span class="hljs-built_in">print</span>(i)<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D2L: Matrix</title>
      <link href="/2022/01/29/d2l_1/"/>
      <url>/2022/01/29/d2l_1/</url>
      
        <content type="html"><![CDATA[<h1 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h1><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a><strong>Basic knowledge</strong></h2><ul><li>标量</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x=torch.tensor([<span class="hljs-number">1.0</span>])<br>y=torch.tensor([<span class="hljs-number">2.0</span>])<br><br><span class="hljs-built_in">print</span>(x+y,x*y,x/y,x**y)<br></code></pre></td></tr></tbody></table></figure><ul><li>向量</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x=torch.arange(<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(x,x[<span class="hljs-number">3</span>],<span class="hljs-built_in">len</span>(x),x.shape,x.size())<br></code></pre></td></tr></tbody></table></figure><ul><li>矩阵</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x=torch.eye(<span class="hljs-number">20</span>)<br>x=torch.arange(<span class="hljs-number">20</span>).view(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(x,x.T,x.t())<br><span class="hljs-built_in">print</span>(torch.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br></code></pre></td></tr></tbody></table></figure><ul><li>运算</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">A=torch.arange(<span class="hljs-number">20</span>,dtype=torch.float32).view(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<br>B=A.clone() <span class="hljs-comment">#创建A的副本给B，A与B指向不同地址</span><br>C=B  <span class="hljs-comment">#B与C指向相同地址</span><br><span class="hljs-comment"># B，C都会改变</span><br>C[<span class="hljs-number">0</span>,:] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(A,B,C)<br><span class="hljs-comment"># 矩阵加 点乘 每个元素求sin</span><br><span class="hljs-built_in">print</span>(A+B,A*B,torch.sin(A))<br>A=torch.arange(<span class="hljs-number">2</span>*<span class="hljs-number">20</span>).view(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>).<span class="hljs-built_in">float</span>()<br><span class="hljs-built_in">print</span>(A)<br><span class="hljs-comment"># 求和可指定维度,不指定则全部维度求和</span><br><span class="hljs-built_in">print</span>(A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>),A.<span class="hljs-built_in">sum</span>([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]),A.<span class="hljs-built_in">sum</span>())<br><span class="hljs-comment"># 求均值可指定维度,不指定则全部维度求均值</span><br><span class="hljs-comment"># keepdim=True代表不把求均值的那个维度丢掉，一般用来保证广播机制的正常运行</span><br><span class="hljs-built_in">print</span>(A.mean(axis=<span class="hljs-number">0</span>,keepdim=<span class="hljs-literal">True</span>),A.mean([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]),A.mean())<br><span class="hljs-comment"># 求个数不可以指定维度</span><br><span class="hljs-built_in">print</span>(A.numel())<br><span class="hljs-comment"># 点积</span><br>X=torch.arange(<span class="hljs-number">4</span>).<span class="hljs-built_in">float</span>()<br>Y=torch.ones(<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(torch.dot(X,Y))<br><span class="hljs-comment"># 2范数,即向量长度，如果输入超过一维，会展成一维然后当成向量计算</span><br><span class="hljs-built_in">print</span>(torch.norm(X))<br><span class="hljs-comment"># 矩阵乘法</span><br>X=X.view(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>Y=Y.view(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(torch.mm(X,Y))<br></code></pre></td></tr></tbody></table></figure><h2 id="Derivative"><a href="#Derivative" class="headerlink" title="Derivative"></a><strong>Derivative</strong></h2><ul><li><p>向量上的导数<br>与向量相关的导数有以下四种形式<br><img src="/images/d2l/1/dv.png"></p><ul><li>y标量，x向量<br>y是由x中各分量计算得到的标量，最终得到y分别每个分量求导得出的向量，这也是梯度的计算过程<br><img src="/images/d2l/1/dv_1.png"></li><li>y向量，x标量<br>向量y的每个分量都是x的函数，每个分量分别对x求导最终得出一个向量<br><img src="/images/d2l/1/dv_2.png"></li><li>y向量，x向量<br>向量y的每个分量都是由x中各分量计算得到的标量，y的每个分量都分别对x的每个分量求导得出一个向量，最后得到一个矩阵<br><img src="/images/d2l/1/dv_3.png"></li></ul></li><li><p>矩阵上的导数<br>导数同样也可以被扩展到矩阵<br><img src="/images/d2l/1/dm.png"></p></li></ul><h2 id="Automatic-derivative"><a href="#Automatic-derivative" class="headerlink" title="Automatic derivative"></a><strong>Automatic derivative</strong></h2><ul><li><p>链式求法则<br>在神经网络中需要关注向量的链式求导，关键还是要把形状搞对<br><img src="/images/d2l/1/chain.png"> </p></li><li><p>自动求导<br>其含义是计算一个函数在指定值上的导数，它不同于符号求导和数值求导</p><ul><li>计算图<br>计算图本质上就等价于链式求导法则的求导过程，它将计算表示成一个无环图，下面是具体的例子<br><img src="/images/d2l/1/calc_graph.png"></li><li>反向传播<br>反向传播解决了自动求导的问题，它利用在前向传播时计算图中存储的计算中间结果，一步一步反向算出链式求导中各步导数<br><img src="/images/d2l/1/bp.png"></li><li>自动求导实现</li></ul></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pytorch中做Tensor的计算时默认直接构造计算图</span><br><span class="hljs-comment"># 计算图中入度为0的节点还应该存储关于自身的梯度，需要手动设置requires_grad=True</span><br>x=torch.arange(<span class="hljs-number">4.0</span>,requires_grad=<span class="hljs-literal">True</span>)<br>y=<span class="hljs-number">2</span>*torch.dot(x,x)<br><span class="hljs-built_in">print</span>(y)<br>y.backward()<br><span class="hljs-comment"># 计算得出的梯度存在grad中</span><br><span class="hljs-built_in">print</span>(x.grad)<br><br><span class="hljs-comment"># 要注意pytorch默认会把同一个变量上的梯度累加</span><br><span class="hljs-comment"># 在另一轮计算梯度时需要先清除之前的值</span><br>x.grad.zero_()<br>y=x.<span class="hljs-built_in">sum</span>()<br>y.backward()<br><span class="hljs-built_in">print</span>(x.grad)<br><br><span class="hljs-comment"># 在成batch地计算损失的过程中</span><br><span class="hljs-comment"># 最后将一个向量对向量求导的过程，转化为了标量对向量求导</span><br>x.grad.zero_()<br><br><span class="hljs-comment"># y是一个向量，把它对应为一个batch的损失</span><br>y=x*x<br><span class="hljs-comment"># 如果直接求导，会得到一个矩阵，这不是我们想要的</span><br><span class="hljs-comment"># 将batch中每一个损失加起来得到一个batch的损失后再求导</span><br><span class="hljs-comment"># 这也是pytorch中所做的</span><br>y.<span class="hljs-built_in">sum</span>().backward()<br><span class="hljs-built_in">print</span>(x.grad)<br><br><span class="hljs-comment"># 将某些计算移动到计算图之外</span><br><span class="hljs-comment"># 这在想要固定网络中某些参数时是有用的</span><br>x.grad.zero_()<br>y=x*x<br><span class="hljs-comment"># u不被计入为计算图的一个单元，而是一个常数</span><br>u=y.detach()<br>z=u*x<br>z.<span class="hljs-built_in">sum</span>().backward()<br><span class="hljs-built_in">print</span>(x.grad==u)<br><br><span class="hljs-comment"># 控制流也可以正确构成计算图并计算梯度</span><br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neural-Network </tag>
            
            <tag> Dive-Into-Deep-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C# Asynchronous programming #3</title>
      <link href="/2022/01/29/CSAsync3/"/>
      <url>/2022/01/29/CSAsync3/</url>
      
        <content type="html"><![CDATA[<h1 id="C-Asynchronous-programming-3"><a href="#C-Asynchronous-programming-3" class="headerlink" title="C# Asynchronous programming #3"></a>C# Asynchronous programming #3</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><h3 id="Use-Thread-in-Applications"><a href="#Use-Thread-in-Applications" class="headerlink" title="Use Thread in Applications"></a><strong>Use Thread in Applications</strong></h3><ul><li>在带界面的WPF、UWP、WinForm等程序种，若主线程执行耗时的操作，就会导致整个程序无响应。因为主线程同时还要处理消息循环，而渲染和鼠标键盘事件处理等工作都是在消息循环中执行的</li><li>针对这种耗时的操作，一种流行的做法使启用一个worker线程，执行完操作后再更新到UI</li><li>富客户端应用的线程模型通常是：<ul><li>UI控件只能从创建他们的线程来进行访问（通常是主线程）</li><li>当想从worker线程更新UI时，应该把请求交给UI线程</li></ul></li></ul><h3 id="Synchronization-Contexts"><a href="#Synchronization-Contexts" class="headerlink" title="Synchronization Contexts"></a><strong>Synchronization Contexts</strong></h3><ul><li>在System.ComponentModel下有一个抽象类：SynchronizationContext，它使得Thread Marshaling得到泛化<ul><li>Thread Marshaling：把一些数据的所有权从一个线程交给了另一个线程</li></ul></li></ul><h3 id="Thread-Pool"><a href="#Thread-Pool" class="headerlink" title="Thread Pool"></a><strong>Thread Pool</strong></h3><ul><li><p>当开始一个线程时，将花费数百微秒来组织一些内容（如一个新的局部变量栈），产生了开销</p></li><li><p>线程池可以节省这些开销：</p><ul><li>预先创建一个可循环使用的线程的池来减少创建新线程的开销</li></ul></li><li><p>线程池对于搞笑的并行编程和细粒度的并发是必不可少的</p></li><li><p>关于c#中的线程池需要注意以下几点：</p><ul><li>不可以设置池线程的<code>Name</code></li><li>池线程都是后台线程</li><li>阻塞池线程可能使性能降低</li><li>池线程优先级可以被自由的更改，当它被释放回池的时候优先级将被还原为正常状态</li><li>可以通过<code>IsThreadPoolThread</code>属性来判断是否为池线程</li><li><code>Task</code>也使用线程池</li></ul></li><li><p>线程池中的整洁</p><ul><li>线程池提供了另一个功能，即确保不会产生CPU超额订阅（活跃的线程数超过CPU核数），超额订阅对性能影响很大</li><li>CLR通过对任务排队并对其启动进行节流限制来避免线程池中的超额订阅</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> C# </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS-Threading </tag>
            
            <tag> Asynchronous-programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C# Asynchronous programming #2</title>
      <link href="/2022/01/29/CSAsync2/"/>
      <url>/2022/01/29/CSAsync2/</url>
      
        <content type="html"><![CDATA[<h1 id="C-Asynchronous-programming-2"><a href="#C-Asynchronous-programming-2" class="headerlink" title="C# Asynchronous programming #2"></a>C# Asynchronous programming #2</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><h3 id="Thread-Safety"><a href="#Thread-Safety" class="headerlink" title="Thread Safety"></a><strong>Thread Safety</strong></h3><ul><li>本地状态与共享状态<ul><li>Local 本地独立：CLR为每个线程分配自己的内存栈，以便使本地变量保持独立</li><li>Shared 共享：如果多个线程引用到了同一个对象实例，那么他们就共享了数据；被Lambad表达式或匿名委托捕获的本地变量，会被编译器转化为字段（field），所以也会被共享；静态字段也会在线程间共享</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-comment">/*本地变量不共享*/</span><br>        <span class="hljs-comment">//打印六次hello...</span><br>        Thread thread = <span class="hljs-keyword">new</span> Thread(GO);<br>        thread.Start();<br>        GO();<br>        <span class="hljs-comment">/*同一实例共享*/</span><br>        <span class="hljs-comment">//打印一次Done！</span><br>        ThreadTest test = <span class="hljs-keyword">new</span> ThreadTest();<br>        <span class="hljs-keyword">new</span> Thread(test.Test).Start();<br>        Thread.Sleep(<span class="hljs-number">1000</span>);<br>        test.Test();<br>        <span class="hljs-comment">/*匿名委托共享*/</span><br>        <span class="hljs-comment">//打印一次Done</span><br>        <span class="hljs-built_in">bool</span> done = <span class="hljs-literal">false</span>;<br>        ThreadStart action = () =&gt;<br>            {<br>                <span class="hljs-keyword">if</span> (!done)<br>                {<br>                    <span class="hljs-comment">//此处还是会有输出多次的风险</span><br>                    done = <span class="hljs-literal">true</span>;<br>                    System.Console.WriteLine(<span class="hljs-string">"Done"</span>);<br>                }<br>            };<br>        <span class="hljs-keyword">new</span> Thread(action).Start();<br>        Thread.Sleep(<span class="hljs-keyword">new</span> TimeSpan(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>));<br>        action.Invoke();<br>        <span class="hljs-comment">/*静态字段共享*/</span><br>        <span class="hljs-comment">//打印一次Done！</span><br>        ThreadTest test1 = <span class="hljs-keyword">new</span> ThreadTest();<br>        ThreadTest test2 = <span class="hljs-keyword">new</span> ThreadTest();<br>        <span class="hljs-keyword">new</span> Thread(test1.AnotherTest).Start();<br>        Thread.Sleep(<span class="hljs-number">10</span>);<br>        test2.AnotherTest();<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">GO</span>(<span class="hljs-params"></span>)</span><br>    {   <br>        <span class="hljs-comment">//cycle是本地变量，属于本地状态</span><br>        <span class="hljs-comment">//在每个线程的内存栈上，都会创建cycle的 独立副本</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> cycle = <span class="hljs-number">0</span>;  cycle&lt; <span class="hljs-number">3</span>; cycle++)<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"hello..."</span>);<br>        }<br>    }<br><br>}<br><br><span class="hljs-keyword">class</span> <span class="hljs-title">ThreadTest</span><br>{<br>    <span class="hljs-keyword">private</span> <span class="hljs-built_in">bool</span> _done;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-built_in">bool</span> done;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">AnotherTest</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-comment">//此处还是会有输出多次的风险</span><br>        <span class="hljs-keyword">if</span>(!done)<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"DONE!"</span>);<br>            done = <span class="hljs-literal">true</span>;<br>        }<br>    }<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Test</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-comment">//此处还是会有输出多次的风险</span><br>        <span class="hljs-keyword">if</span>(!_done)<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"Done!"</span>);<br>            _done = <span class="hljs-literal">true</span>;<br>        }<br>    }<br><br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>线程安全<ul><li>上述后涉及线程共享数据的代码是缺乏线程安全的，其实际输出无法确定，理论上Done有可能会打印两次，因为一个线程可能正在评估if，而另一个语句没来得及调整<code>done</code>为<code>true</code></li><li>保证线程安全：消除代码执行过程中的不确定性<ol><li>尽可能的避免使用共享状态以保证线程安全</li><li>使用<code>lock</code>语句加锁，在读取和写入共享数据的时候，通过使用互斥锁，就可以修复前面代码中的问题，当两个线程同时竞争一个锁的时候（锁可以基于任何引用类型对象），一个线程会等待或阻塞，直到锁重新变成可用状态</li></ol></li><li>然而，lock也并非线程安全的银弹，lock也会引起一些其他问题（如死锁）</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <span class="hljs-keyword">static</span> <span class="hljs-keyword">readonly</span> <span class="hljs-built_in">object</span> locker=<span class="hljs-keyword">new</span> <span class="hljs-built_in">object</span>();<br>    <span class="hljs-keyword">static</span> <span class="hljs-built_in">bool</span> done;<br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-keyword">new</span> Thread(GO).Start();<br>        GO();<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">GO</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-comment">// 锁要基于引用类型的变量</span><br>        <span class="hljs-comment">// 线程安全</span><br>        <span class="hljs-keyword">lock</span>(locker)<br>        {<br>            <span class="hljs-keyword">if</span> (!done)<br>            {<br>                System.Console.WriteLine(<span class="hljs-string">"Done"</span>);<br>                Thread.Sleep(<span class="hljs-number">1000</span>);<br>                done = <span class="hljs-literal">true</span>;<br>            }<br>        }<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="Transfer-Parameter"><a href="#Transfer-Parameter" class="headerlink" title="Transfer Parameter"></a><strong>Transfer Parameter</strong></h3><ul><li>如果想往线程启动方法里传递参数，最简单的方式就是使用lambda表达式，在里面使用参数调用方法</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>    {<br>        <br>        <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>        {<br>            <span class="hljs-keyword">new</span> Thread(() =&gt; { Print(<span class="hljs-string">"hello world"</span>); }).Start();<br><br>        }<br><br>        <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Print</span>(<span class="hljs-params"><span class="hljs-built_in">string</span> s</span>)</span><br>        {<br>            System.Console.WriteLine(s);<br>        }<br>    }<br></code></pre></td></tr></tbody></table></figure><ul><li>还可以使用<code>Thread.Start</code>方法来传递参数，类似委托中<code>Invoke</code>时的传参<ul><li>Thread的重载构造函数可以接受下列两个委托之一作为参数：<ol><li><code>public delegate void ThreadStart();</code></li><li><code>public delegate void ParameterizedThreadStart(object obj);</code></li></ol></li><li>第二种委托接收带参数的方法名</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        Thread thread = <span class="hljs-keyword">new</span> Thread(Print);<br>        thread.Start(<span class="hljs-string">"hello world"</span>);<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Print</span>(<span class="hljs-params"><span class="hljs-built_in">object</span> s</span>)</span><br>    {<br>        s = s <span class="hljs-keyword">as</span> <span class="hljs-built_in">string</span>;<br>        System.Console.WriteLine(s);<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>需要注意的是lambda表达式（匿名委托）传的参数会当作字段，即就算传的是值类型的变量也会得到其地址并被线程共享；而第二种传参强制要求了引用变量，也会被线程共享</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{  <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-comment">// Thread1();</span><br>        Thread2();<br>    }<br><br>    <span class="hljs-comment">// 线程共享了同一个局部变量i</span><br>    <span class="hljs-comment">// 会出现重复的数</span><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Thread1</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++)<br>        {<br>            <span class="hljs-keyword">new</span> Thread(() =&gt; { System.Console.WriteLine(i); }).Start();<br>        }<br>    }<br>    <span class="hljs-comment">// 每个线程获得不同局部变量的地址</span><br>    <span class="hljs-comment">// 不会出现重复的数</span><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Thread2</span>(<span class="hljs-params"></span>)</span><br>    {<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++)<br>        {<br>            <span class="hljs-built_in">int</span> temp = i;<br>            <span class="hljs-keyword">new</span> Thread(() =&gt; { System.Console.WriteLine(temp); }).Start();<br>        }<br>    }<br>    <br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="Exception"><a href="#Exception" class="headerlink" title="Exception"></a><strong>Exception</strong></h3><ul><li>异常处理块种的线程抛出异常时，不会被捕获，解决方案是传入线程的方法中处理异常</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{  <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-comment">// 正常捕获</span><br>        <span class="hljs-keyword">new</span> Thread(Go2).Start();<br>        <span class="hljs-comment">// 无法捕获</span><br>        <span class="hljs-keyword">try</span><br>        {<br>            <span class="hljs-keyword">new</span> Thread(Go1).Start();<br>        }<br>        catch<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"Exceptoin!"</span>);<br>        }<br>    }   <br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Go1</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> Exception();<br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Go2</span>(<span class="hljs-params"></span>)</span><br>    {<br>        <span class="hljs-keyword">try</span><br>        {<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> Exception();<br>        }<br><br>        catch<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"catch sucessfully"</span>);<br>        }<br>    }<br>    <br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="Foreground-Threads-and-Background-Threads"><a href="#Foreground-Threads-and-Background-Threads" class="headerlink" title="Foreground Threads and Background Threads"></a><strong>Foreground Threads and Background Threads</strong></h3><ul><li>默认情况下，手动创建的线程就是前台线程</li><li>只要有前台线程在运行，那么应用程序就会一直处于活动状态<ul><li>后台线程运行不会保持应用程序的活动状态</li><li>一旦所有前台线程停止，应用程序随即停止，后台线程也会立即终止</li></ul></li><li>线程的前台、后台与它的优先级无关</li><li>通过<code>IsBackground</code>属性判断线程是否是后台线程</li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{  <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        Thread thread = <span class="hljs-keyword">new</span> Thread(() =&gt; { Console.WriteLine(Console.ReadLine());});<br>        <br>        <span class="hljs-comment">// 如果将thread设置为后台线程，当主线程（前台线程）执行完之后程序立即结束</span><br>        <span class="hljs-keyword">if</span>(args.Length&gt;<span class="hljs-number">0</span>)<br>            thread.IsBackground = <span class="hljs-literal">true</span>;<br><br>        thread.Start();<br>    }   <br>    <br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>如果在退出前想要等待后台线程执行完毕，可以考虑使用<code>Join</code></li><li>应用程序无法正常退出的一个常见原因就是还有活跃的前台线程</li></ul><h3 id="Priority-of-Threading"><a href="#Priority-of-Threading" class="headerlink" title="Priority of Threading"></a><strong>Priority of Threading</strong></h3><ul><li>线程的优先级（<code>Thread</code>中<code>Priority</code>属性）决定了相对于操作系统中其他活跃线程所占的执行时间</li><li>优先级划分：<ul><li><code>enum ThreadPriority{Lowest,BelowNormal,Normal,AboveNormal,Highest}</code></li></ul></li><li>提升线程优先级：<ul><li>提升线程优先级时要特别注意，因为他可能饿死其他线程</li><li>如果想让某线程的优先级比其他进程中的线程优先级高，那么就必须提升进程优先级</li></ul></li><li>手动提升进程/线程优先级适用于只做少量工作且需要较低延迟的非UI进程</li><li>对于需要大量计算的应用程序（尤其是带UI的），手动提升进程/线程优先级可能会使其他进程/线程饿死，从而降低整个计算机的速度</li></ul><h3 id="Signaling"><a href="#Signaling" class="headerlink" title="Signaling"></a><strong>Signaling</strong></h3><ul><li>某个线程在收到其他线程发来通知之前一直处于等待状态，发送通知的过程就称为Signaling，不同于信号量机制</li><li>最简单的信号结构就是<code>ManualResetEvent</code><ul><li>调用其上<code>WaitOne</code>方法会阻塞当前线程，直到另一个线程通过调用<code>Set</code>方法开启信号</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{ <br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        <span class="hljs-keyword">var</span> signal = <span class="hljs-keyword">new</span> ManualResetEvent(<span class="hljs-literal">false</span>);<br><br>        <span class="hljs-keyword">new</span> Thread(() =&gt;<br>        {<br>            System.Console.WriteLine(<span class="hljs-string">"Waiting for signal..."</span>);<br>            <span class="hljs-comment">// 因为在实例化时设置了false主线程打开信号之前处于阻塞状态</span><br>            <span class="hljs-comment">// 成功接收到信号时不会将signal状态重置为nonusignaled</span><br>            signal.WaitOne();<br>            <span class="hljs-comment">// 获得信号后直接关闭该信号量</span><br>            System.Console.WriteLine(<span class="hljs-string">"Got signal!"</span>);<br>        }).Start();<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++)<br>        {<br>            System.Console.WriteLine(i);<br>        }<br>        Thread.Sleep(<span class="hljs-number">1000</span>);<br>        signal.Set();<br>    }   <br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>调用完<code>Set</code>后信号会处于signaled状态，通过调用<code>Reset</code>将信号重新变为nonsignaled状态</li></ul>]]></content>
      
      
      <categories>
          
          <category> C# </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS-Threading </tag>
            
            <tag> Asynchronous-programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C# Asynchronous programming #1</title>
      <link href="/2022/01/29/CSAsync1/"/>
      <url>/2022/01/29/CSAsync1/</url>
      
        <content type="html"><![CDATA[<h1 id="C-Asynchronous-programming-1"><a href="#C-Asynchronous-programming-1" class="headerlink" title="C# Asynchronous programming #1"></a>C# Asynchronous programming #1</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a><strong>Thread</strong></h2><hr><h3 id="What-is-Thread"><a href="#What-is-Thread" class="headerlink" title="What is Thread ?"></a><strong>What is Thread ?</strong></h3><ul><li>线程是一个可执行路径，每一个线程可以独立于其他线程执行</li><li>每个线程在均进程(Process)内执行，在操作系统中，进行提供了程序运行的独立环境</li><li>单线程应用，在进程的独立环境中只跑一个线程，该线程具有独占权</li><li>多线程应用，单个进程中跑多个线程，多个线程共享当前的执行环境（尤其是内存）<ul><li>共享：多个线程共同占有某种资源，如一个线程在后台读取数据，另一个线程在数据到达后进行展示。  </li></ul></li></ul><p>下面是c#中最简单的异步编程实例：</p><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>{<br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>    {<br>        Thread.CurrentThread.Name = <span class="hljs-string">"Main Thread..."</span>;<br>        <span class="hljs-comment">//开辟一个新线程            </span><br>        Thread thread = <span class="hljs-keyword">new</span> Thread(WriteY);<br>        thread.Name = <span class="hljs-string">"Y Thread..."</span>;<br>        thread.Start();<br>        Thread.Sleep(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//同时在主线程上做一些工作</span><br>        System.Console.WriteLine(Thread.CurrentThread.Name);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">1000</span>; i++)<br>        {<br>            System.Console.Write(<span class="hljs-string">"x"</span>);<br>        }<br>        <br><br>    }<br><br>    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">WriteY</span>(<span class="hljs-params"></span>)</span><br>    {<br>        System.Console.WriteLine(Thread.CurrentThread.Name);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">1000</span>; i++)<br>        {<br>            System.Console.Write(<span class="hljs-string">"y"</span>);<br>        }<br>        <br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><ul><li>在单核计算机上，操作系统必须为每个线程分配<strong>时间片</strong>（Windows下通常为20ms）来模拟并发，从而在本例中，会输出重复的x块与y块</li><li>在多核或多处理器的计算机上，使用c#创建的多线程可以真正意义上并行执行。然而，在本例中由于控制台程序处理并发请求机制的微妙性，仍然会得到重复的x块与y块<br><img src="/images/CSAsynchronousProgramming/first_example.png" alt="first_example"></li><li>c#中线程的一些属性：<ul><li>线程一旦开始执行，属性<code>IsAlive</code>就变为<code>true</code>，线程结束就变为<code>false</code></li><li>线程结束的条件：线程构造器中传入的委托结束了执行</li><li>线程一旦结束，便无法重启</li><li>每个线程都有一个<code>Name</code>属性，通常用于调试，<code>Name</code>只能设置一次，多次更改会抛出异常</li><li>静态属性<code>Thread.CurrentThread</code>，指向当前执行的线程</li></ul></li></ul><h3 id="Join-and-Sleep"><a href="#Join-and-Sleep" class="headerlink" title="Join and Sleep"></a><strong>Join and Sleep</strong></h3><ul><li>在线程A中调用另一个线程B实例的<code>Join</code>方法，线程A便会等待线程B执行结束后继续执行。</li><li>调用<code>Jion</code>方法时，可在参数中设置一个超时(毫秒/<code>TimeSpan</code>)<ul><li>使用有超时的重载方法时返回值<code>bool</code>类型，<code>true</code>：线程结束；<code>false</code>：超时</li></ul></li><li><code>Thread.Sleep</code>方法会暂停当前的线程，并等待一段时间<ul><li><code>Thread.Sleep(0)</code>会导致县城立即放弃本身当前时间片，自动将cpu转交给其他线程</li><li><code>Thread.Yield</code>与<code>Thread.Sleep</code>做同样的事情，不同的是<code>Thread.Yield</code>只会把执行交给同一处理器的其他线程</li><li>当等待<code>Sleep</code>和<code>Join</code>时，线程处于阻塞状态</li></ul></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">class</span> <span class="hljs-title">Program</span><br>    {<br>        <span class="hljs-keyword">static</span> Thread thread1;<br>        <span class="hljs-keyword">static</span> Thread thread2;<br>        <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Main</span>(<span class="hljs-params"><span class="hljs-built_in">string</span>[] args</span>)</span><br>        {<br>            thread1 = <span class="hljs-keyword">new</span> Thread(ThreadProc);<br>            thread1.Name = <span class="hljs-string">"t1"</span>;<br>            thread1.Start();<br><br>            thread2 = <span class="hljs-keyword">new</span> Thread(ThreadProc);<br>            thread2.Name = <span class="hljs-string">"t2"</span>;<br>            thread2.Start();<br>        }<br><br><br>        <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">ThreadProc</span>(<span class="hljs-params"></span>)</span><br>        {<br>            System.Console.WriteLine(<span class="hljs-string">$"\nCurrent thread:<span class="hljs-subst">{Thread.CurrentThread.Name}</span>"</span>);<br>            <span class="hljs-keyword">if</span>(Thread.CurrentThread.Name==<span class="hljs-string">"t1"</span>&amp;&amp;<br>            ((thread2.ThreadState&amp;ThreadState.Unstarted)==<span class="hljs-number">0</span>))<br>                <span class="hljs-keyword">if</span>(thread2.Join(<span class="hljs-number">2000</span>))<br>                    System.Console.WriteLine(<span class="hljs-string">"t2 has been terminated"</span>);<br>                <span class="hljs-keyword">else</span><br>                    System.Console.WriteLine(<span class="hljs-string">"overtime"</span>);<br><br>            <span class="hljs-comment">//TimeSpan(0,0,4) 0h，0m，4s</span><br>            Thread.Sleep(<span class="hljs-keyword">new</span> TimeSpan(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">4</span>));<br>            System.Console.WriteLine(<span class="hljs-string">$"\nCurrent thread:<span class="hljs-subst">{Thread.CurrentThread.Name}</span>"</span>);<br>            System.Console.WriteLine(<span class="hljs-string">$"Thread1:<span class="hljs-subst">{thread1.ThreadState}</span>"</span>);<br>            System.Console.WriteLine(<span class="hljs-string">$"Thread2:<span class="hljs-subst">{thread2.ThreadState}</span>"</span>);<br><br>        }<br>    }<br></code></pre></td></tr></tbody></table></figure><p>在一次执行中，输出结果如下：<br><img src="/images/CSAsynchronousProgramming/join_sleep.png" alt="join_sleep"><br>在此次运行过程中，线程<code>t2</code>首先进入<code>ThreadProc</code>，之后开始Sleep，时间片交给线程<code>t1</code>，if判断成立，<code>t1</code>进入阻塞状态等待<code>t2</code>运行结束。<code>t2</code>Sleep结束后开始运行，运行结束后，<code>t1</code>继续运行直到结束</p><h3 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a><strong>Blocking</strong></h3><ul><li>如果线程的执行由于某种原因导致暂停，那么就认为该线程被阻塞了<ul><li>如<code>Sleep</code>、<code>Join</code></li></ul></li><li>被阻塞的线程会立即将其时间片交给其他线程，从此不再消耗处理器时间，知道满足阻塞结束条件为止</li><li>可以通过<code>ThreadState</code>属性来判断线程是否处于阻塞状态</li><li><code>ThreadState</code>是一个<em>flags enum</em>，通过按位与/或来合并数据项<br><img src="/images/CSAsynchronousProgramming/thread_state.png" alt="thread_state"><br><code>ThreadState</code>变化图如下：<br><img src="/images/CSAsynchronousProgramming/transform.png" alt="transform"><br>常用的状态只有四个：<code>Unstarted</code>、<code>Running</code>、<code>WaitSleepJoin</code>和<code>Stopped</code></li></ul><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> ThreadState <span class="hljs-title">Foo</span>(<span class="hljs-params">ThreadState ts</span>)</span><br>        {<br>            <span class="hljs-keyword">return</span> ts &amp; (<br>                ThreadState.Stopped |<br>                ThreadState.Unstarted |<br>                ThreadState.WaitSleepJoin<br>            );<br>        }<br></code></pre></td></tr></tbody></table></figure><ul><li><p>当遇到下列四种情况解除阻塞：</p><ul><li>阻塞条件被满足</li><li>操作超时(如果设置了超时)</li><li>通过<code>Thread.Interrupt()</code>进行打断</li><li>通过<code>Thread.Abort()</code>进行中止</li></ul></li><li><p>上下文切换</p><ul><li>当线程阻塞或解除阻塞时，操作系统执行上下文切换。这会产生少量开销，通常为1或2微秒</li></ul></li><li><p>I/O密集型与CPU密集型</p><ul><li>花费大部分时间等待某事发生的操作称为I/O密集型，通常此事指输入/输出，但不是硬性要求，如<code>Thread.Sleep()</code>被视为I/O密集型</li><li>相反，一个花费大部分时间执行CPU密集型的操作成为CPU密集型</li></ul></li><li><p>阻塞与忙等待</p><ul><li>阻塞是在当前线程上同步的等待，<code>Console.ReadLine()</code>、<code>Thread.Sleep()</code>、<code>Thread.Join()</code>都是阻塞操作</li><li>忙等待以周期性的在一个循环里打转，也是同步的<br><code>while(DataTime.Now&lt;nextStartTime)</code></li><li>还有一种异步的操作，在操作完成后触发回调</li><li>如果条件很快得到满足（在几微秒之内）,短暂的忙等待更为适合，因为他避免了上下文切换的开销</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> C# </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS-Threading </tag>
            
            <tag> Asynchronous-programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typography and tags</title>
      <link href="/2021/10/15/Typography%20and%20tags/"/>
      <url>/2021/10/15/Typography%20and%20tags/</url>
      
        <content type="html"><![CDATA[<h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><hr><p>This post uses <code>hexo-renderer-markdown-it</code> plugin as markdown processor, so please install it to achieve the effect.</p><figure class="highlight bash"><figcaption><span>installation</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm un hexo-renderer-marked --save<br>npm i hexo-renderer-markdown-it --save<br>npm i markdown-it-emoji --save<br>npm i markdown-it-mark --save<br>npm i markdown-it-deflist --save<br>npm i markdown-it-container --save<br></code></pre></td></tr></tbody></table></figure><h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><p>Add following to <code>_config.yml</code> of your site.</p><figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">markdown:</span><br>  <span class="hljs-attr">render:</span><br>    <span class="hljs-attr">html:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">xhtmlOut:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">breaks:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">linkify:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">typographer:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">quotes:</span> <span class="hljs-string">'“”‘’'</span><br>  <span class="hljs-attr">plugins:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">markdown-it-abbr</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">markdown-it-footnote</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">markdown-it-ins</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">markdown-it-sub</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">markdown-it-sup</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">markdown-it-deflist</span><br>  <span class="hljs-attr">anchors:</span><br>    <span class="hljs-attr">level:</span> <span class="hljs-number">2</span><br>    <span class="hljs-attr">collisionSuffix:</span> <span class="hljs-string">'v'</span><br>    <span class="hljs-attr">permalink:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">permalinkClass:</span> <span class="hljs-string">header-anchor</span><br>    <span class="hljs-attr">permalinkSymbol:</span> <span class="hljs-string">" "</span><br>    <span class="hljs-attr">permalinkBefore:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></tbody></table></figure><h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><hr><h2 id="Headings"><a href="#Headings" class="headerlink" title="Headings"></a>Headings</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># h1 Heading 8-)</span><br><span class="hljs-section">## h2 Heading</span><br><span class="hljs-section">### h3 Heading</span><br><span class="hljs-section">#### h4 Heading</span><br><span class="hljs-section">##### h5 Heading</span><br><span class="hljs-section">###### h6 Heading</span><br></code></pre></td></tr></tbody></table></figure><h1 id="h1-Heading-8"><a href="#h1-Heading-8" class="headerlink" title="h1 Heading 8-)"></a>h1 Heading 8-)</h1><h2 id="h2-Heading"><a href="#h2-Heading" class="headerlink" title="h2 Heading"></a>h2 Heading</h2><h3 id="h3-Heading"><a href="#h3-Heading" class="headerlink" title="h3 Heading"></a>h3 Heading</h3><h4 id="h4-Heading"><a href="#h4-Heading" class="headerlink" title="h4 Heading"></a>h4 Heading</h4><h5 id="h5-Heading"><a href="#h5-Heading" class="headerlink" title="h5 Heading"></a>h5 Heading</h5><h6 id="h6-Heading"><a href="#h6-Heading" class="headerlink" title="h6 Heading"></a>h6 Heading</h6><h2 id="Horizontal-Rules"><a href="#Horizontal-Rules" class="headerlink" title="Horizontal Rules"></a>Horizontal Rules</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">__<span class="hljs-emphasis">_</span></span><br><span class="hljs-emphasis"><span class="hljs-strong"></span></span><br><span class="hljs-emphasis"><span class="hljs-strong">---</span></span><br><span class="hljs-emphasis"><span class="hljs-strong"></span></span><br><span class="hljs-emphasis"><span class="hljs-strong"><span class="hljs-strong">**<span class="hljs-emphasis">*</span></span></span></span><br></code></pre></td></tr></tbody></table></figure><hr><hr><hr><h2 id="Typographic-replacements"><a href="#Typographic-replacements" class="headerlink" title="Typographic replacements"></a>Typographic replacements</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown">(c) (C) (r) (R) (tm) (TM) (p) (P) +-<br><br>test.. test... test..... test?..... test!....<br><br>!!!!!! ???? ,,  -- ---<br><br>"Smartypants, double quotes" and 'single quotes'<br></code></pre></td></tr></tbody></table></figure><p>(c) (C) (r) (R) (tm) (TM) (p) (P) +-</p><p>test.. test… test….. test?….. test!….</p><p>!!!!!! ???? ,,  – —</p><p>“Smartypants, double quotes” and ‘single quotes’</p><h2 id="Emphasis"><a href="#Emphasis" class="headerlink" title="Emphasis"></a>Emphasis</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">**This is bold text**</span><br><br><span class="hljs-strong">__This is bold text__</span><br><br><span class="hljs-emphasis">*This is italic text*</span><br><br><span class="hljs-emphasis">_This is italic text_</span><br><br>~~Strikethrough~~<br></code></pre></td></tr></tbody></table></figure><p><strong>This is bold text</strong></p><p><strong>This is bold text</strong></p><p><em>This is italic text</em></p><p><em>This is italic text</em></p><p><del>Strikethrough</del></p><h2 id="Blockquotes"><a href="#Blockquotes" class="headerlink" title="Blockquotes"></a>Blockquotes</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-quote">&gt; Blockquotes can also be nested...</span><br>&gt;&gt; ...by using additional greater-than signs right next to each other...<br><span class="hljs-quote">&gt; &gt; &gt; ...or with spaces between arrows.</span><br></code></pre></td></tr></tbody></table></figure><blockquote><p>Blockquotes can also be nested…</p><blockquote><p>…by using additional greater-than signs right next to each other…</p><blockquote><p>…or with spaces between arrows.</p></blockquote></blockquote></blockquote><h2 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h2><h3 id="Unordered"><a href="#Unordered" class="headerlink" title="Unordered"></a>Unordered</h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">+</span> Create a list by starting a line with <span class="hljs-code">`+`</span>, <span class="hljs-code">`-`</span>, or <span class="hljs-code">`*`</span><br><span class="hljs-bullet">+</span> Sub-lists are made by indenting 2 spaces:<br><span class="hljs-bullet">  -</span> Marker character change forces new list start:<br><span class="hljs-bullet">    *</span> Ac tristique libero volutpat at<br><span class="hljs-bullet">    +</span> Facilisis in pretium nisl aliquet<br><span class="hljs-bullet">    -</span> Nulla volutpat aliquam velit<br><span class="hljs-bullet">+</span> Very easy!<br></code></pre></td></tr></tbody></table></figure><ul><li>Create a list by starting a line with <code>+</code>, <code>-</code>, or <code>*</code></li><li>Sub-lists are made by indenting 2 spaces:<ul><li>Marker character change forces new list start:<ul><li>Ac tristique libero volutpat at</li></ul><ul><li>Facilisis in pretium nisl aliquet</li></ul><ul><li>Nulla volutpat aliquam velit</li></ul></li></ul></li><li>Very easy!</li></ul><h3 id="Ordered"><a href="#Ordered" class="headerlink" title="Ordered"></a>Ordered</h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> Lorem ipsum dolor sit amet<br><span class="hljs-bullet">  1.</span> Indented list<br><span class="hljs-bullet">    1.</span> Another level<br><span class="hljs-bullet">  2.</span> Indent<br><span class="hljs-bullet">2.</span> Consectetur adipiscing elit<br><span class="hljs-bullet">3.</span> Integer molestie lorem at massa<br></code></pre></td></tr></tbody></table></figure><ol><li>Lorem ipsum dolor sit amet</li><li>Indented list    1. Another level</li><li>Indent</li><li>Consectetur adipiscing elit</li><li>Integer molestie lorem at massa</li></ol><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> You can use sequential numbers...<br><span class="hljs-bullet">1.</span> ...or keep all the numbers as <span class="hljs-code">`1.`</span><br></code></pre></td></tr></tbody></table></figure><ol><li>You can use sequential numbers…</li><li>…or keep all the numbers as <code>1.</code></li></ol><h4 id="Start-numbering-with-offset"><a href="#Start-numbering-with-offset" class="headerlink" title="Start numbering with offset:"></a>Start numbering with offset:</h4><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">57.</span> foo<br><span class="hljs-bullet">1.</span> bar<br></code></pre></td></tr></tbody></table></figure><ol start="57"><li>foo</li><li>bar</li></ol><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Inline <span class="hljs-code">`code`</span><br></code></pre></td></tr></tbody></table></figure><p>Inline <code>code</code></p><h3 id="Indented-code"><a href="#Indented-code" class="headerlink" title="Indented code"></a>Indented code</h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">// Some comments<br><span class="hljs-code">    line 1 of code</span><br><span class="hljs-code">    line 2 of code</span><br><span class="hljs-code">    line 3 of code</span><br></code></pre></td></tr></tbody></table></figure><pre><code>// Some commentsline 1 of codeline 2 of codeline 3 of code</code></pre><h3 id="Block-code-“fences”"><a href="#Block-code-“fences”" class="headerlink" title="Block code “fences”"></a>Block code “fences”</h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-code">```</span><br><span class="hljs-code">Sample text here...</span><br><span class="hljs-code">```</span><br></code></pre></td></tr></tbody></table></figure><figure class="highlight gams"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-function"><span class="hljs-title">Sample</span></span> text here...<br></code></pre></td></tr></tbody></table></figure><p>Syntax highlighting</p><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-code">``` js sample.js</span><br><span class="hljs-code">var foo = function (bar) {</span><br><span class="hljs-code">  return bar++;</span><br><span class="hljs-code">};</span><br><span class="hljs-code"></span><br><span class="hljs-code">console.log(foo(5));</span><br><span class="hljs-code">```</span><br></code></pre></td></tr></tbody></table></figure><figure class="highlight js"><figcaption><span>sample.js</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">var</span> foo = <span class="hljs-function"><span class="hljs-keyword">function</span> (<span class="hljs-params">bar</span>) </span>{<br>  <span class="hljs-keyword">return</span> bar++;<br>};<br><br><span class="hljs-built_in">console</span>.log(foo(<span class="hljs-number">5</span>));<br></code></pre></td></tr></tbody></table></figure><h2 id="Tables"><a href="#Tables" class="headerlink" title="Tables"></a>Tables</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown">| Option | Description |Description | Description | Description | Description |<br>| ------ | ----------- |----------- | ----------- | ----------- | ----------- |<br>| data   | path to data files to supply the data that will be passed into templates. |<br>| engine | engine to be used for processing templates. Handlebars is the default. |<br>| ext    | extension to be used for dest files. |<br></code></pre></td></tr></tbody></table></figure><table><thead><tr><th>Option</th><th>Description</th><th>Description</th><th>Description</th><th>Description</th><th>Description</th></tr></thead><tbody><tr><td>data</td><td>path to data files to supply the data that will be passed into templates.</td><td></td><td></td><td></td><td></td></tr><tr><td>engine</td><td>engine to be used for processing templates. Handlebars is the default.</td><td></td><td></td><td></td><td></td></tr><tr><td>ext</td><td>extension to be used for dest files.</td><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="Right-aligned-columns"><a href="#Right-aligned-columns" class="headerlink" title="Right aligned columns"></a>Right aligned columns</h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown">| Option | Description |<br>| ------:| -----------:|<br>| data   | path to data files to supply the data that will be passed into templates. |<br>| engine | engine to be used for processing templates. Handlebars is the default. |<br>| ext    | extension to be used for dest files. |<br></code></pre></td></tr></tbody></table></figure><table><thead><tr><th align="right">Option</th><th align="right">Description</th></tr></thead><tbody><tr><td align="right">data</td><td align="right">path to data files to supply the data that will be passed into templates.</td></tr><tr><td align="right">engine</td><td align="right">engine to be used for processing templates. Handlebars is the default.</td></tr><tr><td align="right">ext</td><td align="right">extension to be used for dest files.</td></tr></tbody></table><h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">[<span class="hljs-string">link text</span>](<span class="hljs-link">http://dev.nodeca.com</span>)<br></code></pre></td></tr></tbody></table></figure><p><a href="http://dev.nodeca.com/">link text</a></p><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">[<span class="hljs-string">link with title</span>](<span class="hljs-link">http://nodeca.github.io/pica/demo/ "title text!"</span>)<br></code></pre></td></tr></tbody></table></figure><p><a href="http://nodeca.github.io/pica/demo/" title="title text!">link with title</a></p><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Autoconverted link https://github.com/nodeca/pica (enabled linkify)<br></code></pre></td></tr></tbody></table></figure><p>Autoconverted link <a href="https://github.com/nodeca/pica">https://github.com/nodeca/pica</a> (enabled linkify)</p><h2 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h2><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown">![<span class="hljs-string">Minion</span>](<span class="hljs-link">https://octodex.github.com/images/minion.png</span>)<br>![<span class="hljs-string">Stormtroopocat</span>](<span class="hljs-link">https://octodex.github.com/images/stormtroopocat.jpg "The Stormtroopocat"</span>)<br></code></pre></td></tr></tbody></table></figure><p><img src="https://octodex.github.com/images/minion.png" alt="Minion"><br><img src="https://octodex.github.com/images/stormtroopocat.jpg" alt="Stormtroopocat" title="The Stormtroopocat"></p><p>Like links, Images also have a footnote style syntax</p><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown">![<span class="hljs-string">Alt text</span>][<span class="hljs-symbol">id</span>]<br><br>With a reference later in the document defining the URL location:<br><br>[<span class="hljs-symbol">id</span>]: <span class="hljs-link">https://octodex.github.com/images/dojocat.jpg  "The Dojocat"</span><br></code></pre></td></tr></tbody></table></figure><p><img src="https://octodex.github.com/images/dojocat.jpg" alt="Alt text" title="The Dojocat"></p><p>With a reference later in the document defining the URL location:</p><h2 id="Plugins"><a href="#Plugins" class="headerlink" title="Plugins"></a>Plugins</h2><p>The killer feature of <code>markdown-it</code> is very effective support of<br><a href="https://www.npmjs.org/browse/keyword/markdown-it-plugin">syntax plugins</a>. The sample <a href="#configuration">configuration snippet</a></p><h3 id="Emojies"><a href="#Emojies" class="headerlink" title="Emojies"></a><a href="https://github.com/markdown-it/markdown-it-emoji">Emojies</a></h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Classic markup: :wink: :crush: :cry: :tear: :laughing: :yum:<br><br>Shortcuts (emoticons): :-) :-( 8-) ;)<br></code></pre></td></tr></tbody></table></figure><p>Classic markup: <span class="github-emoji"><span>😉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> :crush: <span class="github-emoji"><span>😢</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> :tear: <span class="github-emoji"><span>😆</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f60b.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><p>Shortcuts (emoticons): :-) :-( 8-) ;)</p><h3 id="Subscript-Superscript"><a href="#Subscript-Superscript" class="headerlink" title="Subscript / Superscript"></a><a href="https://github.com/markdown-it/markdown-it-sub">Subscript</a> / <a href="https://github.com/markdown-it/markdown-it-sup">Superscript</a></h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Superscript: 19^th^<br><br>Subscript: H~2~O<br></code></pre></td></tr></tbody></table></figure><p>Superscript: 19^th^</p><p>Subscript: H<del>2</del>O</p><h3 id="lt-ins-gt"><a href="#lt-ins-gt" class="headerlink" title="<ins>"></a><a href="https://github.com/markdown-it/markdown-it-ins">&lt;ins&gt;</a></h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">++Inserted text++<br></code></pre></td></tr></tbody></table></figure><p>++Inserted text++</p><h3 id="lt-mark-gt"><a href="#lt-mark-gt" class="headerlink" title="<mark>"></a><a href="https://github.com/markdown-it/markdown-it-mark">&lt;mark&gt;</a></h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">==Marked text==<br></code></pre></td></tr></tbody></table></figure><p>==Marked text==</p><h3 id="Footnotes"><a href="#Footnotes" class="headerlink" title="Footnotes"></a><a href="https://github.com/markdown-it/markdown-it-footnote">Footnotes</a></h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Footnote 1 link[^first].<br><br>Footnote 2 link[^second].<br><br>Inline footnote^[Text of inline footnote] definition.<br><br>Duplicated footnote reference[^second].<br><br>[<span class="hljs-symbol">^first</span>]: <span class="hljs-link">Footnote **can have markup**</span><br><br><span class="hljs-code">    and multiple paragraphs.</span><br><span class="hljs-code"></span><br>[<span class="hljs-symbol">^second</span>]: <span class="hljs-link">Footnote text.</span><br></code></pre></td></tr></tbody></table></figure><p>Footnote 1 link[^first].</p><p>Footnote 2 link[^second].</p><p>Inline footnote^[Text of inline footnote] definition.</p><p>Duplicated footnote reference[^second].</p><p>[^first]: Footnote <strong>can have markup</strong></p><pre><code>and multiple paragraphs.</code></pre><p>[^second]: Footnote text.</p><h3 id="Definition-lists"><a href="#Definition-lists" class="headerlink" title="Definition lists"></a><a href="https://github.com/markdown-it/markdown-it-deflist">Definition lists</a></h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Term 1<br>:   Definition 1<br>with lazy continuation.<br></code></pre></td></tr></tbody></table></figure><dl><dt>Term 1</dt><dd>Definition 1<br>with lazy continuation.</dd></dl><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Term 2 with <span class="hljs-emphasis">*inline markup*</span><br>:   Definition 2<br><br><span class="hljs-code">        { some code, part of Definition 2 }</span><br><span class="hljs-code"></span><br><span class="hljs-code">    Third paragraph of definition 2.</span><br></code></pre></td></tr></tbody></table></figure><dl><dt>Term 2 with <em>inline markup</em></dt><dd>Definition 2</dd></dl><pre><code>    { some code, part of Definition 2 }Third paragraph of definition 2.</code></pre><p><em>Compact style:</em></p><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Term 1<br>  ~ Definition 1<br><br>Term 2<br>  ~ Definition 2a<br>  ~ Definition 2b<br></code></pre></td></tr></tbody></table></figure><p>Term 1<br>  ~ Definition 1</p><p>Term 2<br>  ~ Definition 2a<br>  ~ Definition 2b</p><h3 id="Abbreviations"><a href="#Abbreviations" class="headerlink" title="Abbreviations"></a><a href="https://github.com/markdown-it/markdown-it-abbr">Abbreviations</a></h3><figure class="highlight markdown"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown">This is HTML abbreviation example.<br><br>It converts "HTML", but keep intact partial entries like "xxxHTMLyyy" and so on.<br><br><span class="hljs-emphasis">*[HTML]: Hyper Text Markup Language</span><br></code></pre></td></tr></tbody></table></figure><p>This is HTML abbreviation example.</p><p>It converts “HTML”, but keep intact partial entries like “xxxHTMLyyy” and so on.</p><p>*[HTML]: Hyper Text Markup Language</p><h3 id="Custom-containers"><a href="#Custom-containers" class="headerlink" title="Custom containers"></a><a href="https://github.com/markdown-it/markdown-it-container">Custom containers</a></h3><p>::: warning<br><em>here be dragons</em><br>:::</p><h2 id="Hexo-Built-in-Tags"><a href="#Hexo-Built-in-Tags" class="headerlink" title="Hexo Built-in Tags"></a>Hexo Built-in Tags</h2><h3 id="Blockquote-with-author"><a href="#Blockquote-with-author" class="headerlink" title="Blockquote with author"></a>Blockquote with author</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% blockquote David Levithan, Wide Awake %}<br>Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy.<br>{% endblockquote %}<br></code></pre></td></tr></tbody></table></figure><blockquote><p>Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy.</p><footer><strong>David Levithan</strong><cite>Wide Awake</cite></footer></blockquote><h3 id="Blockquote-for-twitter"><a href="#Blockquote-for-twitter" class="headerlink" title="Blockquote for twitter"></a>Blockquote for twitter</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% blockquote @DevDocs https://twitter.com/devdocs/status/356095192085962752 %}<br>NEW: DevDocs now comes with syntax highlighting. http://devdocs.io<br>{% endblockquote %}<br></code></pre></td></tr></tbody></table></figure><blockquote><p>NEW: DevDocs now comes with syntax highlighting. <a href="http://devdocs.io/">http://devdocs.io</a></p><footer><strong>@DevDocs</strong><cite><a href="https://twitter.com/devdocs/status/356095192085962752">twitter.com/devdocs/status/356095192085962752</a></cite></footer></blockquote><h3 id="Blockquote-for-weblink"><a href="#Blockquote-for-weblink" class="headerlink" title="Blockquote for weblink"></a>Blockquote for weblink</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% blockquote Seth Godin http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html Welcome to Island Marketing %}<br>Every interaction is both precious and an opportunity to delight.<br>{% endblockquote %}<br></code></pre></td></tr></tbody></table></figure><blockquote><p>Every interaction is both precious and an opportunity to delight.</p><footer><strong>Seth Godin</strong><cite><a href="http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html">Welcome to Island Marketing</a></cite></footer></blockquote><h3 id="Pull-Quotes"><a href="#Pull-Quotes" class="headerlink" title="Pull Quotes"></a>Pull Quotes</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% pullquote %}<br>content<br>{% endpullquote %}<br></code></pre></td></tr></tbody></table></figure><blockquote class="pullquote"><p>content</p></blockquote><h3 id="jsFiddle"><a href="#jsFiddle" class="headerlink" title="jsFiddle"></a>jsFiddle</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% jsfiddle o2gxgz9r default light %}<br></code></pre></td></tr></tbody></table></figure><iframe scrolling="no" width="100%" height="300" src="https://jsfiddle.net/o2gxgz9r/embedded/js,resources,html,css,result/light" frameborder="0" loading="lazy" allowfullscreen=""></iframe><h3 id="Gist"><a href="#Gist" class="headerlink" title="Gist"></a>Gist</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% gist b6365e79be6052e7531e7ba6ea8caf23 'Sample gist' %}<br></code></pre></td></tr></tbody></table></figure><script src="//gist.github.com/b6365e79be6052e7531e7ba6ea8caf23.js?file=Sample gist"></script><h3 id="iFrame"><a href="#iFrame" class="headerlink" title="iFrame"></a>iFrame</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% iframe https://www.bing.com %}<br></code></pre></td></tr></tbody></table></figure><iframe src="https://www.bing.com/" width="100%" height="300" frameborder="0" loading="lazy" allowfullscreen=""></iframe><h3 id="Link-to-open-in-new-tab"><a href="#Link-to-open-in-new-tab" class="headerlink" title="Link to open in new tab"></a>Link to open in new tab</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% link Google https://www.google.com default Google %}<br></code></pre></td></tr></tbody></table></figure><a href="https://www.google.com/" title="default Google" target="">Google</a><h3 id="Youtube"><a href="#Youtube" class="headerlink" title="Youtube"></a>Youtube</h3><figure class="highlight plaintext"><figcaption><span>source code</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs swig">{% youtube l_lblj8Cq0o %}<br></code></pre></td></tr></tbody></table></figure><div class="video-container"><iframe src="https://www.youtube.com/embed/l_lblj8Cq0o" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> typography </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/10/14/first-post/"/>
      <url>/2021/10/14/first-post/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/helloworld.jpg" alt="$cover"></p>]]></content>
      
      
      <categories>
          
          <category> Genesis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hello </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
