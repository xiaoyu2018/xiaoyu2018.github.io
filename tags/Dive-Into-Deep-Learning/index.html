<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Xavier's blog</title><meta name="description" content="while(true) me.Learn();"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/tom.jpg"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Xavier's blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Xavier's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Tags · Dive-Into-Deep-Learning</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/d2l/1/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/28/d2l_8/"><img class="post-cover-img js-img-fadeIn" src="/images/d2l/1/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/28/d2l_8/">D2L: Basic Pytorch</a></h2><time class="has-text-grey" datetime="2021-11-28T09:15:03.446Z">2021-11-28</time><p class="is-flex-grow-2 mt-2">
Basic Pytorch
模型构造

import torch 
from torch import nn
net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))
X=torch.normal(0,1,size=(1,20))
print(net(X))

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.rand_weight=torch.randn((20,64),requires_grad=False,dtype=torch.float32)
        self.hidden=nn.Sequenti..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/28/d2l_8/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/d2l/1/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/28/d2l_7/"><img class="post-cover-img js-img-fadeIn" src="/images/d2l/1/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/28/d2l_7/">D2L: Numerical Stability &amp; Initialization</a></h2><time class="has-text-grey" datetime="2021-11-28T06:18:56.312Z">2021-11-28</time><p class="is-flex-grow-2 mt-2">
D2L: Numerical Stability &amp;amp; InitializationBasic Knowledge
数值的稳定性
神经网络的梯度求某一层的参数的梯度，直接就对损失函数关于该层参数求导,然后通过链式法则，化成d-t次的矩阵乘法
梯度爆炸与梯度消失上述连续的乘法运算会带来两个问题：梯度爆炸与梯度消失梯度爆炸带来的问题：梯度值超过计算机可表示大小、对学习率敏感梯度消失带来的问题：梯度值变为0（超出计算可表示精度的小浮点数）、无论如何选择学习率训练都没有进展、神经网络无法做到更深


模型初始化
如何让训练更加稳定？（不产生梯度消失和梯度爆炸）要让梯度值保持在合理的范围内，一般有如下方法：
将乘法变为加法（ResNet、LSTM）
归一化（梯度归一化、梯度裁剪）
选定合适的激活函数
合理的初..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/28/d2l_7/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/d2l/1/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/25/d2l_6/"><img class="post-cover-img js-img-fadeIn" src="/images/d2l/1/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/25/d2l_6/">D2L: Weight Decay &amp; Dropout</a></h2><time class="has-text-grey" datetime="2021-11-25T13:32:01.344Z">2021-11-25</time><p class="is-flex-grow-2 mt-2">
Weight Decay &amp;amp; DropoutBasic Knowledge
权重衰退权重衰退可以控制模型复杂度，使其复杂度不会太大，从而一定程度上避免过拟合
使用均方范数作为硬性限制通过限制参数的取值范围来控制模型容量，具体有如下例子在限制参数向量范数的情况下优化损失函数，一般不会使用这种正则方式
使用均方范数作为柔性限制上述硬性限制有一个等价方案，具体如下，这就是一般的正则化方法，其作用同样也是使得参数被限制在一个较小的范围下面是正则项对最优解影响的一个演示坐标轴分别是w的各个分量，圆线是等高线。正则项给了另外一个梯度，把原始的损失函数算出的最优解往原点拉，必然会导致W的取值范围变小从而使模型复杂度降低，也就减小了过拟合。另外一种理解，正则项加入后优化目标就不再全局最优点了，所以肯定会减小训练集..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/25/d2l_6/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/d2l/1/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/24/d2l_5/"><img class="post-cover-img js-img-fadeIn" src="/images/d2l/1/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/24/d2l_5/">D2L: Modle Selection</a></h2><time class="has-text-grey" datetime="2021-11-24T09:15:12.949Z">2021-11-24</time><p class="is-flex-grow-2 mt-2">
Modle SelectionBasic Knowledge
训练误差和泛化误差  
训练误差：模型在训练数据上的误差（不太关心）
泛化误差：模型在新数据上的误差（很关心）


验证数据集和测试数据集
验证数据集用于在训练过程中评估模型好坏的数据集，一般从训练集中划分出一部分，验证数据集不能作为训练集让模型训练，用来动态调整模型超参数
测试数据集模型最终训练完毕后，使用测试集测试模型泛化能力，不能使用测试集来调整模型超参数，大多数情况下不会被打上标签


K-折交叉验证通常情况下，我们都没有足够富裕的数据去从训练集中划分验证集，这是使用K-折交叉验证能较简单的解决问题
思想：一般情况将K折交叉验证用于模型调优，找到使得模型泛化性能最优的超参值。找到后，在全部训练集上重新训练模型，并使用独立验证集对模型性能..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/24/d2l_5/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/d2l/1/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/24/d2l_4/"><img class="post-cover-img js-img-fadeIn" src="/images/d2l/1/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/24/d2l_4/">D2L: Perceptron</a></h2><time class="has-text-grey" datetime="2021-11-24T08:09:16.313Z">2021-11-24</time><p class="is-flex-grow-2 mt-2">
PerceptronBasic Knowledge
感知机  
模型感知机只比线性分类多了一个激活函数，激活函数为单层感知机带来了分类能力，为多层感知机带来了非线性因素
训练训练感知机等价于批量大小为1的梯度下降，按顺序逐个取样本，与随机梯度下降不同
单层感知机无法解决异或问题，他只能产生线性分割面，这导致了第一次AI寒冬


多层感知机
多层感知机由多个感知机组成，分为输入层、隐藏层、输出层，层内不连接，层间全连接
每个感知机输出后要经过一个非线性的激活函数，否则多层感知机等价于单层感知机
常用激活函数：Sigmoiod、Tanh、ReLU，性能都没太大区别，ReLU计算更容易，如果没有特别的想法，用ReLU就行



Implementation
从零实现

from torch import nn,..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/24/d2l_4/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/images/d2l/1/cover.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/11/23/d2l_3/"><img class="post-cover-img js-img-fadeIn" src="/images/d2l/1/cover.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Dive-Into-Deep-Learning"><i class="tag post-item-tag">Dive-Into-Deep-Learning</i></a><a href="/tags/Neural-Network"><i class="tag post-item-tag">Neural-Network</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/11/23/d2l_3/">D2L: Softmax Regression</a></h2><time class="has-text-grey" datetime="2021-11-23T14:29:44.147Z">2021-11-23</time><p class="is-flex-grow-2 mt-2">
Softmax RegressionBasic Knowledge
softmax操作子将输出变为一个概率分布（保证非负性与归一性）
交叉熵损失用于衡量两个概率分布的区别,将softmax输出的分布与one-hot形式的标签作为两个分布

Implementation
数据集

import torch
import torchvision
from torch.utils import data
from torchvision import transforms

def LoadData(batch_size,resize=None):
    # 用于把图片转为Tensor，会自动归一化
    trans=[transforms.ToTensor()]
    if resize:
      ..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/11/23/d2l_3/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><span class="page-number current">1</span><a class="page-number" href="/tags/Dive-Into-Deep-Learning/page/2/">2</a><a class="extend next" rel="next" href="/tags/Dive-Into-Deep-Learning/page/2/"><i class="iconfont icon-next has-text-grey"></i></a></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container tag-widget is-in-tag-page"><h3>Tags</h3><section><a href="/tags/typography"><span class="tag post-item-tag" style="margin-bottom: 5px;">typography</span></a><a href="/tags/hexo"><span class="tag post-item-tag" style="margin-bottom: 5px;">hexo</span></a><a href="/tags/Hello"><span class="tag post-item-tag" style="margin-bottom: 5px;">Hello</span></a><a href="/tags/papers"><span class="tag post-item-tag" style="margin-bottom: 5px;">papers</span></a><a href="/tags/Dive-Into-Deep-Learning"><span class="tag post-item-tag" style="margin-bottom: 5px;">Dive-Into-Deep-Learning</span></a><a href="/tags/Neural-Network"><span class="tag post-item-tag" style="margin-bottom: 5px;">Neural-Network</span></a><a href="/tags/Asynchronous-programming"><span class="tag post-item-tag" style="margin-bottom: 5px;">Asynchronous-programming</span></a><a href="/tags/CS-Threading"><span class="tag post-item-tag" style="margin-bottom: 5px;">CS-Threading</span></a><a href="/tags/Generative-Model"><span class="tag post-item-tag" style="margin-bottom: 5px;">Generative-Model</span></a><a href="/tags/Residual-learning"><span class="tag post-item-tag" style="margin-bottom: 5px;">Residual-learning</span></a><a href="/tags/Seq2Seq-Model"><span class="tag post-item-tag" style="margin-bottom: 5px;">Seq2Seq-Model</span></a></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/xiaoyu2018"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Xavier 2021</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>